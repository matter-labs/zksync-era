<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ZKsync Era Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="css/version-box.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ZKsync Era Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/matter-labs/zksync-era/tree/main/docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the documentation! This guide provides comprehensive insights into the architecture, setup, usage, and
advanced features of ZKsync.</p>
<h2 id="documentation-structure"><a class="header" href="#documentation-structure">Documentation Structure</a></h2>
<ul>
<li>
<p><strong>Guides</strong>: The Guides section is designed to help users at every level, from setup and development to advanced
configuration and debugging techniques. It covers essential topics, including Docker setup, repository management, and
architecture.</p>
</li>
<li>
<p><strong>Specs</strong>: This section dives into the technical specifications of our system. Here, you’ll find detailed
documentation on data availability, L1 and L2 communication, smart contract interactions, Zero-Knowledge proofs, and
more. Each topic includes an in-depth explanation to support advanced users and developers.</p>
</li>
<li>
<p><strong>Announcements</strong>: This section highlights important updates, announcements, and committee details, providing
essential information to keep users informed on the latest changes.</p>
</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>Feel free to explore each section according to your needs. This documentation is designed to be modular, so you can jump
to specific topics or follow through step-by-step.</p>
<hr />
<p>Thank you for using our documentation!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-basic-guides"><a class="header" href="#zksync-basic-guides">ZKsync basic guides</a></h1>
<p>This section contains basic guides that aim to explain the ZKsync ecosystem in an easy to grasp way.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="guides/./architecture.html">Architecture</a></li>
<li><a href="guides/./build-docker.html">Build Docker</a></li>
<li><a href="guides/./development.html">Development</a></li>
<li><a href="guides/./launch.html">Launch</a></li>
<li><a href="guides/./repositories.html">Repositories</a></li>
<li><a href="guides/./setup-dev.html">Setup Dev</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-dependencies"><a class="header" href="#installing-dependencies">Installing dependencies</a></h1>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>This is a shorter version of setup guide to make it easier subsequent initializations. If it’s the first time you’re
initializing the workspace, it’s recommended that you read the whole guide below, as it provides more context and tips.</p>
<p>If you run on ‘clean’ Ubuntu on GCP:</p>
<pre><code class="language-bash"># For VMs only! They don't have SSH keys, so we override SSH with HTTPS
git config --global url."https://github.com/".insteadOf git@github.com:
git config --global url."https://".insteadOf git://

# Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# NVM
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash

# All necessary stuff
sudo apt-get update
sudo apt-get install -y build-essential pkg-config cmake clang lldb lld libssl-dev libpq-dev apt-transport-https ca-certificates curl software-properties-common

# Install docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
sudo apt install docker-ce
sudo usermod -aG docker ${USER}

# Start docker.
sudo systemctl start docker

## You might need to re-connect (due to usermod change).

# Node &amp; yarn
nvm install 20
# Important: there will be a note in the output to load
# new paths in your local session, either run it or reload the terminal.
npm install -g yarn
yarn set version 1.22.19

# For running unit tests
cargo install cargo-nextest
# SQL tools
cargo install sqlx-cli --version 0.8.1

# Foundry ZKsync
curl -L https://raw.githubusercontent.com/matter-labs/foundry-zksync/main/install-foundry-zksync | bash
foundryup-zksync

# Non CUDA (GPU) setup, can be skipped if the machine has a CUDA installed for provers
# Don't do that if you intend to run provers on your machine. Check the prover docs for a setup instead.
echo "export ZKSYNC_USE_CUDA_STUBS=true" &gt;&gt; ~/.bashrc
# You will need to reload your `*rc` file here

# Clone the repo to the desired location
git clone git@github.com:matter-labs/zksync-era.git
cd zksync-era
git submodule update --init --recursive
</code></pre>
<p>Don’t forget to look at <a href="guides/setup-dev.html#tips">tips</a>.</p>
<h2 id="supported-operating-systems"><a class="header" href="#supported-operating-systems">Supported operating systems</a></h2>
<p>ZKsync currently can be launched on any *nix operating system (e.g. any linux distribution or macOS).</p>
<p>If you’re using Windows, then make sure to use WSL 2.</p>
<p>Additionally, if you are going to use WSL 2, make sure that your project is located in the <em>linux filesystem</em>, since
accessing NTFS partitions from within WSL is very slow.</p>
<p>If you’re using macOS with an ARM processor (e.g. M1/M2), make sure that you are working in the <em>native</em> environment
(e.g., your terminal and IDE don’t run in Rosetta, and your toolchain is native). Trying to work with ZKsync code via
Rosetta may cause problems that are hard to spot and debug, so make sure to check everything before you start.</p>
<p>If you are a NixOS user or would like to have a reproducible environment, skip to the section about <code>nix</code>.</p>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>Install <code>docker</code>. It is recommended to follow the instructions from the
<a href="https://docs.docker.com/install/">official site</a>.</p>
<p>Note: currently official site proposes using Docker Desktop for Linux, which is a GUI tool with plenty of quirks. If you
want to only have CLI tool, you need the <code>docker-ce</code> package and you can follow
<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04">this guide</a> for Ubuntu.</p>
<p>Installing <code>docker</code> via <code>snap</code> or from the default repository can cause troubles.</p>
<p>You need to install both <code>docker</code> and <code>docker compose</code>.</p>
<p><strong>Note:</strong> <code>docker compose</code> is installed automatically with <code>Docker Desktop</code>.</p>
<p><strong>Note:</strong> On linux you may encounter the following error when you’ll try to work with <code>zksync</code>:</p>
<pre><code>ERROR: Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
</code></pre>
<p>If so, you <strong>do not need</strong> to install <code>docker-machine</code>. Most probably, it means that your user is not added to
the<code>docker</code> group. You can check it as follows:</p>
<pre><code class="language-bash">docker-compose up # Should raise the same error.
sudo docker-compose up # Should start doing things.
</code></pre>
<p>If the first command fails, but the second succeeds, then you need to add your user to the <code>docker</code> group:</p>
<pre><code class="language-bash">sudo usermod -a -G docker your_user_name
</code></pre>
<p>After that, you should logout and login again (user groups are refreshed after the login). The problem should be solved
at this step.</p>
<p>If logging out does not resolve the issue, restarting the computer should.</p>
<h2 id="nodejs--yarn"><a class="header" href="#nodejs--yarn">Node.js &amp; Yarn</a></h2>
<ol>
<li>Install <code>Node</code> (requires version <code>v20</code>). The recommended way is via <a href="https://github.com/nvm-sh/nvm">nvm</a>.</li>
<li>Install <code>yarn</code>. Can be done via <code>npm install -g yarn</code>. Make sure to get version 1.22.19 - you can change the version
by running <code>yarn set version 1.22.19</code>.</li>
</ol>
<h2 id="clang"><a class="header" href="#clang">clang</a></h2>
<p>In order to compile RocksDB, you must have LLVM available. On debian-based linux it can be installed as follows:</p>
<p>On debian-based linux:</p>
<pre><code class="language-bash">sudo apt-get install build-essential pkg-config cmake clang lldb lld
</code></pre>
<p>On macOS:</p>
<p>You need to have an up-to-date <code>Xcode</code>. You can install it directly from <code>App Store</code>. With Xcode command line tools, you
get the Clang compiler installed by default. Thus, having XCode you don’t need to install <code>clang</code>.</p>
<h2 id="openssl"><a class="header" href="#openssl">OpenSSL</a></h2>
<p>Install OpenSSL:</p>
<p>On mac:</p>
<pre><code class="language-bash">brew install openssl
</code></pre>
<p>On debian-based linux:</p>
<pre><code class="language-bash">sudo apt-get install libssl-dev
</code></pre>
<h2 id="rust"><a class="header" href="#rust">Rust</a></h2>
<p>Install <code>Rust</code>’s toolchain version reported in <code>/rust-toolchain.toml</code> (also a later stable version should work).</p>
<p>Instructions can be found on the <a href="https://www.rust-lang.org/tools/install">official site</a>.</p>
<p>Verify the <code>rust</code> installation:</p>
<pre><code class="language-bash">rustc --version
rustc 1.xx.y (xxxxxx 20xx-yy-zz) # Output may vary depending on actual version of rust
</code></pre>
<p>If you are using macOS with ARM processor (e.g. M1/M2), make sure that you use an <code>aarch64</code> toolchain. For example, when
you run <code>rustup show</code>, you should see a similar input:</p>
<pre><code class="language-bash">rustup show
Default host: aarch64-apple-darwin
rustup home:  /Users/user/.rustup

installed toolchains
--------------------

...

active toolchain
----------------

1.67.1-aarch64-apple-darwin (overridden by '/Users/user/workspace/zksync-era/rust-toolchain')
</code></pre>
<p>If you see <code>x86_64</code> mentioned in the output, probably you’re running (or used to run) your IDE/terminal in Rosetta. If
that’s the case, you should probably change the way you run terminal, and/or reinstall your IDE, and then reinstall the
Rust toolchain as well.</p>
<h2 id="postgresql-client-library"><a class="header" href="#postgresql-client-library">PostgreSQL Client Library</a></h2>
<p>For development purposes, you typically only need the PostgreSQL client library, not the full server installation.
Here’s how to install it:</p>
<p>On macOS:</p>
<pre><code class="language-bash">brew install libpq
</code></pre>
<p>On Debian-based Linux:</p>
<pre><code class="language-bash">sudo apt-get install libpq-dev
</code></pre>
<h3 id="cargo-nextest"><a class="header" href="#cargo-nextest">Cargo nextest</a></h3>
<p><a href="https://nexte.st/">cargo-nextest</a> is the next-generation test runner for Rust projects. <code>zkstack dev test rust</code> uses
<code>cargo nextest</code> by default.</p>
<pre><code class="language-bash">cargo install cargo-nextest
</code></pre>
<h3 id="sqlx-cli"><a class="header" href="#sqlx-cli">SQLx CLI</a></h3>
<p>SQLx is a Rust library we use to interact with Postgres, and its CLI is used to manage DB migrations and support several
features of the library.</p>
<pre><code class="language-bash">cargo install --locked sqlx-cli --version 0.8.1
</code></pre>
<h2 id="easier-method-using-nix"><a class="header" href="#easier-method-using-nix">Easier method using <code>nix</code></a></h2>
<p>Nix is a tool that can fetch <em>exactly</em> the right dependencies specified via hashes. The current config is Linux-only but
it is likely that it can be adapted to Mac.</p>
<p>Install <code>nix</code>. Enable the nix command and flakes.</p>
<p>Install docker, rustup and use rust to install SQLx CLI like described above. If you are on NixOS, you also need to
enable nix-ld.</p>
<p>Go to the zksync folder and run <code>nix develop</code>. After it finishes, you are in a shell that has all the dependencies.</p>
<h2 id="foundry-zksync"><a class="header" href="#foundry-zksync">Foundry ZKsync</a></h2>
<p>ZKSync depends on Foundry ZKsync (which is is a specialized fork of Foundry, tailored for ZKsync). Please follow this
<a href="https://foundry-book.zksync.io/getting-started/installation">installation guide</a> to get started with Foundry ZKsync.</p>
<p>Foundry ZKsync can also be used for deploying smart contracts. For commands related to deployment, you can pass flags
for Foundry integration.</p>
<h2 id="non-gpu-setup"><a class="header" href="#non-gpu-setup">Non-GPU setup</a></h2>
<p>Circuit Prover requires a CUDA bindings to run. If you still want to be able to build everything locally on non-CUDA
setup, you’ll need use CUDA stubs.</p>
<p>For a single run, it’s enough to export it on the shell:</p>
<pre><code>export ZKSYNC_USE_CUDA_STUBS=true
</code></pre>
<p>For persistent runs, you can echo it in your ~/.<shell>rc file</p>
<pre><code>echo "export ZKSYNC_USE_CUDA_STUBS=true" &gt;&gt; ~/.&lt;SHELL&gt;rc
</code></pre>
<p>Note that the same can be achieved with RUSTFLAGS (discouraged). The flag is <code>--cfg=no_cuda</code>. You can either set
RUSTFLAGS as env var, or pass it in <code>config.toml</code> (either project level or global). The config would need the following:</p>
<pre><code class="language-toml">[build]
rustflags = ["--cfg=no_cuda"]
</code></pre>
<h2 id="tips"><a class="header" href="#tips">Tips</a></h2>
<h3 id="tip-mold"><a class="header" href="#tip-mold">Tip: <code>mold</code></a></h3>
<p>Optionally, you may want to optimize the build time with the modern linker, <a href="https://github.com/rui314/mold"><code>mold</code></a>.</p>
<p>This linker will speed up the build times, which can be pretty big for Rust binaries.</p>
<p>Follow the instructions in the repo in order to install it and enable for Rust.</p>
<p>If you installed <code>mold</code> to <code>/usr/local/bin/mold</code>, then the quickest way to use it without modifying any files is:</p>
<pre><code class="language-bash">export RUSTFLAGS='-C link-arg=-fuse-ld=/usr/local/bin/mold'
export CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_LINKER="clang"
</code></pre>
<h3 id="tip-speeding-up-building-rocksdb"><a class="header" href="#tip-speeding-up-building-rocksdb">Tip: Speeding up building <code>RocksDB</code></a></h3>
<p>By default, each time you compile <code>rocksdb</code> crate, it will compile required C++ sources from scratch. It can be avoided
by using precompiled versions of library, and it will significantly improve your build times.</p>
<p>In order to do so, you can put compiled libraries to some persistent location, and add the following to your shell
configuration file (e.g. <code>.zshrc</code> or <code>.bashrc</code>):</p>
<pre><code>export ROCKSDB_LIB_DIR=&lt;library location&gt;
export SNAPPY_LIB_DIR=&lt;library location&gt;
</code></pre>
<p>Make sure that compiled libraries match the current version of RocksDB. One way to obtain them, is to compile the
project in the usual way once, and then take built libraries from
<code>target/{debug,release}/build/librocksdb-sys-{some random value}/out</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="development-guide"><a class="header" href="#development-guide">Development guide</a></h1>
<p>This document outlines the steps for setting up and working with ZKsync.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>If you haven’t already, install the prerequisites as described in <a href="guides/./setup-dev.html">Install Dependencies</a>.</p>
<h2 id="installing-the-local-zk-stack-cli"><a class="header" href="#installing-the-local-zk-stack-cli">Installing the local ZK Stack CLI</a></h2>
<p>To set up local development, begin by installing
<a href="https://github.com/matter-labs/zksync-era/blob/main/zkstack_cli/README.md">ZK Stack CLI</a>. From the project’s root
directory, run the following commands:</p>
<pre><code class="language-bash">cd ./zkstack_cli/zkstackup
./install --local
</code></pre>
<p>This installs <code>zkstackup</code> in your user binaries directory (e.g., <code>$HOME/.local/bin/</code>) and adds it to your <code>PATH</code>.</p>
<p>After installation, open a new terminal or reload your shell profile. From the project’s root directory, you can now
run:</p>
<pre><code class="language-bash">zkstackup --local
</code></pre>
<p>This command installs <code>zkstack</code> from the current source directory.</p>
<p>You can proceed to verify the installation and start familiarizing with the CLI by running:</p>
<pre><code class="language-bash">zkstack --help
</code></pre>
<blockquote>
<p>NOTE: Whenever you want to update you local installation with your changes, just rerun:</p>
<pre><code class="language-bash">zkstackup --local
</code></pre>
<p>You might find convenient to add this alias to your shell profile:</p>
<p><code>alias zkstackup='zkstackup --path /path/to/zksync-era'</code></p>
</blockquote>
<h2 id="configure-ecosystem"><a class="header" href="#configure-ecosystem">Configure Ecosystem</a></h2>
<p>The project root directory includes configuration files for an ecosystem with a single chain, <code>era</code>. To initialize the
ecosystem, first start the required containers:</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<p>Next, run:</p>
<pre><code class="language-bash">zkstack ecosystem init
</code></pre>
<p>These commands will guide you through the configuration options for setting up the ecosystem.</p>
<blockquote>
<p>NOTE: For local development only. You can also use the development defaults by supplying the <code>--dev</code> flag.</p>
</blockquote>
<p>Initialization may take some time, but key steps (such as downloading and unpacking keys or setting up containers) only
need to be completed once.</p>
<p>To see more detailed output, you can run commands with the <code>--verbose</code> flag.</p>
<h2 id="cleanup"><a class="header" href="#cleanup">Cleanup</a></h2>
<p>To clean up the local ecosystem (e.g., removing containers and clearing the contract cache), run:</p>
<pre><code class="language-bash">zkstack dev clean all
</code></pre>
<p>You can then reinitialize the ecosystem as described in the <a href="guides/development.html#configure-ecosystem">Configure Ecosystem</a> section.</p>
<pre><code class="language-bash">zkstack containers
zkstack ecosystem init
</code></pre>
<h2 id="committing-changes"><a class="header" href="#committing-changes">Committing changes</a></h2>
<p><code>zksync</code> uses pre-commit and pre-push git hooks for basic code integrity checks. Hooks are set up automatically within
the workspace initialization process. These hooks will not allow to commit the code which does not pass several checks.</p>
<p>Currently the following criteria are checked:</p>
<ul>
<li>Code must be formatted via <code>zkstack dev fmt</code>.</li>
<li>Code must be linted via <code>zkstack dev lint</code>.</li>
</ul>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>ZKstack CLI offers multiple subcommands to run specific integration and unit test:</p>
<pre><code class="language-bash">zkstack dev test --help
</code></pre>
<pre><code class="language-bash">Usage: zkstack dev test [OPTIONS] &lt;COMMAND&gt;

Commands:
  integration   Run integration tests
  fees          Run fees test
  revert        Run revert tests
  recovery      Run recovery tests
  upgrade       Run upgrade tests
  build         Build all test dependencies
  rust          Run unit-tests, accepts optional cargo test flags
  l1-contracts  Run L1 contracts tests
  prover        Run prover tests
  wallet        Print test wallets information
  loadtest      Run loadtest
  help          Print this message or the help of the given subcommand(s)
</code></pre>
<h3 id="running-unit-tests"><a class="header" href="#running-unit-tests">Running unit tests</a></h3>
<p>You can run unit tests for the Rust crates in the project by running:</p>
<pre><code class="language-bash">zkstack dev test rust
</code></pre>
<h3 id="running-integration-tests"><a class="header" href="#running-integration-tests">Running integration tests</a></h3>
<p>Running integration tests is more complex. Some tests require a running server, while others need the system to be in a
specific state. Please refer to our CI scripts
<a href="https://github.com/matter-labs/zksync-era/blob/main/.github/workflows/ci-core-reusable.yml">ci-core-reusable.yml</a> to
have a better understanding of the process.</p>
<h3 id="running-load-tests"><a class="header" href="#running-load-tests">Running load tests</a></h3>
<p>The current load test implementation only supports the legacy bridge. To use it, you need to create a new chain with
legacy bridge support:</p>
<pre><code class="language-bash">zkstack chain create --legacy-bridge
zkstack chain init
</code></pre>
<p>After initializing the chain with a legacy bridge, you can run the load test against it.</p>
<pre><code class="language-bash">zkstack dev test loadtest
</code></pre>
<blockquote>
<p>WARNING: Never use legacy bridges in non-testing environments.</p>
</blockquote>
<h2 id="contracts"><a class="header" href="#contracts">Contracts</a></h2>
<h3 id="build-contracts"><a class="header" href="#build-contracts">Build contracts</a></h3>
<p>Run:</p>
<pre><code class="language-bash">zkstack dev contracts --help
</code></pre>
<p>to see all the options.</p>
<h3 id="publish-source-code-on-etherscan"><a class="header" href="#publish-source-code-on-etherscan">Publish source code on Etherscan</a></h3>
<h4 id="verifier-options"><a class="header" href="#verifier-options">Verifier Options</a></h4>
<p>Most commands interacting with smart contracts support the same verification options as Foundry’s <code>forge</code> command. Just
double check if the following options are available in the subcommand:</p>
<pre><code class="language-bash">--verifier                  -- Verifier to use
--verifier-api-key          -- Verifier API key
--verifier-url              -- Verifier URL, if using a custom provider
</code></pre>
<h4 id="using-foundry"><a class="header" href="#using-foundry">Using Foundry</a></h4>
<p>You can use <code>foundry</code> to verify the source code of the contracts.</p>
<pre><code class="language-bash">forge verify-contract
</code></pre>
<p>Verifies a smart contract on a chosen verification provider.</p>
<p>You must provide:</p>
<ul>
<li>The contract address</li>
<li>The contract name or the path to the contract.</li>
<li>In case of Etherscan verification, you must also provide:
<ul>
<li>Your Etherscan API key, either by passing it as an argument or setting <code>ETHERSCAN_API_KEY</code></li>
</ul>
</li>
</ul>
<p>For more information check <a href="https://book.getfoundry.sh/reference/forge/forge-verify-contract">Foundry’s documentation</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-application"><a class="header" href="#running-the-application">Running the application</a></h1>
<p>This document covers common scenarios for launching ZKsync applications set locally.</p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<p>Prepare dev environment prerequisites: see</p>
<p><a href="guides/./setup-dev.html">Installing dependencies</a></p>
<h2 id="setup-local-dev-environment"><a class="header" href="#setup-local-dev-environment">Setup local dev environment</a></h2>
<p>Run the required containers with:</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<p>Setup:</p>
<pre><code class="language-bash">zkstack ecosystem init
</code></pre>
<p>To completely reset the dev environment:</p>
<ul>
<li>
<p>Stop services:</p>
<pre><code class="language-bash">zkstack dev clean all
</code></pre>
</li>
<li>
<p>Repeat the setup procedure above</p>
<pre><code class="language-bash">zkstack containers
zkstack ecosystem init
</code></pre>
</li>
</ul>
<h3 id="run-observability-stack"><a class="header" href="#run-observability-stack">Run observability stack</a></h3>
<p>If you want to run <a href="https://github.com/stefanprodan/dockprom/">Dockprom</a> stack (Prometheus, Grafana) alongside other
containers - add <code>--observability</code> parameter during initialisation.</p>
<pre><code class="language-bash">zkstack containers --observability
</code></pre>
<p>or select <code>yes</code> when prompted during the interactive execution of the command.</p>
<p>That will also provision Grafana with
<a href="https://github.com/matter-labs/era-observability/tree/main/dashboards">era-observability</a> dashboards. You can then
access it at <code>http://127.0.0.1:3000/</code> under credentials <code>admin/admin</code>.</p>
<blockquote>
<p>If you don’t see any data displayed on the Grafana dashboards - try setting the timeframe to “Last 30 minutes”. You
will also have to have <code>jq</code> installed on your system.</p>
</blockquote>
<h2 id="ecosystem-configuration"><a class="header" href="#ecosystem-configuration">Ecosystem Configuration</a></h2>
<p>The ecosystem configuration is spread across multiple files and directories:</p>
<ol>
<li>
<p>Root level:</p>
<ul>
<li><code>ZkStack.yaml</code>: Main configuration file for the entire ecosystem.</li>
</ul>
</li>
<li>
<p><code>configs/</code> directory:</p>
<ul>
<li><code>apps/</code>:
<ul>
<li><code>portal_config.json</code>: Configuration for the portal application.</li>
</ul>
</li>
<li><code>contracts.yaml</code>: Defines smart contract settings and addresses.</li>
<li><code>erc20.yaml</code>: Configuration for ERC20 tokens.</li>
<li><code>initial_deployments.yaml</code>: Specifies initial ERC20 token deployments.</li>
<li><code>wallets.yaml</code>: Contains wallet configurations.</li>
</ul>
</li>
<li>
<p><code>chains/&lt;chain_name&gt;/</code> directory:</p>
<ul>
<li><code>artifacts/</code>: Contains build/execution artifacts.</li>
<li><code>configs/</code>: Chain-specific configuration files.
<ul>
<li><code>contracts.yaml</code>: Chain-specific smart contract settings.</li>
<li><code>external_node.yaml</code>: Configuration for external nodes.</li>
<li><code>general.yaml</code>: General chain configuration.</li>
<li><code>genesis.yaml</code>: Genesis configuration for the chain.</li>
<li><code>secrets.yaml</code>: Secrets and private keys for the chain.</li>
<li><code>wallets.yaml</code>: Wallet configurations for the chain.</li>
</ul>
</li>
<li><code>db/main/</code>: Database files for the chain.</li>
<li><code>ZkStack.yaml</code>: Chain-specific ZkStack configuration.</li>
</ul>
</li>
</ol>
<p>These configuration files are automatically generated during the ecosystem initialization (<code>zkstack ecosystem init</code>) and
chain initialization (<code>zkstack chain init</code>) processes. They control various aspects of the ZKsync ecosystem, including:</p>
<ul>
<li>Network settings</li>
<li>Smart contract deployments</li>
<li>Token configurations</li>
<li>Database settings</li>
<li>Application/Service-specific parameters</li>
</ul>
<p>It’s important to note that while these files can be manually edited, any changes may be overwritten if the ecosystem or
chain is reinitialized. Always back up your modifications and exercise caution when making direct changes to these
files.</p>
<p>For specific configuration needs, it’s recommended to use the appropriate <code>zkstack</code> commands or consult the
documentation for safe ways to customize your setup.</p>
<h2 id="build-and-run-server"><a class="header" href="#build-and-run-server">Build and run server</a></h2>
<p>Run server:</p>
<pre><code class="language-bash">zkstack server
</code></pre>
<p>The server’s configuration files can be found in <code>/chains/&lt;chain_name&gt;/configs</code> directory. These files are created when
running <code>zkstack chain init</code> command.</p>
<h3 id="modifying-configuration-files-manually"><a class="header" href="#modifying-configuration-files-manually">Modifying configuration files manually</a></h3>
<p>To manually modify configuration files:</p>
<ol>
<li>Locate the relevant config file in <code>/chains/&lt;chain_name&gt;/configs</code></li>
<li>Open the file in a text editor</li>
<li>Make necessary changes, following the existing format</li>
<li>Save the file</li>
<li>Restart the relevant services for changes to take effect:</li>
</ol>
<pre><code class="language-bash">zkstack server
</code></pre>
<blockquote>
<p>NOTE: Manual changes to configuration files may be overwritten if the ecosystem is reinitialized or the chain is
reinitialized.</p>
</blockquote>
<blockquote>
<p>WARNING: Some properties, such as ports, may require manual modification across different configuration files to
ensure consistency and avoid conflicts.</p>
</blockquote>
<h2 id="running-server-using-google-cloud-storage-object-store-instead-of-default-in-memory-store"><a class="header" href="#running-server-using-google-cloud-storage-object-store-instead-of-default-in-memory-store">Running server using Google cloud storage object store instead of default In memory store</a></h2>
<p>Get the <code>service_account.json</code> file containing the GCP credentials from kubernetes secret for relevant
environment(stage2/ testnet2) add that file to the default location <code>~/gcloud/service_account.json</code> or update
<code>object_store.toml</code> with the file location</p>
<pre><code class="language-bash">zkstack prover init --bucket-base-url={url} --credentials-file={path/to/service_account.json}
</code></pre>
<h2 id="running-prover-server"><a class="header" href="#running-prover-server">Running prover server</a></h2>
<p>Running on a machine with GPU</p>
<pre><code class="language-bash">zkstack prover run --component=prover
</code></pre>
<blockquote>
<p>NOTE: Running on machine without GPU is currently not supported by <code>zkstack</code>.</p>
</blockquote>
<h2 id="running-the-verification-key-generator"><a class="header" href="#running-the-verification-key-generator">Running the verification key generator</a></h2>
<pre><code class="language-bash"># ensure that the setup_2^26.key in the current directory, the file can be download from  https://storage.googleapis.com/matterlabs-setup-keys-us/setup-keys/setup_2\^26.key

# To generate all verification keys
cargo run --release --bin zksync_verification_key_generator
</code></pre>
<h2 id="generating-binary-verification-keys-for-existing-json-verification-keys"><a class="header" href="#generating-binary-verification-keys-for-existing-json-verification-keys">Generating binary verification keys for existing json verification keys</a></h2>
<pre><code class="language-bash">cargo run --release --bin zksync_json_to_binary_vk_converter -- -o /path/to/output-binary-vk
</code></pre>
<h2 id="generating-commitment-for-existing-verification-keys"><a class="header" href="#generating-commitment-for-existing-verification-keys">Generating commitment for existing verification keys</a></h2>
<pre><code class="language-bash">cargo run --release --bin zksync_commitment_generator
</code></pre>
<h2 id="running-the-contract-verifier"><a class="header" href="#running-the-contract-verifier">Running the contract verifier</a></h2>
<pre><code class="language-bash">zkstack contract-verifier run
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="connection-refused"><a class="header" href="#connection-refused">Connection Refused</a></h3>
<h4 id="problem"><a class="header" href="#problem">Problem</a></h4>
<pre><code class="language-bash">error sending request for url (http://127.0.0.1:8545/): error trying to connect: tcp connect error: Connection refused (os error 61)
</code></pre>
<h4 id="description"><a class="header" href="#description">Description</a></h4>
<p>It appears that no containers are currently running, which is likely the reason you’re encountering this error.</p>
<h4 id="solution"><a class="header" href="#solution">Solution</a></h4>
<p>Ensure that the necessary containers have been started and are functioning correctly to resolve the issue.</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-v2-project-architecture"><a class="header" href="#zksync-v2-project-architecture">ZKsync v2 Project Architecture</a></h1>
<p>This document will help you answer the question: <em>where can I find the logic for x?</em> by giving a directory-tree style
structure of the physical architecture of the ZKsync Era project.</p>
<h2 id="high-level-overview"><a class="header" href="#high-level-overview">High-Level Overview</a></h2>
<p>The zksync-era repository has the following main units:</p>
<p><ins><strong>Smart Contracts:</strong></ins> All the smart contracts in charge of the protocols on the L1 &amp; L2. Some main contracts:</p>
<ul>
<li>L1 &amp; L2 bridge contracts.</li>
<li>The ZKsync rollup contract on Ethereum.</li>
<li>The L1 proof verifier contract.</li>
</ul>
<p><strong><ins>Core App:</strong></ins> The execution layer. A node running the ZKsync network in charge of the following components:</p>
<ul>
<li>Monitoring the L1 smart contract for deposits or priority operations.</li>
<li>Maintaining a mempool that receives transactions.</li>
<li>Picking up transactions from the mempool, executing them in a VM, and changing the state accordingly.</li>
<li>Generating ZKsync chain blocks.</li>
<li>Preparing circuits for executed blocks to be proved.</li>
<li>Submitting blocks and proofs to the L1 smart contract.</li>
<li>Exposing the Ethereum-compatible web3 API.</li>
</ul>
<p><strong><ins>Prover App:</strong></ins> The prover app takes blocks and metadata generated by the server and constructs a validity zk
proof for them.</p>
<p><strong><ins>Storage Layer:</strong></ins> The different components and subcomponents don’t communicate with each other directly via
APIs, rather via the single source of truth – the db storage layer.</p>
<h2 id="low-level-overview"><a class="header" href="#low-level-overview">Low-Level Overview</a></h2>
<p>This section provides a physical map of folders &amp; files in this repository. It doesn’t aim to be complete, it only shows
the most important parts.</p>
<ul>
<li>
<p><code>/contracts</code>: A submodule with L1, L2, and system contracts. See
<a href="https://github.com/matter-labs/era-contracts/">repository</a>.</p>
</li>
<li>
<p><code>/core</code></p>
<ul>
<li>
<p><code>/bin</code>: Executables for the microservices components comprising ZKsync Core Node.</p>
<ul>
<li><code>/zksync_server</code>: Main sequencer implementation.</li>
<li><code>/external_node</code>: A read replica that can sync from the main node.</li>
<li><code>/tee_prover</code>: Implementation of the TEE prover.</li>
</ul>
</li>
<li>
<p><code>/node</code>: Composable node parts.</p>
<ul>
<li><code>/node_framework</code>: Framework used to compose parts of the node.</li>
<li><code>/api_server</code>: Implementation of Web3 JSON RPC server.</li>
<li><code>/base_token_adjuster</code>: Adaptor to support custom (non-ETH) base tokens.</li>
<li><code>/block_reverter</code>: Component for reverting L2 blocks and L1 batches.</li>
<li><code>/commitment_generator</code>: Component for calculation of commitments required for ZKP generation.</li>
<li><code>/consensus</code>: p2p utilities.</li>
<li><code>/consistency_checker</code>: Security component for the external node.</li>
<li><code>/da_clients</code>: Clients for different data availability solutions.</li>
<li><code>/da_dispatcher</code>: Adaptor for alternative DA solutions.</li>
<li><code>/eth_sender</code>: Component responsible for submitting batches to L1 contract.</li>
<li><code>/eth_watch</code>: Component responsible for retrieving data from the L1 contract.</li>
<li><code>/fee_model</code>: Fee logic implementation.</li>
<li><code>/genesis</code>: Logic for performing chain genesis.</li>
<li><code>/metadata_calculator</code>: Component responsible for Merkle tree maintenance.</li>
<li><code>/node_storage_init</code>: Strategies for the node initialization.</li>
<li><code>/node_sync</code>: Node synchronization for the external node.</li>
<li><code>/proof_data_handler</code>: Gateway API for interaction with the prover subsystem.</li>
<li><code>/reorg_detector</code>: Component responsible for detecting reorgs on the external node.</li>
<li><code>/state_keeper</code>: Main part of the sequencer, responsible for forming blocks and L1 batches.</li>
<li><code>/vm_runner</code>: Set of components generating various data by re-running sealed L1 batches.</li>
</ul>
</li>
<li>
<p><code>/lib</code>: All the library crates used as dependencies of the binary crates above.</p>
<ul>
<li><code>/basic_types</code>: Crate with essential ZKsync primitive types.</li>
<li><code>/config</code>: All the configuration values used by the different ZKsync apps.</li>
<li><code>/contracts</code>: Contains definitions of commonly used smart contracts.</li>
<li><code>/crypto_primitives</code>: Cryptographical primitives used by the different ZKsync crates.</li>
<li><code>/dal</code>: Data availability layer
<ul>
<li><code>/migrations</code>: All the db migrations applied to create the storage layer.</li>
<li><code>/src</code>: Functionality to interact with the different db tables.</li>
</ul>
</li>
<li><code>/db_connection</code>: Generic DB interface.</li>
<li><code>/eth_client</code>: Module providing an interface to interact with an Ethereum node.</li>
<li><code>/eth_signer</code>: Module to sign messages and txs.</li>
<li><code>/mempool</code>: Implementation of the ZKsync transaction pool.</li>
<li><code>/merkle_tree</code>: Implementation of a sparse Merkle tree.</li>
<li><code>/mini_merkle_tree</code>: In-memory implementation of a sparse Merkle tree.</li>
<li><code>/multivm</code>: A wrapper over several versions of VM that have been used by the main node.</li>
<li><code>/object_store</code>: Abstraction for storing blobs outside the main data store.</li>
<li><code>/queued_job_processor</code>: An abstraction for async job processing</li>
<li><code>/state</code>: A state keeper responsible for handling transaction execution and creating miniblocks and L1 batches.</li>
<li><code>/storage</code>: An encapsulated database interface.</li>
<li><code>/test_account</code>: A representation of ZKsync account.</li>
<li><code>/types</code>: ZKsync network operations, transactions, and common types.</li>
<li><code>/utils</code>: Miscellaneous helpers for ZKsync crates.</li>
<li><code>/vlog</code>: ZKsync observability stack.</li>
<li><code>/vm_interface</code>: Generic interface for ZKsync virtual machine.</li>
<li><code>/web3_decl</code>: Declaration of the Web3 API.</li>
</ul>
</li>
<li>
<p><code>/tests</code>: Testing infrastructure for ZKsync network.</p>
<ul>
<li><code>/loadnext</code>: An app for load testing the ZKsync server.</li>
<li><code>/ts-integration</code>: Integration tests set implemented in TypeScript.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>/prover</code>: ZKsync prover orchestrator application.</p>
</li>
<li>
<p><code>/docker</code>: Project docker files.</p>
</li>
<li>
<p><code>/bin</code> &amp; <code>/infrastructure</code>: Infrastructure scripts that help to work with ZKsync applications.</p>
</li>
<li>
<p><code>/etc</code>: Configuration files.</p>
<ul>
<li><code>/env</code>:<code>.env</code> files that contain environment variables for different configurations of ZKsync Server / Prover.</li>
</ul>
</li>
<li>
<p><code>/keys</code>: Verification keys for <code>circuit</code> module.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="build-docker-images"><a class="header" href="#build-docker-images">Build docker images</a></h1>
<p>This document explains how to build Docker images from the source code, instead of using prebuilt ones we distribute</p>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<p>Install prerequisites: see</p>
<p><a href="guides/./setup-dev.html">Installing dependencies</a></p>
<h2 id="build-docker-files"><a class="header" href="#build-docker-files">Build docker files</a></h2>
<p>You may build all images with <a href="guides/../../docker/Makefile">Makefile</a> located in <a href="guides/../../docker">docker</a> directory in this
repository</p>
<blockquote>
<p>All commands should be run from the root directory of the repository</p>
</blockquote>
<pre><code class="language-shell">make -C ./docker build-all
</code></pre>
<p>You will get those images:</p>
<pre><code class="language-shell">contract-verifier:2.0
server-v2:2.0
prover:2.0
witness-generator:2.0
external-node:2.0
</code></pre>
<p>Alternatively, you may build only needed components - available targets are</p>
<pre><code class="language-shell">make -C ./docker build-contract-verifier
make -C ./docker build-server-v2
make -C ./docker build-circuit-prover-gpu
make -C ./docker build-witness-generator
make -C ./docker build-external-node
</code></pre>
<h2 id="building-updated-images"><a class="header" href="#building-updated-images">Building updated images</a></h2>
<p>Simply run</p>
<pre><code class="language-shell">make -C ./docker clean-all
make -C ./docker build-all
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repositories"><a class="header" href="#repositories">Repositories</a></h1>
<h2 id="zksync"><a class="header" href="#zksync">ZKsync</a></h2>
<h3 id="core-components"><a class="header" href="#core-components">Core components</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync-era">zksync-era</a></td><td>zk server logic, including the APIs and database accesses</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-wallet-vue">zksync-wallet-vue</a></td><td>Wallet frontend</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-contracts">era-contracts</a></td><td>L1 &amp; L2 contracts, that are used to manage bridges and communication between L1 &amp; L2. Privileged contracts that are running on L2 (like Bootloader or ContractDeployer)</td></tr>
</tbody></table>
</div>
<h3 id="compiler"><a class="header" href="#compiler">Compiler</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-compiler-tester">era-compiler-tester</a></td><td>Integration testing framework for running executable tests on zkEVM</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-tests">era-compiler-tests</a></td><td>Collection of executable tests for zkEVM</td></tr>
<tr><td><a href="https://github.com/matter-labs//era-compiler-llvm">era-compiler-llvm</a></td><td>zkEVM fork of the LLVM framework</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-solidity">era-compiler-solidity</a></td><td>Solidity Yul/EVMLA compiler front end</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-vyper">era-compiler-vyper</a></td><td>Vyper LLL compiler front end</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-llvm-context">era-compiler-llvm-context</a></td><td>LLVM IR generator logic shared by multiple front ends</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-common">era-compiler-common</a></td><td>Common compiler constants</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-llvm-builder">era-compiler-llvm-builder</a></td><td>Tool for building our fork of the LLVM framework</td></tr>
</tbody></table>
</div>
<h3 id="zkevm--crypto"><a class="header" href="#zkevm--crypto">zkEVM / crypto</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_opcode_defs">era-zkevm_opcode_defs</a></td><td>Opcode definitions for zkEVM - main dependency for many other repos</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zk_evm">era-zk_evm</a></td><td>EVM implementation in pure rust, without circuits</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-sync_vm">era-sync_vm</a></td><td>EVM implementation using circuits</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkEVM-assembly">era-zkEVM-assembly</a></td><td>Code for parsing zkEVM assembly</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_test_harness">era-zkevm_test_harness</a></td><td>Tests that compare the two implementation of the zkEVM - the non-circuit one (zk_evm) and the circuit one (sync_vm)</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_tester">era-zkevm_tester</a></td><td>Assembly runner for zkEVM testing</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-boojum">era-boojum</a></td><td>New proving system library - containing gadgets and gates</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-shivini">era-shivini</a></td><td>Cuda / GPU implementation for the new proving system</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_circuits">era-zkevm_circuits</a></td><td>Circuits for the new proving system</td></tr>
<tr><td><a href="https://github.com/matter-labs/franklin-crypto">franklin-crypto</a></td><td>Gadget library for the Plonk / plookup</td></tr>
<tr><td><a href="https://github.com/matter-labs/rescue-poseidon">rescue-poseidon</a></td><td>Library with hash functions used by the crypto repositories</td></tr>
<tr><td><a href="https://github.com/matter-labs/snark-wrapper">snark-wrapper</a></td><td>Circuit to wrap the final FRI proof into snark for improved efficiency</td></tr>
</tbody></table>
</div>
<h4 id="old-proving-system"><a class="header" href="#old-proving-system">Old proving system</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-bellman-cuda">era-bellman-cuda</a></td><td>Cuda implementations for cryptographic functions used by the prover</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-heavy-ops-service">era-heavy-ops-service</a></td><td>Main circuit prover that requires GPU to run</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-circuit_testing">era-circuit_testing</a></td><td>??</td></tr>
</tbody></table>
</div>
<h3 id="tools--contract-developers"><a class="header" href="#tools--contract-developers">Tools &amp; contract developers</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-test-node">era-test-node</a></td><td>In memory node for development and smart contract debugging</td></tr>
<tr><td><a href="https://github.com/matter-labs/local-setup">local-setup</a></td><td>Docker-based zk server (together with L1), that can be used for local testing</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-cli">zksync-cli</a></td><td>Command line tool to interact with ZKsync</td></tr>
<tr><td><a href="https://github.com/matter-labs/block-explorer">block-explorer</a></td><td>Online blockchain browser for viewing and analyzing ZKsync chain</td></tr>
<tr><td><a href="https://github.com/matter-labs/dapp-portal">dapp-portal</a></td><td>ZKsync Wallet + Bridge DApp</td></tr>
<tr><td><a href="https://github.com/matter-labs/hardhat-zksync">hardhat-zksync</a></td><td>ZKsync Hardhat plugins</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksolc-bin">zksolc-bin</a></td><td>solc compiler binaries</td></tr>
<tr><td><a href="https://github.com/matter-labs/zkvyper-bin">zkvyper-bin</a></td><td>vyper compiler binaries</td></tr>
</tbody></table>
</div>
<h3 id="examples--documentation"><a class="header" href="#examples--documentation">Examples &amp; documentation</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync-docs">zksync-web-era-docs</a></td><td><a href="https://docs.zksync.io">Public ZKsync documentation</a>, API descriptions etc.</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-contract-templates">zksync-contract-templates</a></td><td>Quick contract deployment and testing with tools like Hardhat on Solidity or Vyper</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-frontend-templates">zksync-frontend-templates</a></td><td>Rapid UI development with templates for Vue, React, Next.js, Nuxt, Vite, etc.</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-scripting-templates">zksync-scripting-templates</a></td><td>Automated interactions and advanced ZKsync operations using Node.js</td></tr>
<tr><td><a href="https://github.com/matter-labs/tutorials">tutorials</a></td><td>Tutorials for developing on ZKsync</td></tr>
</tbody></table>
</div>
<h2 id="zksync-lite"><a class="header" href="#zksync-lite">ZKsync Lite</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync">zksync</a></td><td>ZKsync Lite implementation</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-lite-docs">ZKsync-lite-docs</a></td><td>Public ZKsync Lite documentation</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-dapp-checkout">zksync-dapp-checkout</a></td><td>Batch payments DApp</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-advanced-guides"><a class="header" href="#zksync-advanced-guides">ZKsync advanced guides</a></h1>
<p>This section contains more advanced guides that aim to explain complex internals of ZKsync ecosystem in an easy to grasp
way.</p>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="guides/advanced/./01_initialization.html">Local initialization</a></li>
<li><a href="guides/advanced/./02_deposits.html">Deposits</a></li>
<li><a href="guides/advanced/./03_withdrawals.html">Withdrawals</a></li>
<li><a href="guides/advanced/./04_contracts.html">Contracts</a></li>
<li><a href="guides/advanced/./05_how_call_works.html">Calls</a></li>
<li><a href="guides/advanced/./06_how_transaction_works.html">Transactions</a></li>
<li><a href="guides/advanced/./07_fee_model.html">Fee model</a></li>
<li><a href="guides/advanced/./08_how_l2_messaging_works.html">L2 messaging</a></li>
<li><a href="guides/advanced/./09_pubdata.html">Pubdata</a></li>
<li><a href="guides/advanced/./10_pubdata_with_blobs.html">Pubdata with blobs</a></li>
<li><a href="guides/advanced/./11_compression.html">Bytecode compression</a></li>
<li><a href="guides/advanced/./12_alternative_vm_intro.html">EraVM intro</a></li>
<li><a href="guides/advanced/./13_zk_intuition.html">ZK intuition</a></li>
<li><a href="guides/advanced/./14_zk_deeper_overview.html">ZK deeper overview</a></li>
<li><a href="guides/advanced/./15_prover_keys.html">Prover keys</a></li>
<li><a href="guides/advanced/./16_decentralization.html">Decentralization</a></li>
</ul>
<p>Additionally, there are a few articles that cover specific topics that may be useful for developers actively working on
<code>zksync-era</code> repo:</p>
<ul>
<li><a href="guides/advanced/./90_advanced_debugging.html">Advanced debugging</a></li>
<li><a href="guides/advanced/./91_docker_and_ci.html">Docker and CI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-deeper-dive"><a class="header" href="#zksync-deeper-dive">ZKsync Deeper Dive</a></h1>
<p>The goal of this doc is to show you some more details on how ZKsync works internally.</p>
<p>Please do the dev_setup.md and development.md (these commands do all the heavy lifting on starting the components of the
system).</p>
<p>Now let’s take a look at what’s inside:</p>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<p>Let’s take a deeper look into what <code>zkstack ecosystem init</code> does.</p>
<h4 id="zk-stack-cli"><a class="header" href="#zk-stack-cli">ZK Stack CLI</a></h4>
<p><code>zkstack</code> itself is implemented in Rust (you can see the code in <code>/zkstack_cli</code> directory). If you change anything
there, make sure to run <code>zkstackup --local</code> from the root folder (that compiles and installs this code), before
re-running any <code>zkstack</code> command.</p>
<h4 id="containers"><a class="header" href="#containers">Containers</a></h4>
<p>The first step to initialize a ZK Stack ecosystem is to run the command <code>zkstack containers</code>. This command gets the
docker images for <code>postgres</code> and <code>reth</code>. If the <code>--observability</code> option is passed to the command, or the corresponding
option is selected in the interactive prompt, then Prometheus, Grafana and other observability-related images are
downloaded and run.</p>
<p>Reth (one of the Ethereum clients) will be used to setup our own copy of L1 chain (that our local ZKsync would use).</p>
<p>Postgres is one of the two databases, that is used by ZKsync (the other one is RocksDB). Currently most of the data is
stored in postgres (blocks, transactions etc) - while RocksDB is only storing the state (Tree &amp; Map) - and it used by
VM.</p>
<h4 id="ecosystem"><a class="header" href="#ecosystem">Ecosystem</a></h4>
<p>The next step is to run the command <code>zkstack ecosystem init</code>.</p>
<p>This command:</p>
<ul>
<li>Collects and finalize the ecosystem configuration.</li>
<li>Builds and deploys L1 &amp; L2 contracts.</li>
<li>Initializes each chain defined in the <code>/chains</code> folder. (Currently, a single chain <code>era</code> is defined there, but you can
create your own chains running <code>zkstack chain create</code>).</li>
<li>Sets up observability.</li>
<li>Runs the genesis process.</li>
<li>Initializes the database.</li>
</ul>
<h4 id="postgres"><a class="header" href="#postgres">Postgres</a></h4>
<p>First - postgres database: you’ll be able to see something like</p>
<pre><code>DATABASE_URL = postgres://postgres:notsecurepassword@localhost/zksync_local
</code></pre>
<p>After which we setup the schema (lots of lines with <code>Applied XX</code>).</p>
<p>You can try connecting to postgres now, to see what’s inside:</p>
<pre><code class="language-shell">psql postgres://postgres:notsecurepassword@localhost/zksync_local
</code></pre>
<p>(and then commands like <code>\dt</code> to see the tables, <code>\d TABLE_NAME</code> to see the schema, and <code>select * from XX</code> to see the
contents).</p>
<p>As our network has just started, the database would be quite empty.</p>
<p>You can see the schema for the database in <a href="guides/advanced/../../../core/lib/dal/README.html">dal/README.md</a> TODO: add the link to the
document with DB schema.</p>
<h4 id="docker-1"><a class="header" href="#docker-1">Docker</a></h4>
<p>We’re running two things in a docker:</p>
<ul>
<li>a postgres (that we’ve covered above)</li>
<li>a reth (that is the L1 Ethereum chain).</li>
</ul>
<p>Let’s see if they are running:</p>
<pre><code class="language-shell">docker container ls
</code></pre>
<p>and then we can look at the Reth logs:</p>
<pre><code class="language-shell">docker logs zksync-era-reth-1
</code></pre>
<p>Where <code>zksync-era-reth-1</code> is the container name, that we got from the first command.</p>
<p>If everything goes well, you should see that L1 blocks are being produced.</p>
<h4 id="server"><a class="header" href="#server">Server</a></h4>
<p>Now we can start the main server:</p>
<pre><code class="language-bash">zkstack server
</code></pre>
<p>This will actually run a cargo binary (<code>zksync_server</code>).</p>
<p>The server will wait for the new transactions to generate the blocks (these can either be sent via JSON RPC, but it also
listens on the logs from the L1 contract - as things like token bridging etc comes from there).</p>
<p>Currently we don’t send any transactions there (so the logs might be empty).</p>
<p>But you should see some initial blocks in postgres:</p>
<pre><code class="language-sql">select * from miniblocks;
</code></pre>
<h4 id="our-l1-reth"><a class="header" href="#our-l1-reth">Our L1 (reth)</a></h4>
<p>Let’s finish this article, by taking a look at our L1:</p>
<p>We will use the <code>web3</code> tool to communicate with the L1, have a look at <a href="guides/advanced/02_deposits.html">02_deposits.md</a> for installation
instructions. You can check that you’re a (localnet) crypto trillionaire, by running:</p>
<pre><code class="language-bash">./web3 --rpc-url http://localhost:8545 balance 0x36615Cf349d7F6344891B1e7CA7C72883F5dc049
</code></pre>
<p>This is one of the “rich wallets” we predefined for local L1.</p>
<p><strong>Note:</strong> This reth shell is running official Ethereum JSON RPC with Reth-specific extensions documented at
<a href="https://paradigmxyz.github.io/reth/jsonrpc/intro.html">reth docs</a></p>
<p>In order to communicate with L2 (our ZKsync) - we have to deploy multiple contracts onto L1 (our local reth created
Ethereum). You can look on the <code>deployL1.log</code> file - to see the list of contracts that were deployed and their accounts.</p>
<p>First thing in the file, is the deployer/governor wallet - this is the account that can change, freeze and unfreeze the
contracts (basically the owner). You can verify the token balance using the <code>getBalance</code> method above.</p>
<p>Then, there are a bunch of contracts (CRATE2_FACTOR, DIAMOND_PROXY, L1_ALLOW_LIST etc etc) - for each one, the file
contains the address.</p>
<p>You can quickly verify that they were really deployed, by calling:</p>
<pre><code class="language-bash">./web3 --rpc-url http://localhost:8545 address XXX
</code></pre>
<p>Where XXX is the address in the file.</p>
<p>The most important one of them is CONTRACTS_DIAMOND_PROXY_ADDR (which acts as ‘loadbalancer/router’ for others - and
this is the contract that our server is ‘listening’ on).</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Ok - so let’s sum up what we have:</p>
<ul>
<li>a postgres running in docker (main database)</li>
<li>a local instance of ethereum (reth running in docker)
<ul>
<li>which also has a bunch of ‘magic’ contracts deployed</li>
<li>and two accounts with lots of tokens</li>
</ul>
</li>
<li>and a server process</li>
</ul>
<p>In the <a href="guides/advanced/02_deposits.html">next article</a>, we’ll start playing with the system (bridging tokens etc).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zk-sync-deeper-dive---bridging--deposits"><a class="header" href="#zk-sync-deeper-dive---bridging--deposits">ZK-Sync deeper dive - bridging &amp; deposits</a></h1>
<p>In the <a href="guides/advanced/01_initialization.html">first article</a>, we’ve managed to setup our system on local machine and verify that it
works. Now let’s actually start using it.</p>
<h2 id="seeing-the-status-of-the-accounts"><a class="header" href="#seeing-the-status-of-the-accounts">Seeing the status of the accounts</a></h2>
<p>Let’s use a small command line tool (web3 - <a href="https://github.com/mm-zk/web3">https://github.com/mm-zk/web3</a>) to interact with our blockchains.</p>
<pre><code class="language-shell">git clone https://github.com/mm-zk/web3
make build
</code></pre>
<p>Then let’s create the keypair for our temporary account:</p>
<pre><code class="language-shell">./web3 account create
</code></pre>
<p>It will produce a public and private key (for example):</p>
<pre><code>Private key: 0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6
Public address: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p><strong>NOTE:</strong> Keep track of this key and address, as they will be constantly used throughout these articles</p>
<p>Now, let’s see how many tokens we have:</p>
<pre><code class="language-shell">// This checks the tokens on 'L1' (reth)
./web3 --rpc-url http://localhost:8545 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd

// This checks the tokens on 'L2' (ZKsync)
./web3 --rpc-url http://localhost:3050 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>Unsurprisingly we have 0 on both - let’s fix it by first transferring some tokens on L1:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 transfer --pk 0x7726827caac94a7f9e1b160f7ea819f172f7b6f9d2a97f992c38edeab82d4110 7.4 to 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>And now when we check the balance, we should see:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>that we have 7.4 ETH.</p>
<p>and now let’s bridge it over to L2.</p>
<h2 id="bridging-over-to-l2"><a class="header" href="#bridging-over-to-l2">Bridging over to L2</a></h2>
<p>For an easy way to bridge we’ll use <a href="https://github.com/matter-labs/zksync-cli">ZKsync CLI</a></p>
<pre><code class="language-shell">npx zksync-cli bridge deposit --chain=dockerized-node --amount 3 --pk=0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6 --to=0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
# Amount of ETH to deposit: 3
# Private key of the sender: 0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6
# Recipient address on L2: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>If everything goes well, you should be able to see 3 tokens transferred:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:3050 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<h3 id="diving-deeper---what-exactly-happened"><a class="header" href="#diving-deeper---what-exactly-happened">Diving deeper - what exactly happened</a></h3>
<p>Let’s take a deeper look at what the ‘deposit’ call actually did.</p>
<p>If we look at what ‘deposit’ command has printed, we’ll see something like this:</p>
<pre><code>Transaction submitted 💸💸💸
[...]/tx/0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
</code></pre>
<p>Let’s use the web3 tool and see the details:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 tx --input hex 0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
</code></pre>
<p>returns</p>
<pre><code>Hash: 0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
From: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
To: 0xa6Bcd8124d42293D3DDFAE6003940A62D8C280F2
Value: 3.000120034768750000 GO
Nonce: 0
Gas Limit: 134871
Gas Price: 1.500000001 gwei
Block Number: 100074
Block Hash: 0x5219e6fef442b4cfd38515ea7119dd6d2e12df82b4d95b1f75fd3650c012f133
Input: 0xeb672419000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd00000000000000000000000000000000000000000000000029a2241af62c000000000000000000000000000000000000000000000000000000000000000000e0000000000000000000000000000000000000000000000000000000000006d0b100000000000000000000000000000000000000000000000000000000000003200000000000000000000000000000000000000000000000000000000000000100000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
</code></pre>
<p>The deposit command has called the contract on address <code>0xa6B</code> (which is exactly the <code>CONTRACTS_DIAMOND_PROXY_ADDR</code> from
<code>deployL1.log</code>), and it has called the method <code>0xeb672419</code> - which is the <code>requestL2Transaction</code> from
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/facets/Mailbox.sol#L220">Mailbox.sol</a></p>
<h4 id="quick-note-on-our-l1-contracts"><a class="header" href="#quick-note-on-our-l1-contracts">Quick note on our L1 contracts</a></h4>
<p>We’re using the DiamondProxy setup, that allows us to have a fixed immutable entry point (DiamondProxy) - that forwards
the requests to different contracts (facets) that can be independently updated and/or frozen.</p>
<p><img src="https://user-images.githubusercontent.com/128217157/229521292-1532a59b-665c-4cc4-8342-d25ad45a8fcd.png" alt="Diamond proxy layout" /></p>
<p>You can find more detailed description in
<a href="https://github.com/matter-labs/era-contracts/blob/main/docs/Overview.md">Contract docs</a></p>
<h4 id="requestl2transaction-function-details"><a class="header" href="#requestl2transaction-function-details">requestL2Transaction Function details</a></h4>
<p>You can use some of the online tools (like <a href="https://calldata-decoder.apoorv.xyz/">https://calldata-decoder.apoorv.xyz/</a>) and pass the input data to it - and
get the nice result:</p>
<pre><code class="language-json">"function": "requestL2Transaction(address,uint256,bytes,uint256,uint256,bytes[],address)",
"params": [
    "0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd",
    "3000000000000000000",
    "0x",
    "641858",
    "800",
    [],
    "0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd"
  ]

</code></pre>
<p>This means that we requested that the 3 ETH (2nd argument) is transferred to 0x6182 (1st argument). The Calldata being
0x0 - means that we’re talking about ETH (this would be a different value for other ERC tokens). Then we also specify a
gas limit (641k) and set the gas per pubdata byte limit to 800. (TODO: explain what these values mean.)</p>
<h4 id="what-happens-under-the-hood"><a class="header" href="#what-happens-under-the-hood">What happens under the hood</a></h4>
<p>The call to requestL2Transaction, is adding the transaction to the priorityQueue and then emits the NewPriorityRequest.</p>
<p>The zk server (that you started with <code>zk server</code> command) is listening on events that are emitted from this contract
(via the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/eth_watch/"><code>eth_watcher</code> component</a>) and adds
them to the postgres database (into <code>transactions</code> table).</p>
<p>You can actually check it - by running the psql and looking at the contents of the table - then you’ll notice that
transaction was successfully inserted, and it was also marked as ‘priority’ (as it came from L1) - as regular
transactions that are received by the server directly are not marked as priority.</p>
<p>You can verify that this is your transaction, by looking at the <code>l1_block_number</code> column (it should match the
<code>block_number</code> from the <code>web3 tx</code> call above).</p>
<p>Notice that the hash of the transaction in the postgres will be different from the one returned by <code>web3 tx</code>. This is
because the postgres keeps the hash of the ‘L2’ transaction (which was ‘inside’ the L1 transaction that <code>web3 tx</code>
returned).</p>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>In this article, we’ve learned how ETH gets bridged from L1 to L2. In the <a href="guides/advanced/03_withdrawals.html">next article</a>, we’ll look
at the other direction - how we transmit messages (and ETH) from L2 to L1.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-deeper-dive-bridging-stuff-back-aka-withdrawals"><a class="header" href="#zksync-deeper-dive-bridging-stuff-back-aka-withdrawals">ZKsync deeper dive bridging stuff back (a.k.a withdrawals)</a></h1>
<p>Assuming that you have completed <a href="guides/advanced/01_initialization.html">part 1</a> and <a href="guides/advanced/02_deposits.html">part 2</a> already, we can bridge the
tokens back by simply calling the zksync-cli:</p>
<pre><code class="language-bash">npx zksync-cli bridge withdraw --chain=dockerized-node
</code></pre>
<p>And providing the account name (public address) and private key.</p>
<p>Afterward, by using <code>web3</code> tools, we can quickly check that funds were transferred back to L1. <strong>And you discover that
they didn’t</strong> - what happened?</p>
<p>Actually we’ll have to run one additional step:</p>
<pre><code class="language-bash">npx zksync-cli bridge withdraw-finalize --chain=dockerized-node
</code></pre>
<p>and pass the transaction that we received from the first call, into the <code>withdraw-finalize</code> call.</p>
<p><strong>Note:</strong> This is not needed on testnet - as we (MatterLabs) - are running an automatic tool that confirms withdrawals.</p>
<h3 id="looking-deeper"><a class="header" href="#looking-deeper">Looking deeper</a></h3>
<p>But let’s take a look what happened under the hood.</p>
<p>Let’s start by looking at the output of our <code>zksync-cli</code>:</p>
<pre><code>Withdrawing 7ETH to 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd on localnet
Transaction submitted 💸💸💸
L2: tx/0xe2c8a7beaf8879cb197555592c6eb4b6e4c39a772c3b54d1b93da14e419f4683
Your funds will be available in L1 in a couple of minutes.
</code></pre>
<p><strong>important</strong> - your transaction id will be different - make sure that you use it in the methods below.</p>
<p>The tool created the withdraw transaction and it sent it directly to our server (so this is a L2 transaction). The zk
server has received it, and added it into its database. You can check it by querying the <code>transactions</code> table:</p>
<pre><code class="language-shell"># select * from transactions where hash = '\x&lt;YOUR_L2_TRANSACTION_ID_FROM_ABOVE&gt;`
select * from transactions where hash = '\xe2c8a7beaf8879cb197555592c6eb4b6e4c39a772c3b54d1b93da14e419f4683';
</code></pre>
<p>This will print a lot of columns, but let’s start by looking at the <code>data</code> column:</p>
<pre><code class="language-json">{
  "value": "0x6124fee993bc0000",
  "calldata": "0x51cff8d9000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd",
  "factoryDeps": null,
  "contractAddress": "0x000000000000000000000000000000000000800a"
}
</code></pre>
<p>We can use the ABI decoder tool <a href="https://calldata-decoder.apoorv.xyz/">https://calldata-decoder.apoorv.xyz/</a> to see what this call data means:</p>
<pre><code class="language-json">{
  "function": "withdraw(address)",
  "params": ["0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd"]
}
</code></pre>
<p>(and the 0x6124fee993bc0000 in the value is 7000000000000000000 == 7 ETH that we wanted to send).</p>
<p>So the last question is – what is the ‘magic’ contract address: <code>0x800a</code> ?</p>
<pre><code class="language-solidity">/// @dev The address of the eth token system contract
address constant L2_BASE_TOKEN_SYSTEM_CONTRACT_ADDR = address(0x800a);

</code></pre>
<h3 id="system-contracts-on-l2"><a class="header" href="#system-contracts-on-l2">System contracts (on L2)</a></h3>
<p>This is a good opportunity to talk about system contracts that are automatically deployed on L2. You can find the full
list here
<a href="https://github.com/matter-labs/era-system-contracts/blob/436d57da2fb35c40e38bcb6637c3a090ddf60701/scripts/constants.ts#L29">in github</a></p>
<p>This is the place where we specify that <code>bootloader</code> is at address 0x8001, <code>NonceHolder</code> at 0x8003 etc.</p>
<p>This brings us to
<a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/L2EthToken.sol">L2BaseToken.sol</a> that has the
implementation of the L2 Eth.</p>
<p>When we look inside, we can see:</p>
<pre><code class="language-solidity">// Send the L2 log, a user could use it as proof of the withdrawal
bytes memory message = _getL1WithdrawMessage(_l1Receiver, amount);
L1_MESSENGER_CONTRACT.sendToL1(message);
</code></pre>
<p>And <code>L1MessengerContract</code> (that is deployed at 0x8008).</p>
<h3 id="committing-to-l1"><a class="header" href="#committing-to-l1">Committing to L1</a></h3>
<p>And how do these messages get into the L1? The <code>eth_sender</code> class from our server is taking care of this. You can see
the details of the transactions that it posts to L1 in our database in <code>eth_txs</code> table.</p>
<p>If you look at the <code>tx_type</code> column (in psql), you can see that we have 3 different transaction types:</p>
<pre><code class="language-sql">zksync_local=# select contract_address, tx_type from eth_txs;
              contract_address              |          tx_type
--------------------------------------------+---------------------------
 0x54e8159f006750466084913d5bd288d4afb1ee9a | CommitBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | PublishProofBlocksOnchain
 0x54e8159f006750466084913d5bd288d4afb1ee9a | ExecuteBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | CommitBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | PublishProofBlocksOnchain
 0x54e8159f006750466084913d5bd288d4afb1ee9a | ExecuteBlocks
</code></pre>
<p>BTW - all the transactions are sent to the 0x54e address - which is the <code>DiamondProxy</code> deployed on L1 (this address will
be different on your local node - see previous tutorial for more info) .</p>
<p>And inside, all three methods above belong to
<a href="https://github.com/matter-labs/era-contracts/blob/dev/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol">Executor.sol</a>
facet and you can look at
<a href="https://github.com/matter-labs/era-contracts/blob/main/docs/Overview.md#executorfacet">README</a> to see the details of
what each method does.</p>
<p>The short description is:</p>
<ul>
<li>‘CommitBlocks’ - is verifying the block metadata and stores the hash into the L1 contract storage.</li>
<li>‘PublishProof’ - gets the proof, checks that the proof is correct and that it is a proof for the block hash that was
stored in commit blocks. (IMPORTANT: in testnet/localnet we allow empty proofs - so that you don’t have to run the
full prover locally)</li>
<li>‘ExecuteBlocks’ - is the final call, that stores the root hashes in L1 storage. This allows other calls (like
finalizeWithdrawal) to work.</li>
</ul>
<p>So to sum it up - after these 3 calls, the L1 contract has a root hash of a merkle tree, that contains the ‘message’
about the withdrawal.</p>
<h3 id="final-step---finalizing-withdrawal"><a class="header" href="#final-step---finalizing-withdrawal">Final step - finalizing withdrawal</a></h3>
<p>Now we’re ready to actually claim our ETH on L1. We do this by calling a <code>finalizeEthWithdrawal</code> function on the
DiamondProxy contract (Mailbox.sol to be exact).</p>
<p>To prove that we actually can withdraw the money, we have to say in which L2 block the withdrawal happened, and provide
the merkle proof from our withdrawal log, to the root that is stored in the L1 contract.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-contracts"><a class="header" href="#zksync-contracts">ZKsync contracts</a></h1>
<p>Now that we know how to bridge tokens back and forth, let’s talk about running things on ZKsync.</p>
<p>We have a bunch of great tutorials (like this one <a href="https://docs.zksync.io/build/tooling/hardhat/getting-started">https://docs.zksync.io/build/tooling/hardhat/getting-started</a>) that
you can follow to get the exact code &amp; command line calls to create the contracts - so in this article, let’s focus on
how things differ between ZKsync and Ethereum.</p>
<p><strong>Note</strong> Before reading this article, I’d recommend doing the hardhat tutorial above.</p>
<h2 id="ethereum-flow"><a class="header" href="#ethereum-flow">Ethereum flow</a></h2>
<p>In case of Ethereum, you start by writing a contract code in solidity, then you compile it with <code>solc</code>, and you get the
EVM bytecode, deployment bytecode (which is a function that should return the bytecode itself) and ABI (interface).</p>
<p>Afterwards, you send the deployment bytecode to the 0x000 address on Ethereum, which does some magic (executes the
deployment bytecode, that should contain the constructor etc) and puts the contract under the address that is generated
based on your account id and a nonce.</p>
<p>From this moment on, you can send the transactions to this new address (and most of the tools would ask you to provide
the ABI, so that they can set the proper function arguments).</p>
<p>All the bytecode will be run on the EVM (Ethereum Virtual Machine) - that has a stack, access to memory and storage, and
a bunch of opcodes.</p>
<h2 id="zksync-flow"><a class="header" href="#zksync-flow">ZKsync flow</a></h2>
<p>The main part (and the main cost) of the ZKsync is the proving system. In order to make proof as fast as possible, we’re
running a little bit different virtual machine (zkEVM) - that has a slightly different set of opcodes, and also contains
a bunch of registers. More details on this will be written in the future articles.</p>
<p>Having a different VM means that we must have a separate compiler <a href="https://github.com/matter-labs/zksolc-bin">zk-solc</a> -
as the bytecode that is produced by this compiler has to use the zkEVM specific opcodes.</p>
<p>While having a separate compiler introduces a bunch of challenges (for example, we need a custom
<a href="https://github.com/matter-labs/hardhat-zksync">hardhat plugins</a> ), it brings a bunch of benefits too: for example it
allows us to move some of the VM logic (like new contract deployment) into System contracts - which allows faster &amp;
cheaper modifications and increased flexibility.</p>
<h3 id="zksync-system-contracts"><a class="header" href="#zksync-system-contracts">ZKsync system contracts</a></h3>
<p>Small note on system contracts: as mentioned above, we moved some of the VM logic into system contracts, which allows us
to keep VM simpler (and with this - keep the proving system simpler).</p>
<p>You can see the full list (and codes) of the system contracts here:
<a href="https://github.com/matter-labs/era-system-contracts">https://github.com/matter-labs/era-system-contracts</a>.</p>
<p>While some of them are not really visible to the contract developer (like the fact that we’re running a special
<code>Bootleader</code> to package a bunch of transactions together - more info in a future article) - some others are very
visible - like our <code>ContractDeployer</code></p>
<h3 id="contractdeployer"><a class="header" href="#contractdeployer">ContractDeployer</a></h3>
<p>Deploying a new contract differs on Ethereum and ZKsync.</p>
<p>While on Ethereum - you send the transaction to 0x00 address - on ZKsync you have to call the special <code>ContractDeployer</code>
system contract.</p>
<p>If you look on your hardhat example, you’ll notice that your <code>deploy.ts</code> is actually using a <code>Deployer</code> class from the
<code>hardhat-zksync-deploy</code> plugin.</p>
<p>Which inside uses the ZKsync’s web3.js, that calls the contract deployer
<a href="https://github.com/zksync-sdk/zksync2-js/blob/b1d11aa016d93ebba240cdeceb40e675fb948133/src/contract.ts#L76">here</a></p>
<pre><code class="language-typescript">override getDeployTransaction(..) {
    ...
    txRequest.to = CONTRACT_DEPLOYER_ADDRESS;
    ...
}
</code></pre>
<p>Also <code>ContractDeployer</code> adding a special prefix for all the new contract addresses. This means that contract addresses
WILL be different on <code>ZKsync</code> and Ethereum (and also leaves us the possibility of adding Ethereum addresses in the
future if needed).</p>
<p>You can look for <code>CREATE2_PREFIX</code> and <code>CREATE_PREFIX</code> in the code.</p>
<h3 id="gas-costs"><a class="header" href="#gas-costs">Gas costs</a></h3>
<p>Another part, where ZKsync differs from Ethereum is gas cost. The best example for this are storage slots.</p>
<p>If you have two transactions that are updating the same storage slot - and they are in the same ‘batch’ - only the first
one would be charged (as when we write the final storage to ethereum, we just write the final diff of what slots have
changed - so updating the same slot multiple times doesn’t increase the amount of data that we have to write to L1).</p>
<h3 id="account-abstraction-and-some-method-calls"><a class="header" href="#account-abstraction-and-some-method-calls">Account abstraction and some method calls</a></h3>
<p>As <code>ZKsync</code> has a built-in Account Abstraction (more on this in a separate article) - you shouldn’t depend on some of
the solidity functions (like <code>ecrecover</code> - that checks the keys, or <code>tx.origin</code>) - in all the cases, the compiler will
try to warn you.</p>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>In this article, we looked at how contract development &amp; deployment differs on Ethereum and ZKsync (looking at
differences in VMs, compilers and system contracts).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="life-of-a-call"><a class="header" href="#life-of-a-call">Life of a ‘call’</a></h1>
<p>This article will show you how the <code>call</code> method works in our backend. The <code>call</code> method is a ‘read-only’ operation,
which means it doesn’t change anything on the blockchain. This will give you a chance to understand the system,
including the bootloader and VM.</p>
<p>For this example, let’s assume that the contract is already deployed, and we will use the <code>call</code> method to interact with
it.</p>
<p>Since the ‘call’ method is only for reading data, all the calculations will happen in the <code>api_server</code>.</p>
<h3 id="calling-the-call-method"><a class="header" href="#calling-the-call-method">Calling the ‘call’ method</a></h3>
<p>If you need to make calls quickly, you can use the ‘cast’ binary from the
<a href="https://foundry-book.zksync.io/getting-started/installation">Foundry ZKsync</a> suite:</p>
<pre><code class="language-shell=">cast call 0x23DF7589897C2C9cBa1C3282be2ee6a938138f10 "myfunction()()" --rpc-url http://localhost:3050
</code></pre>
<p>The address of your contract is represented by 0x23D…</p>
<p>Alternatively, you can make an RPC call directly, but this can be complicated as you will have to create the correct
payload, which includes computing the ABI for the method, among other things.</p>
<p>An example of an RPC call would be:</p>
<pre><code class="language-shell=">curl --location 'http://localhost:3050' \
--header 'Content-Type: application/json' \
--data '{
    "jsonrpc": "2.0",
    "id": 2,
    "method": "eth_call",
    "params": [
        {
            "from": "0x0000000000000000000000000000000000000000",
            "data": "0x0dfe1681",
            "to": "0x2292539b1232A0022d1Fc86587600d86e26396D2"
        }

    ]
}'
</code></pre>
<p>As you can see, using the RPC call directly is much more complex. That’s why I recommend using the ‘cast’ tool instead.</p>
<h3 id="whats-happening-in-the-server"><a class="header" href="#whats-happening-in-the-server">What’s happening in the server</a></h3>
<p>Under the hood, the ‘cast’ tool calls the <code>eth_call</code> RPC method, which is part of the official Ethereum API set. You can
find the definition of these methods in the <a href="https://github.com/matter-labs/zksync-era/blob/edd48fc37bdd58f9f9d85e27d684c01ef2cac8ae/core/bin/zksync_core/src/api_server/web3/backend_jsonrpc/namespaces/eth.rs" title="namespaces RPC api">namespaces/eth.rs</a> file in our code.</p>
<p>Afterward, it goes to the implementation, which is also in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/api_server/src/web3/namespaces/eth.rs" title="namespaces RPC implementation">namespaces/eth.rs</a> file but in a
different parent directory.</p>
<p>The server then executes the function in a VM sandbox. Since this is a <code>call</code> function, the VM only runs this function
before shutting down. This is handled by the <code>execute_tx_eth_call</code> method, which fetches metadata like block number and
timestamp from the database, and the <code>execute_tx_in_sandbox</code> method, which takes care of the execution itself. Both of
these functions are in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/api_server/src/execution_sandbox/execute.rs" title="execution sandbox">api_server/execution_sandbox.rs</a> file.</p>
<p>Finally, the transaction is pushed into bootloader memory, and the VM executes it until it finishes.</p>
<h3 id="vm"><a class="header" href="#vm">VM</a></h3>
<p>Before we look at the bootloader, let’s briefly examine the VM itself.</p>
<p>The zkEVM is a state machine with a heap, stack, 16 registers, and state. It executes zkEVM assembly, which has many
opcodes similar to EVM, but operates on registers rather than a stack. We have two implementations of the VM: one is in
‘pure rust’ without circuits (in the zk_evm repository), and the other has circuits (in the sync_vm repository). In this
example, the api server uses the ‘zk_evm’ implementation without circuits.</p>
<p>Most of the code that the server uses to interact with the VM is in
<a href="https://github.com/matter-labs/zksync-era/blob/ccd13ce88ff52c3135d794c6f92bec3b16f2210f/core/lib/multivm/src/versions/vm_latest/implementation/execution.rs#L108" title="vm code">core/lib/multivm/src/versions/vm_latest/implementation/execution.rs</a>.</p>
<p>In this line, we’re calling self.state.cycle(), which executes a single VM instruction. You can see that we do a lot of
things around this, such as executing multiple tracers after each instruction. This allows us to debug and provide
additional feedback about the state of the VM.</p>
<h3 id="bootloader--transaction-execution"><a class="header" href="#bootloader--transaction-execution">Bootloader &amp; transaction execution</a></h3>
<p>The Bootloader is a large ‘quasi’ system contract, written in Yul and located in
<a href="https://github.com/matter-labs/era-system-contracts/blob/93a375ef6ccfe0181a248cb712c88a1babe1f119/bootloader/bootloader.yul" title="bootloader code">system_contracts/bootloader/bootloader.yul</a> .</p>
<p>It’s a ‘quasi’ contract because it isn’t actually deployed under any address. Instead, it’s loaded directly into the VM
by the binary in the constructor <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/multivm/src/versions/vm_m6/vm_with_bootloader.rs#L330" title="vm constructor">init_vm_inner</a>.</p>
<p>So why do we still need a bootloader if we have the call data, contract binary, and VM? There are two main reasons:</p>
<ul>
<li>It allows us to ‘glue’ transactions together into one large transaction, making proofs a lot cheaper.</li>
<li>It allows us to handle some system logic (checking gas, managing some L1-L2 data, etc.) in a provable way. From the
circuit/proving perspective, this behaves like contract code.</li>
<li>You’ll notice that the way we run the bootloader in the VM is by first ‘kicking it off’ and cycling step-by-step until
it’s ready to accept the first transaction. Then we ‘inject’ the transaction by putting it in the right place in VM
memory and start iterating the VM again. The bootloader sees the new transaction and simply executes its opcodes.</li>
</ul>
<p>This allows us to ‘insert’ transactions one by one and easily revert the VM state if something goes wrong. Otherwise,
we’d have to start with a fresh VM and re-run all the transactions again.</p>
<h3 id="final-steps"><a class="header" href="#final-steps">Final steps</a></h3>
<p>Since our request was just a ‘call’, after running the VM to the end, we can collect the result and return it to the
caller. Since this isn’t a real transaction, we don’t have to do any proofs, witnesses, or publishing to L1.</p>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>In this article, we covered the ‘life of a call’ from the RPC to the inner workings of the system, and finally to the
‘out-of-circuit’ VM with the bootloader.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="life-of-transaction"><a class="header" href="#life-of-transaction">Life of transaction</a></h1>
<p>In this article, we will explore the lifecycle of a transaction, which is an operation that is stored permanently in the
blockchain and results in a change of its overall state.</p>
<p>To better understand the content discussed here, it is recommended that you first read the <a href="guides/advanced/how_call_works.html" title="life of call">life of a
call</a>.</p>
<h2 id="l1-vs-l2-transactions"><a class="header" href="#l1-vs-l2-transactions">L1 vs L2 transactions</a></h2>
<p>There are two main methods through which transactions can enter the system. The most common approach involves making a
call to the RPC (Remote Procedure Call), where you send what is known as an <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/l2/mod.rs#L140" title="l2 tx"><code>L2Tx</code></a> transaction.</p>
<p>The second method involves interacting with Ethereum directly by sending a ‘wrapped’ transaction to our Ethereum
contract. These transactions are referred to as <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/l1/mod.rs#L183" title="l1 tx"><code>L1Tx</code></a> or Priority transactions, and the process of sending
transactions in this manner is called the ‘priority queue’.</p>
<h3 id="transaction-types"><a class="header" href="#transaction-types">Transaction types</a></h3>
<p>We provide support for five different types of transactions.</p>
<p>Here’s a simplified table of the transaction types:</p>
<div class="table-wrapper"><table><thead><tr><th>Type id</th><th>Transaction type</th><th>Features</th><th>Use cases</th><th>% of transactions (mainnet/testnet)</th></tr></thead><tbody>
<tr><td>0x0</td><td>‘Legacy’</td><td>Only includes <code>gas price</code></td><td>These are traditional Ethereum transactions.</td><td>60% / 82%</td></tr>
<tr><td>0x1</td><td>EIP-2930</td><td>Contains a list of storage keys/addresses the transaction will access</td><td>At present, this type of transaction is not enabled.</td><td></td></tr>
<tr><td>0x2</td><td>EIP-1559</td><td>Includes <code>max_priority_fee_per_gas</code>, <code>max_gas_price</code></td><td>These are Ethereum transactions that provide more control over the gas fee.</td><td>35% / 12%</td></tr>
<tr><td>0x71</td><td>EIP-712 (specific to ZKsync)</td><td>Similar to EIP-1559, but also adds <code>max_gas_per_pubdata</code>, custom signatures, and Paymaster support</td><td>This is used by those who are using ZKsync specific Software Development Kits (SDKs).</td><td>1% / 2%</td></tr>
<tr><td>0xFF</td><td>L1 transactions also known as priority transactions <code>L1Tx</code></td><td>Originating from L1, these have more custom fields like ‘refund’ addresses etc</td><td>Mainly used to transfer funds/data between L1 &amp; L2 layer.</td><td>4% / 3%</td></tr>
</tbody></table>
</div>
<p>Here’s the code that does the parsing: <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/transaction_request.rs#L196" title="transaction request from bytes">TransactionRequest::from_bytes</a></p>
<h2 id="transactions-lifecycle"><a class="header" href="#transactions-lifecycle">Transactions lifecycle</a></h2>
<h3 id="priority-queue-l1-tx-only"><a class="header" href="#priority-queue-l1-tx-only">Priority queue (L1 Tx only)</a></h3>
<p>L1 transactions are first ‘packaged’ and then sent to our Ethereum contract. After this, the L1 contract records this
transaction in L1 logs. <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/eth_watch" title="Ethereum watcher component">The <code>eth_watcher</code> component</a> constantly monitors these logs and then adds them to
the database (mempool).</p>
<h3 id="rpc--validation-l2-tx-only"><a class="header" href="#rpc--validation-l2-tx-only">RPC &amp; validation (L2 Tx only)</a></h3>
<p>Transactions are received via the <code>eth_sendRawTransaction</code> method. These are then parsed and validated using the
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/api_server/tx_sender/mod.rs#L288" title="submit tx"><code>submit_tx</code></a> method on the API server.</p>
<p>The validations ensure that the correct amount of gas has been assigned by the user and that the user’s account has
sufficient gas, among other things.</p>
<p>As part of this validation, we also perform a <code>validation_check</code> to ensure that if account abstraction / paymaster is
used, they are prepared to cover the fees. Additionally, we perform a ‘dry_run’ of the transaction for a better
developer experience, providing almost immediate feedback if the transaction fails.</p>
<p>Please note, that transaction can still fail in the later phases, even if it succeeded in the API, as it is going to be
executed in the context of a different block.</p>
<p>Once validated, the transaction is added to the mempool for later execution. Currently, the mempool is stored in the
<code>transactions</code> table in postgres (see the <code>insert_transaction_l2()</code> method).</p>
<h3 id="batch-executor--state-keeper"><a class="header" href="#batch-executor--state-keeper">Batch executor &amp; State keeper</a></h3>
<p>The State Keeper’s job is to take transactions from the mempool and place them into an L1 batch. This is done using the
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/keeper.rs#L257" title="process l1 batch"><code>process_l1_batch()</code></a> method.</p>
<p>This method takes the next transaction from the mempool (which could be either an L1Tx or L2Tx - but L1Tx are always
given the priority and they are taken first), executes it, and checks if the L1 batch is ready to be sealed (for more
details on when we finalize L1 batches, see the ‘Blocks &amp; Batches’ article).</p>
<p>Once the batch is sealed, it’s ready to be sent for proof generation and have this proof committed into L1. More details
on this will be covered in a separate article.</p>
<p>The transaction can have three different results in state keeper:</p>
<ul>
<li>Success</li>
<li>Failure (but still included in the block, and gas was charged)</li>
<li>Rejection - when it fails validation, and cannot be included in the block. This last case should (in theory) never
happen - as we cannot charge the fee in such scenario, and it opens the possibility for the DDoS attack.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fees-aka-gas"><a class="header" href="#fees-aka-gas">Fees (a.k.a gas)</a></h1>
<p>What is the L2 gas price? It’s <strong>0.1 Gwei</strong> (and as we improve our provers/VM we hope it will go down). However, it can
vary at times. Please see further information below.</p>
<h2 id="what-do-you-pay-for"><a class="header" href="#what-do-you-pay-for">What do you pay for</a></h2>
<p>The gas fee covers the following expenses:</p>
<ul>
<li>Calculation and storage (related to most operations)</li>
<li>Publishing data to L1 (a significant cost for many transactions, with the exact amount depending on L1)</li>
<li>Sending ‘bytecode’ to L1 (if not already there) - typically a one-time cost when deploying a new contract</li>
<li>Closing the batch and handling proofs - This aspect also relies on L1 costs (since proof publication must be covered).</li>
</ul>
<h2 id="price-configuration"><a class="header" href="#price-configuration">Price configuration</a></h2>
<p>We have two pricing models (old and new):</p>
<ul>
<li>the <code>L1Pegged</code> - until protocol version 19</li>
<li>the <code>PubdataIndependent</code> - from protocol version 20 (release 1.4.1)</li>
</ul>
<h3 id="l1-pegged-old-fee-model"><a class="header" href="#l1-pegged-old-fee-model">L1 pegged (‘old’ fee model)</a></h3>
<p>Under this fee model, operator was providing <code>FeeParamsV1</code>, which contained:</p>
<ul>
<li>l1_gas_price</li>
<li>minimal_l2_gas_price</li>
</ul>
<p>then, the system was computing <code>L1PeggedBatchFeeModelInput</code>, that contained</p>
<ul>
<li>l1_gas_price</li>
<li>‘fair’ l2 gas price - which in 99% of cases was equal to minimal_gas_price, and was greater from it only if l1 gas
price was huge, to guarantee that we can publish enough data in each transaction (see table below for details).</li>
</ul>
<p>Many other values, were ‘hardcoded’ within the system (for example how to compute the pubdata price based on l1 gas
price, how much committing the proof to L1 costs etc).</p>
<h3 id="pubdataindependent-new-fee-model"><a class="header" href="#pubdataindependent-new-fee-model">PubdataIndependent (‘new’ fee model)</a></h3>
<p>This method is called <code>PubdataIndependent</code> and the change was done to allow more flexibility in pubdata costs (for
example if pubdata is published to another Data Availability layer, or if not published at all - in case of validium).</p>
<p>In this model, there are 8 config options, let’s walk through them:</p>
<p><code>FeeParamsV2</code> contains 2 dynamic prices:</p>
<ul>
<li><code>l1_gas_price</code> - which is used to compute the cost of submitting the proofs on L1</li>
<li><code>l1_pubdata_price</code> - which is the cost of submitting a single byte of pubdata</li>
</ul>
<p>And config options (<code>FeeModelConfigV2</code>) contain:</p>
<ul>
<li><code>minimal_l2_gas_price</code> - similar meaning to the one in the previous model - this should cover the $ costs of running
the machines (node operator costs).</li>
</ul>
<p>2 fields around the maximum capacity of the batch (note - that these are used only for the fee calculation, the actual
sealing criteria are specified in different configuration):</p>
<ul>
<li><code>max_gas_per_batch</code> - expected maximum amount of gas in a batch</li>
<li><code>max_pubdata_per_batch</code> - expected maximum amount of pubdata that we can put in a single batch - due to Ethereum
limitations (usually it is around 128kb when using calldata, and 250 when using 2 blobs)</li>
</ul>
<p>the actual cost of the batch:</p>
<ul>
<li><code>batch_overhead_l1_gas</code> - how much gas operator will have to pay to handle the proof on L1 (this should include
commitBatch, proveBatch and executeBatch costs). This should NOT include any cost that is related to pubdata.</li>
</ul>
<p>And 2 fields about who contributed to closing the batch:</p>
<ul>
<li><code>pubdata_overhead_part</code> - from 0 - 1</li>
<li><code>compute_overhead_part</code> - from 0 - 1</li>
</ul>
<h4 id="cost-distribution-between-transactions"><a class="header" href="#cost-distribution-between-transactions">Cost distribution between transactions</a></h4>
<p>Batches are closed, when we either run out of circuits (a.k.a gas / computation), or run out of pubdata capacity (too
much data to publish to L1 in one transaction), or run out of transactions slots (which should be a rare event with
recent improvements)</p>
<p>Closing each batch, has some cost for the operator - especially the one related to L1 costs - where operator has to
submit a bunch of transactions, including the one to verify the proof (these costs are counted in
<code>batch_overhead_l1_gas</code>).</p>
<p>Now the problem that operator is facing is, who should pay for closing of the batch. In a perfect world, we’d look at
the reason for batch closure (for example pubdata) - and then charge the transactions proportionally to the amount of
pubdata that they used. Unfortunately this is not feasible, as we have to charge the transactions as we go, rather than
at the end of the batch (that can have 1000s of transactions).</p>
<p>That’s why we have the logic of <code>pubdata_overhead_part</code> and <code>compute_overhead_part</code>. These represent the ‘odds’ whether
pubdata or compute were the reason for the batch closure - and based on this information, we distribute the costs to
transactions:</p>
<pre><code>cost_of_closing_the_batch = (compute_overhead_part * TX_GAS_USED / MAX_GAS_IN_BLOCK + pubdata_overhead_part * PUBDATA_USED / MAX_PUBDATA_IN_BLOCK)
</code></pre>
<h4 id="custom-base-token-configurations"><a class="header" href="#custom-base-token-configurations">Custom base token configurations</a></h4>
<p>When running a system based on a custom token, all the gas values above, should refer to YOUR custom token.</p>
<p>Example, if you’re running USDC as base token, and ETH currently costs 2000$, and current L1 gas price is 30 Gwei.</p>
<ul>
<li><code>l1_gas_price</code> param should be set to 30 * 2000 == 60’000 Gwei</li>
<li><code>l1_pubdata_price</code> should be also updated accordingly (also multiplied by 2’000). (note: currently bootloader has a
cap of 1M gwei per pubdata price - and we’re working on removing this limitation)</li>
<li><code>minimal_l2_gas_price</code> should be set in such way, that <code>minimal_l2_gas_price * max_gas_per_batch / 10**18 $</code> is enough
to pay for your CPUs and GPUs</li>
</ul>
<h4 id="validium--data-availability-configurations"><a class="header" href="#validium--data-availability-configurations">Validium / Data-availability configurations</a></h4>
<p>If you’re running a system with validium without any DA, you can just set the <code>l1_pubdata_price</code> to 0,
<code>max_pubdata_per_batch</code> to some large value, and set <code>pubdata_overhead_part</code> to 0, and <code>compute_overhead_part</code> to 1.</p>
<p>If you’re running alternative DA, you should adjust the <code>l1_pubdata_price</code> to roughly cover the cost of writing one byte
to the DA, and set <code>max_pubdata_per_batch</code> to the DA limits.</p>
<p>Note: currently system still requires operator to keep the data in memory and compress it, which means that setting huge
values of <code>max_pubdata_per_batch</code> might not work. This will be fixed in the future.</p>
<p>Assumption: ETH costs 2’000$, L1 gas cost is 30 Gwei, blob costs 2Gwei (per byte), and DA allows 1MB payload that cost 1
cent.</p>
<div class="table-wrapper"><table><thead><tr><th>flag</th><th>rollup with calldata</th><th>rollup with 4844 (blobs)</th><th>value for validium</th><th>value for DA</th></tr></thead><tbody>
<tr><td><code>l1_pubdata_price</code></td><td>510’000’000’000</td><td>2’000’000’000</td><td>0</td><td>5’000</td></tr>
<tr><td><code>max_pubdata_per_batch</code></td><td>120’000</td><td>250’000</td><td>1’000’000’000’000</td><td>1’000’000</td></tr>
<tr><td><code>pubdata_overhead_part</code></td><td>0.7</td><td>0.4</td><td>0</td><td>0.1</td></tr>
<tr><td><code>compute_overhead_part</code></td><td>0.5</td><td>0.7</td><td>1</td><td>1</td></tr>
<tr><td><code>batch_overhead_l1_gas</code></td><td>1’000’000</td><td>1’000’000</td><td>1’000’000</td><td>1’400’000</td></tr>
</tbody></table>
</div>
<p>The cost of l1 batch overhead is higher for DA, as it has to cover the additional costs of checking on L1 if DA actually
got the data.</p>
<h2 id="l1-vs-l2-pricing"><a class="header" href="#l1-vs-l2-pricing">L1 vs L2 pricing</a></h2>
<p>Here is a simplified table displaying various scenarios that illustrate the relationship between L1 and L2 fees:</p>
<div class="table-wrapper"><table><thead><tr><th>L1 gas price</th><th>L2 ‘minimal price’</th><th>L2 ‘gas price’</th><th>L2 gas per pubdata</th><th>Note</th></tr></thead><tbody>
<tr><td>0.25 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>17</td><td>Gas prices are equal, so the charge is 17 gas, just like on L1.</td></tr>
<tr><td>10 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>680</td><td>L1 is 40 times more expensive, so we need to charge more L2 gas per pubdata byte to cover L1 publishing costs.</td></tr>
<tr><td>250 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>17000</td><td>L1 is now very expensive (1000 times more than L2), so each pubdata costs a lot of gas.</td></tr>
<tr><td>10000 Gwei</td><td>0.25 Gwei</td><td>8.5 Gwei</td><td>20000</td><td>L1 is so expensive that we have to raise the L2 gas price, so the gas needed for publishing doesn’t exceed the 20k limit, ensuring L2 remains usable.</td></tr>
</tbody></table>
</div>
<p><strong>Why is there a 20k gas per pubdata limit?</strong> - We want to make sure every transaction can publish at least 4kb of data
to L1. The maximum gas for a transaction is 80 million (80M/4k = 20k).</p>
<h3 id="l2-fair-price"><a class="header" href="#l2-fair-price">L2 Fair price</a></h3>
<p>The L2 fair gas price is currently determined by the StateKeeper/Sequencer configuration and is set at 0.10 Gwei (see
<code>fair_l2_gas_price</code> in the config). This price is meant to cover the compute costs (CPU + GPU) for the sequencer and
prover. It can be changed as needed, with a safety limit of 10k Gwei in the bootloader. Once the system is
decentralized, more deterministic rules will be established for this price.</p>
<h3 id="l1-gas-price"><a class="header" href="#l1-gas-price">L1 Gas price</a></h3>
<p>The L1 gas price is fetched by querying L1 every 20 seconds. This is managed by the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/l1_gas_price/gas_adjuster/mod.rs#L30" title="gas_adjuster"><code>GasAdjuster</code></a>, which
calculates the median price from recent blocks and enables more precise price control via the config (for example,
adjusting the price with <code>internal_l1_pricing_multiplier</code> or setting a specific value using
<code>internal_enforced_l1_gas_price</code>).</p>
<h3 id="overhead-gas"><a class="header" href="#overhead-gas">Overhead gas</a></h3>
<p>As mentioned earlier, fees must also cover the overhead of generating proofs and submitting them to L1. While the
detailed calculation is complex, the short version is that a full proof of an L1 batch costs around <strong>1 million L2 gas,
plus 1M L1 gas (roughly equivalent of 60k published bytes)</strong>. In every transaction, you pay a portion of this fee
proportional to the part of the batch you are using.</p>
<h2 id="transactions"><a class="header" href="#transactions">Transactions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Transaction Field</th><th>Conditions</th><th>Note</th></tr></thead><tbody>
<tr><td>gas_limit</td><td><code>&lt;= max_allowed_l2_tx_gas_limit</code></td><td>The limit (4G gas) is set in the <code>StateKeeper</code> config; it’s the limit for the entire L1 batch.</td></tr>
<tr><td>gas_limit</td><td><code>&lt;= MAX_GAS_PER_TRANSACTION</code></td><td>This limit (80M) is set in bootloader.</td></tr>
<tr><td>gas_limit</td><td><code>&gt; l2_tx_intrinsic_gas</code></td><td>This limit (around 14k gas) is hardcoded to ensure that the transaction has enough gas to start.</td></tr>
<tr><td>max_fee_per_gas</td><td><code>&gt;= fair_l2_gas_price</code></td><td>Fair L2 gas price (0.1 Gwei on Era) is set in the <code>StateKeeper</code> config</td></tr>
<tr><td></td><td><code>&lt;=validation_computational_gas_limit</code></td><td>There is an additional, stricter limit (300k gas) on the amount of gas that a transaction can use during validation.</td></tr>
</tbody></table>
</div>
<h3 id="why-do-we-have-two-limits-80m-and-4g"><a class="header" href="#why-do-we-have-two-limits-80m-and-4g">Why do we have two limits: 80M and 4G</a></h3>
<p>The operator can set a custom transaction limit in the bootloader. However, this limit must be within a specific range,
meaning it cannot be less than 80M or more than 4G.</p>
<h3 id="why-validation-is-special"><a class="header" href="#why-validation-is-special">Why validation is special</a></h3>
<p>In Ethereum, there is a fixed cost for verifying a transaction’s correctness by checking its signature. However, in
ZKsync, due to Account Abstraction, we may need to execute some contract code to determine whether it’s ready to accept
the transaction. If the contract rejects the transaction, it must be dropped, and there’s no one to charge for that
process.</p>
<p>Therefore, a stricter limit on validation is necessary. This prevents potential DDoS attacks on the servers, where
people could send invalid transactions to contracts that require expensive and time-consuming verifications. By imposing
a stricter limit, the system maintains stability and security.</p>
<h2 id="actual-gas-calculation"><a class="header" href="#actual-gas-calculation">Actual gas calculation</a></h2>
<p>From the Virtual Machine (VM) point of view, there is only a bootloader. When executing transactions, we insert the
transaction into the bootloader memory and let it run until it reaches the end of the instructions related to that
transaction (for more details, refer to the ‘Life of a Call’ article).</p>
<p>To calculate the gas used by a transaction, we record the amount of gas used by the VM before the transaction execution
and subtract it from the remaining gas after the execution. This difference gives us the actual gas used by the
transaction.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let gas_remaining_before = vm.gas_remaining();
execute_tx();
let gas_used = gas_remaining_before - vm.gas_remaining();
<span class="boring">}</span></code></pre></pre>
<h2 id="gas-estimation"><a class="header" href="#gas-estimation">Gas estimation</a></h2>
<p>Before sending a transaction to the system, most users will attempt to estimate the cost of the request using the
<code>eth_estimateGas</code> call.</p>
<p>To estimate the gas limit for a transaction, we perform a binary search (between 0 and the <code>MAX_L2_TX_GAS_LIMIT</code> of 80M)
to find the smallest amount of gas under which the transaction still succeeds.</p>
<p>For added safety, we include some ‘padding’ by using two additional config options: <code>gas_price_scale_factor</code> (currently
1.5) and <code>estimate_gas_scale_factor</code> (currently 1.3). These options are used to increase the final estimation.</p>
<p>The first option simulates the volatility of L1 gas (as mentioned earlier, high L1 gas can affect the actual gas cost of
data publishing), and the second one serves as a ‘safety margin’.</p>
<p>You can find this code in <a href="https://github.com/matter-labs/zksync-era/blob/714a8905d407de36a906a4b6d464ec2cab6eb3e8/core/lib/zksync_core/src/api_server/tx_sender/mod.rs#L656" title="get_txs_fee_in_wei">get_txs_fee_in_wei</a> function.</p>
<h2 id="qa"><a class="header" href="#qa">Q&amp;A</a></h2>
<h3 id="is-zksync-really-cheaper"><a class="header" href="#is-zksync-really-cheaper">Is ZKsync really cheaper</a></h3>
<p>In short, yes. As seen in the table at the beginning, the regular L2 gas price is set to 0.25 Gwei, while the standard
Ethereum price is around 60-100 Gwei. However, the cost of publishing to L1 depends on L1 prices, meaning that the
actual transaction costs will increase if the L1 gas price rises.</p>
<h3 id="why-do-i-hear-about-large-refunds"><a class="header" href="#why-do-i-hear-about-large-refunds">Why do I hear about large refunds</a></h3>
<p>There are a few reasons why refunds might be ‘larger’ on ZKsync (i.e., why we might be overestimating the fees):</p>
<ul>
<li>We must assume (pessimistically) that you’ll have to pay for all the slot/storage writes. In practice, if multiple
transactions touch the same slot, we only charge one of them.</li>
<li>We have to account for larger fluctuations in the L1 gas price (using gas_price_scale_factor mentioned earlier) - this
might cause the estimation to be significantly higher, especially when the L1 gas price is already high, as it then
impacts the amount of gas used by pubdata.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-l2-to-l1-messaging-works"><a class="header" href="#how-l2-to-l1-messaging-works">How L2 to L1 messaging works</a></h1>
<p>In this article, we will explore the workings of Layer 2 (L2) to Layer 1 (L1) messaging in ZKsync Era.</p>
<p>If you’re uncertain about why messaging is necessary in the first place, please refer to our <a href="https://code.zksync.io/tutorials/how-to-send-l2-l1-message">user
documentation</a>.</p>
<p>For ease of understanding, here’s a quick visual guide. We will unpack each part in detail as we progress.</p>
<p><img src="https://user-images.githubusercontent.com/128217157/257739371-f971c10b-87c7-4ee9-bd0e-731670c616ac.png" alt="overview image" /></p>
<h2 id="part-1---user-generates-a-message"><a class="header" href="#part-1---user-generates-a-message">Part 1 - User Generates a Message</a></h2>
<p>Consider the following contract. Its main function is to forward any received string to L1:</p>
<pre><code class="language-solidity">contract Messenger {
  function sendMessage(string memory message) public returns (bytes32 messageHash) {
    messageHash = L1_MESSENGER_CONTRACT.sendToL1(bytes(message));
  }
}

</code></pre>
<p>From a developer’s standpoint, you only need to invoke the <code>sendToL1</code> method, and your task is complete.</p>
<p>It’s worth noting, however, that transferring data to L1 typically incurs high costs. These costs are associated with
the ‘pubdata cost’ that is charged for each byte in the message. As a workaround, many individuals choose to send the
hash of the message instead of the full message, as this helps to conserve resources.</p>
<h2 id="part-2---system-contract-execution"><a class="header" href="#part-2---system-contract-execution">Part 2 - System Contract Execution</a></h2>
<p>The previously mentioned <code>sendToL1</code> method executes a call to the <code>L1Messenger.sol</code> system contract
<a href="https://github.com/matter-labs/era-system-contracts/blob/f01df555c03860b6093dd669d119eed4d9f8ec99/contracts/L1Messenger.sol#L22">here</a>. This system contract performs tasks such as computing the appropriate gas cost and hashes, and
then it broadcasts an Event carrying the complete message.</p>
<pre><code class="language-solidity">function sendToL1(bytes calldata _message) external override returns (bytes32 hash) {
  // ...
  SystemContractHelper.toL1(true, bytes32(uint256(uint160(msg.sender))), hash);
  emit L1MessageSent(msg.sender, hash, _message);
}

</code></pre>
<p>As depicted in the leading image, this stage is where the message data splits. The full body of the message is emitted
for retrieval in Part 5 by the StateKeeper, while the hash of the message proceeds to be added to the Virtual Machine
(VM) - as it has to be included in the proof.</p>
<p>The method then sends the message’s hash to the <code>SystemContractHelper</code>, which makes an internal call:</p>
<pre><code class="language-solidity">function toL1(
  bool _isService,
  bytes32 _key,
  bytes32 _value
) internal {
  // ...
  address callAddr = TO_L1_CALL_ADDRESS;
  assembly {
    call(_isService, callAddr, _key, _value, 0xFFFF, 0, 0)
  }
}

</code></pre>
<p>Following the <code>TO_L1_CALL_ADDRESS</code>, we discover that it’s set to a placeholder value. So what exactly is occurring here?</p>
<h2 id="part-3---compiler-tricks-and-the-eravm"><a class="header" href="#part-3---compiler-tricks-and-the-eravm">Part 3 - Compiler Tricks and the EraVM</a></h2>
<p>Our VM features special opcodes designed to manage operations that aren’t possible in the Ethereum Virtual Machine
(EVM), such as publishing data to L1. But how can we make these features accessible to Solidity?</p>
<p>We could expand the language by introducing new Solidity opcodes, but that would require modifying the solc compiler,
among other things. Hence, we’ve adopted a different strategy.</p>
<p>To access these unique eraVM opcodes, the Solidity code simply executes a call to a specific address (the full list can
be seen <a href="https://github.com/matter-labs/era-system-contracts/blob/e96dfe0b5093fa95c2fb340c0411c646327db921/contracts/libraries/SystemContractsCaller.sol#L12">here</a>). This call is compiled by the solc frontend, and then on the compiler backend, we
intercept it and replace it with the correct eraVM opcode call <a href="https://github.com/matter-labs/era-compiler-llvm-context/blob/main/src/eravm/evm/call.rs">here</a>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match simulation_address {
  Some(compiler_common::ADDRESS_TO_L1) =&gt; {
    return crate::zkevm::general::to_l1(context, is_first, in_0, in_1);
  }
}
<span class="boring">}</span></code></pre></pre>
<p>This method allows your message to reach the VM.</p>
<h2 id="part-4---inside-the-virtual-machine"><a class="header" href="#part-4---inside-the-virtual-machine">Part 4 - Inside the Virtual Machine</a></h2>
<p>The zkEVM assembly translates these <a href="https://github.com/matter-labs/era-zkEVM-assembly/blob/v1.3.2/src/assembly/instruction/log.rs#L32">opcodes</a> into LogOpcodes.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const ALL_CANONICAL_MODIFIERS: [&amp;'static str; 5] =
    ["sread", "swrite", "event", "to_l1", "precompile"];
let variant = match idx {
  0 =&gt; LogOpcode::StorageRead,
  1 =&gt; LogOpcode::StorageWrite,
  2 =&gt; LogOpcode::Event,
  3 =&gt; LogOpcode::ToL1Message,
  4 =&gt; LogOpcode::PrecompileCall,
}
<span class="boring">}</span></code></pre></pre>
<p>Each opcode is then converted into the corresponding <a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/v1.3.2/src/definitions/log.rs#L16">LogOpcode</a> and written into the Log
<a href="https://github.com/matter-labs/era-zk_evm/blob/v1.3.2/src/opcodes/execution/log.rs">here</a>, which is handled by the EventSink oracle.</p>
<h2 id="part-5---the-role-of-the-state-keeper"><a class="header" href="#part-5---the-role-of-the-state-keeper">Part 5 - The Role of the State Keeper</a></h2>
<p>At this stage, the state keeper needs to collect all the messages generated by the VM execution and append them to the
calldata it transmits to Ethereum.</p>
<p>This process is divided into two steps:</p>
<ul>
<li>Retrieval of the ‘full’ messages</li>
<li>Extraction of all the message hashes.</li>
</ul>
<p>Why are these steps kept separate?</p>
<p>To avoid overwhelming our circuits with the content of entire messages, we relay them through Events, sending only their
hash to the VM. In this manner, the VM only adds to the proof the information that a message with a specific hash was
sent.</p>
<h3 id="retrieving-full-message-contents"><a class="header" href="#retrieving-full-message-contents">Retrieving Full Message Contents</a></h3>
<p>We go through all the Events generated during the run <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/types/src/event.rs#L147">here</a> and identify those coming from the
<code>L1_MESSENGER_ADDRESS</code> that corresponds to the <code>L1MessageSent</code> topic. These Events represent the ‘emit’ calls executed
in Part 2.</p>
<h3 id="retrieving-message-hashes"><a class="header" href="#retrieving-message-hashes">Retrieving Message Hashes</a></h3>
<p>Message hashes are transmitted alongside the other <code>l2_to_l1_logs</code> within the <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/vm/src/vm.rs#L81">VmExecutionResult</a>.</p>
<p>The StateKeeper collects them from the <a href="https://github.com/matter-labs/era-zk_evm_abstractions/blob/15a2af404902d5f10352e3d1fac693cc395fcff9/src/queries.rs#L30C2-L30C2">LogQueries</a> that the VM creates (these log queries also contain
information about storage writes, so we use the AUX_BYTE filter to determine which ones contain L1 messages. The entire
list can be found <a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/v1.3.2/src/system_params.rs#L37C39-L37C39">here</a>). The StateKeeper employs the VM’s EventSink to filter them out <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/vm/src/event_sink.rs#L116">here</a>.</p>
<h2 id="part-6---interaction-with-ethereum-l1"><a class="header" href="#part-6---interaction-with-ethereum-l1">Part 6 - Interaction with Ethereum (L1)</a></h2>
<p>After the StateKeeper has collected all the required data, it invokes the <code>CommitBlocks</code> method from the
<a href="https://github.com/matter-labs/era-contracts/blob/b04dcaf2256a9b2626eeaefbf1b281f0119d30ab/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol#L21">Executor.sol</a> contract.</p>
<p>Inside the <code>processL2Blocks</code> method, we iterate through the list of L2 message hashes, ensuring that the appropriate
full text is present for each:</p>
<pre><code class="language-solidity">// show preimage for hashed message stored in log
if (logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR) {
    (bytes32 hashedMessage, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);
    // check that the full message body matches the hash.
    require(keccak256(l2Messages[currentMessage]) == hashedMessage, "k2");
</code></pre>
<p>Currently, the executor is deployed on Ethereum mainnet at
[0x389a081BCf20e5803288183b929F08458F1d863D][mainnet_executor].</p>
<p>You can view an example of our contract execution from Part 1, carrying the message “My sample message”, in this Sepolia
transaction: <a href="https://sepolia.etherscan.io/tx/0x18c2a113d18c53237a4056403047ff9fafbf772cb83ccd44bb5b607f8108a64c">0x18c2a113d18c53237a4056403047ff9fafbf772cb83ccd44bb5b607f8108a64c</a>.</p>
<h2 id="part-7---verifying-message-inclusion"><a class="header" href="#part-7---verifying-message-inclusion">Part 7 - Verifying Message Inclusion</a></h2>
<p>We’ve now arrived at the final stage — how L1 users and contracts can confirm a message’s presence in L1.</p>
<p>This is accomplished through the <code>ProveL2MessageInclusion</code> function call in <a href="https://github.com/matter-labs/era-contracts/blob/b04dcaf2256a9b2626eeaefbf1b281f0119d30ab/l1-contracts/contracts/state-transition/chain-deps/facets/Mailbox.sol#L70">Mailbox.sol</a>.</p>
<p>Users supply the proof (merkle path) and the message, and the contract verifies that the merkle path is accurate and
matches the root hash.</p>
<pre><code class="language-solidity">bytes32 calculatedRootHash = Merkle.calculateRoot(_proof, _index, hashedLog);
bytes32 actualRootHash = s.l2LogsRootHashes[_blockNumber];

return actualRootHash == calculatedRootHash;
</code></pre>
<h2 id="summary-4"><a class="header" href="#summary-4">Summary</a></h2>
<p>In this article, we’ve traveled through a vast array of topics: from a user contract dispatching a message to L1 by
invoking a system contract, to this message’s hash making its way all the way to the VM via special opcodes. We’ve also
explored how it’s ultimately included in the execution results (as part of QueryLogs), gathered by the State Keeper, and
transmitted to L1 for final verification.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>Pubdata in ZKsync can be divided up into 4 different categories:</p>
<ol>
<li>L2 to L1 Logs</li>
<li>L2 to L1 Messages</li>
<li>Smart Contract Bytecodes</li>
<li>Storage writes</li>
</ol>
<p>Using data corresponding to these 4 facets, across all executed batches, we’re able to reconstruct the full state of L2.
One thing to note is that the way that the data is represented changes in a pre-boojum and post-boojum zkEVM. At a high
level, in a pre-boojum era these are represented as separate fields while in boojum they are packed into a single bytes
array.</p>
<blockquote>
<p>Note: When the 4844 was integrated this bytes array was moved from being part of the calldata to blob data.</p>
</blockquote>
<p>While the structure of the pubdata changes, we can use the same strategy to pull the relevant information. First, we
need to filter all of the transactions to the L1 ZKsync contract for only the <code>commitBlocks/commitBatches</code> transactions
where the proposed block has been referenced by a corresponding <code>executeBlocks/executeBatches</code> call (the reason for this
is that a committed or even proven block can be reverted but an executed one cannot). Once we have all the committed
blocks that have been executed, we then will pull the transaction input and the relevant fields, applying them in order
to reconstruct the current state of L2.</p>
<p>One thing to note is that in both systems some of the contract bytecode is compressed into an array of indices where
each 2 byte index corresponds to an 8 byte word in a dictionary. More on how that is done <a href="guides/advanced/./compression.html">here</a>. Once
the bytecode has been expanded, the hash can be taken and checked against the storage writes within the
<code>AccountCodeStorage</code> contract which connects an address on L2 with the 32 byte code hash:</p>
<pre><code class="language-solidity">function _storeCodeHash(address _address, bytes32 _hash) internal {
  uint256 addressAsKey = uint256(uint160(_address));
  assembly {
    sstore(addressAsKey, _hash)
  }
}

</code></pre>
<h3 id="pre-boojum-era"><a class="header" href="#pre-boojum-era">Pre-Boojum Era</a></h3>
<p>In pre-boojum era the superset of pubdata fields and input to the <code>commitBlocks</code> function follows the following format:</p>
<pre><code class="language-solidity">/// @notice Data needed to commit new block
/// @param blockNumber Number of the committed block
/// @param timestamp Unix timestamp denoting the start of the block execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param l2LogsTreeRoot The root hash of the tree that contains all L2 -&gt; L1 logs in the block
/// @param priorityOperationsHash Hash of all priority operations from this block
/// @param initialStorageChanges Storage write access as a concatenation key-value
/// @param repeatedStorageChanges Storage write access as a concatenation index-value
/// @param l2Logs concatenation of all L2 -&gt; L1 logs in the block
/// @param l2ArbitraryLengthMessages array of hash preimages that were sent as value of L2 logs by special system L2 contract
/// @param factoryDeps (contract bytecodes) array of L2 bytecodes that were deployed
struct CommitBlockInfo {
  uint64 blockNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 l2LogsTreeRoot;
  bytes32 priorityOperationsHash;
  bytes initialStorageChanges;
  bytes repeatedStorageChanges;
  bytes l2Logs;
  bytes[] l2ArbitraryLengthMessages;
  bytes[] factoryDeps;
}

</code></pre>
<p>The 4 main fields to look at here are:</p>
<ol>
<li><code>initialStorageChanges</code>: Storage slots being written to for the first time and the corresponding value
<ol>
<li>Structure: <code>num entries as u32 || for each entry: (32 bytes key, 32 bytes final value)</code></li>
</ol>
</li>
<li><code>repeatedStorageChanges</code>: ids of the slots being written to and the corresponding value
<ol>
<li>Structure: <code>num entries as u32 || for each entry: (8 byte id, 32 bytes final value)</code></li>
</ol>
</li>
<li><code>factoryDeps</code>: An array of uncompressed bytecodes</li>
<li><code>l2ArbitraryLengthMessages</code> : L2 → L1 Messages
<ol>
<li>We don’t need them all, we are just concerned with messages sent from the <code>Compressor/BytecodeCompressor</code> contract</li>
<li>These messages will follow the compression algorithm outline <a href="guides/advanced/./compression.html">here</a></li>
</ol>
</li>
</ol>
<p>For the ids on the repeated writes, they are generated as we process the first time keys. For example: if we see
<code>[&lt;key1, val1&gt;, &lt;key2, val2&gt;]</code> (starting from an empty state) then we can assume that the next time a write happens to
<code>key1</code> it will be encoded as <code>&lt;1, new_val&gt;</code> and so on and so forth. There is a little shortcut here where the last new
id generated as part of a batch will be in the <code>indexRepeatedStorageChanges</code> field.</p>
<h3 id="post-boojum-era"><a class="header" href="#post-boojum-era">Post-Boojum Era</a></h3>
<pre><code class="language-solidity">/// @notice Data needed to commit new batch
/// @param batchNumber Number of the committed batch
/// @param timestamp Unix timestamp denoting the start of the batch execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param priorityOperationsHash Hash of all priority operations from this batch
/// @param bootloaderHeapInitialContentsHash Hash of the initial contents of the bootloader heap. In practice it serves as the commitment to the transactions in the batch.
/// @param eventsQueueStateHash Hash of the events queue state. In practice it serves as the commitment to the events in the batch.
/// @param systemLogs concatenation of all L2 -&gt; L1 system logs in the batch
/// @param pubdataCommitments Packed pubdata commitments/data.
/// @dev pubdataCommitments format: This will always start with a 1 byte pubdataSource flag. Current allowed values are 0 (calldata) or 1 (blobs)
///                             kzg: list of: opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes) = 144 bytes
///                             calldata: pubdataCommitments.length - 1 - 32 bytes of pubdata
///                                       and 32 bytes appended to serve as the blob commitment part for the aux output part of the batch commitment
/// @dev For 2 blobs we will be sending 288 bytes of calldata instead of the full amount for pubdata.
/// @dev When using calldata, we only need to send one blob commitment since the max number of bytes in calldata fits in a single blob and we can pull the
///     linear hash from the system logs
struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes pubdataCommitments;
}

</code></pre>
<p>The main difference between the two <code>CommitBatchInfo</code> and <code>CommitBlockInfo</code> structs is that we have taken a few of the
fields and merged them into a single bytes array called <code>pubdataCommitments</code>. In the <code>calldata</code> mode, the pubdata is
being passed using that field. In the <code>blobs</code> mode, that field is used to store the KZG commitments and proofs. More on
EIP-4844 blobs <a href="guides/advanced/./pubdata-with-blobs.html">here</a>. In the Validium mode, the field will either be empty or store the
inclusion proof for the DA blob.</p>
<p>The 2 main fields needed for state reconstruction are the bytecodes and the state diffs. The bytecodes follow the same
structure and reasoning in the old system (as explained above). The state diffs will follow the compression illustrated
below.</p>
<h3 id="compression-of-state-diffs-in-post-boojum-era"><a class="header" href="#compression-of-state-diffs-in-post-boojum-era">Compression of State Diffs in Post-Boojum Era</a></h3>
<h4 id="keys"><a class="header" href="#keys">Keys</a></h4>
<p>Keys will be packed in the same way as they were before boojum. The only change is that we’ll avoid using the 8-byte
enumeration index and will pack it to the minimal necessary number of bytes. This number will be part of the pubdata.
Once a key has been used, it can already use the 4 or 5 byte enumeration index and it is very hard to have something
cheaper for keys that has been used already. The opportunity comes when remembering the ids for accounts to spare some
bytes on nonce/balance key, but ultimately the complexity may not be worth it.</p>
<p>There is some room for the keys that are being written for the first time, however, these are rather more complex and
achieve only a one-time effect (when the key is published for the first time).</p>
<h4 id="values"><a class="header" href="#values">Values</a></h4>
<p>Values are much easier to compress, since they usually contain only zeroes. Also, we can leverage the nature of how
those values are changed. For instance if nonce has been increased only by 1, we do not need to write the entire 32-byte
new value, we can just tell that the slot has been <em>increased</em> and then supply only 1-byte value of <em>the size by which</em>
it was increased. This way instead of 32 bytes we need to publish only 2 bytes: first byte to denote which operation has
been applied and the second by to denote the size by which the addition has been made.</p>
<p>If we decide to have just the following 4 types of changes: <code>Add</code>, <code>Sub,</code> <code>Transform</code>, <code>NoCompression</code> where:</p>
<ul>
<li><code>Add</code> denotes that the value has been increased. (modulo 2^256)</li>
<li><code>Sub</code> denotes that the value has been decreased. (modulo 2^256)</li>
<li><code>Transform</code> denotes the value just has been changed (i.e. we disregard any potential relation between the previous and
the new value, though the new value might be small enough to save up on the number of bytes).</li>
<li><code>NoCompression</code> denotes that the whole 32 byte value will be used.</li>
</ul>
<p>Where the byte size of the output can be anywhere from 0 to 31 (also 0 makes sense for <code>Transform</code>, since it denotes
that it has been zeroed out). For <code>NoCompression</code> the whole 32 byte value is used.</p>
<p>So the format of the pubdata will be the following:</p>
<h5 id="part-1-header"><a class="header" href="#part-1-header">Part 1. Header</a></h5>
<ul>
<li><code>&lt;version = 1 byte&gt;</code> — this will enable easier automated unpacking in the future. Currently, it will be only equal to
<code>1</code>.</li>
<li><code>&lt;total_logs_len = 3 bytes&gt;</code> — we need only 3 bytes to describe the total length of the L2→L1 logs.</li>
<li><code>&lt;the number of bytes used for derived keys = 1 byte&gt;</code>. At the beginning it will be equal to <code>4</code>, but then it will
automatically switch to <code>5</code> when needed.</li>
</ul>
<h5 id="part-2-initial-writes"><a class="header" href="#part-2-initial-writes">Part 2. Initial writes</a></h5>
<ul>
<li><code>&lt;num_of_initial_writes = 2 bytes&gt;</code> (since each initial write publishes at least 32 bytes for key, then
<code>2^16 * 32 = 2097152</code> will be enough for a lot of time (right now with the limit of 120kb it will take more than 15 L1
txs to use up all the space there).</li>
<li>Then for each <code>&lt;key, value&gt;</code> pair for each initial write:
<ul>
<li>print key as 32-byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>). More on it
<a href="https://www.notion.so/Pubdata-compression-v1-4b0dd8c151014c8ab96dbd7e66e17599?pvs=21">below</a>.</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<h4 id="part-3-repeated-writes"><a class="header" href="#part-3-repeated-writes">Part 3. Repeated writes</a></h4>
<p>Note, that there is no need to write the number of repeated writes, since we know that until the end of the pubdata, all
the writes will be repeated ones.</p>
<ul>
<li>For each <code>&lt;key, value&gt;</code> pair for each repeated write:
<ul>
<li>print key as either 4 or 5 byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pubdata-post-4844"><a class="header" href="#pubdata-post-4844">Pubdata Post 4844</a></h1>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>EIP-4844, commonly known as Proto-Danksharding, is an upgrade to the ethereum protocol that introduces a new data
availability solution embedded in layer 1. More information about it can be found
<a href="https://ethereum.org/en/roadmap/danksharding/">here</a>. With proto-danksharding we can utilize the new blob data
availability for cheaper storage of pubdata when we commit batches resulting in more transactions per batch and cheaper
batches/transactions. We want to ensure we have the flexibility at the contract level to process both pubdata via
calldata, as well as pubdata via blobs. A quick callout here, while 4844 has introduced blobs as new DA layer, it is the
first step in full Danksharding. With full Danksharding ethereum will be able to handle a total of 64 blobs per block
unlike 4844 which supports just 6 per block.</p>
<blockquote>
<p>💡 Given the nature of 4844 development from a solidity viewpoint, we’ve had to create a temporary contract
<code>BlobVersionedHash.yul</code> which acts in place of the eventual <code>BLOBHASH</code> opcode.</p>
</blockquote>
<h2 id="technical-approach"><a class="header" href="#technical-approach">Technical Approach</a></h2>
<p>The approach spans both L2 system contracts and L1 ZKsync contracts (namely <code>Executor.sol</code>). When a batch is sealed on
L2 we will chunk it into blob-sized pieces (4096 elements * 31 bytes per what is required by our circuits), take the
hash of each chunk, and send them to L1 via system logs. Within <code>Executor.sol</code> , when we are dealing with blob-based
commitments, we verify that the blob contains the correct data with the point evaluation precompile. If the batch
utilizes calldata instead, the processing should remain the same as in a pre-4844 ZKsync. Regardless of if pubdata is in
calldata or blobs are used, the batch’s commitment changes as we include new data within the auxiliary output.</p>
<p>Given that this is the first step to a longer-term solution, and the restrictions of proto-danksharding that get lifted
for full danksharding, we impose the following constraints:</p>
<ol>
<li>we will support a maximum of 2 blobs per batch</li>
<li>only 1 batch will be committed in a given transaction</li>
<li>we will always send 2 system logs (one for each potential blob commitment) even if the batch only uses 1 blob.</li>
</ol>
<p>This simplifies the processing logic on L1 and stops us from increasing the blob base fee (increases when there 3 or
more blobs in a given block).</p>
<h2 id="backward-compatibility"><a class="header" href="#backward-compatibility">Backward-compatibility</a></h2>
<p>While some of the parameter formatting changes, we maintain the same function signature for <code>commitBatches</code> and still
allow for pubdata to be submitted via calldata:</p>
<pre><code class="language-solidity">struct StoredBatchInfo {
  uint64 batchNumber;
  bytes32 batchHash;
  uint64 indexRepeatedStorageChanges;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 l2LogsTreeRoot;
  uint256 timestamp;
  bytes32 commitment;
}

struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes pubdataCommitments;
}

function commitBatches(StoredBatchInfo calldata _lastCommittedBatchData, CommitBatchInfo[] calldata _newBatchesData)
  external;

</code></pre>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<h3 id="bootloader-memory"><a class="header" href="#bootloader-memory">Bootloader Memory</a></h3>
<p>With the increase in the amount of pubdata due to blobs, changes can be made to the bootloader memory to facilitate more
l2 to l1 logs, compressed bytecodes, and pubdata. We take the naive approach for l2 to l1 logs and the compressed
bytecode, doubling their previous constraints from <code>2048</code> logs and <code>32768 slots</code> to <code>4096 logs</code> and <code>65536 slots</code>
respectively. We then increase the number of slots for pubdata from <code>208000</code> to <code>411900</code>. Copying the comment around
pubdata slot calculation from our code:</p>
<pre><code class="language-solidity">One of "worst case" scenarios for the number of state diffs in a batch is when 240kb of pubdata is spent
on repeated writes, that are all zeroed out. In this case, the number of diffs is 240k / 5 = 48k. This means that they will have
accommodate 13056000 bytes of calldata for the uncompressed state diffs. Adding 120k on top leaves us with
roughly 13176000 bytes needed for calldata. 411750 slots are needed to accommodate this amount of data.
We round up to 411900 slots just in case.
</code></pre>
<p>The overall bootloader max memory is increased from <code>24000000</code> to <code>30000000</code> bytes to accommodate the increases.</p>
<h3 id="l2-system-contracts"><a class="header" href="#l2-system-contracts">L2 System Contracts</a></h3>
<p>We introduce a new system contract PubdataChunkPublisher that takes the full pubdata, creates chunks that are each
126,976 bytes in length (this is calculated as 4096 elements per blob each of which has 31 bytes), and commits them in
the form of 2 system logs. We have the following keys for system logs:</p>
<pre><code class="language-solidity">enum SystemLogKey {
  L2_TO_L1_LOGS_TREE_ROOT_KEY,
  TOTAL_L2_TO_L1_PUBDATA_KEY,
  STATE_DIFF_HASH_KEY,
  PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY,
  PREV_BATCH_HASH_KEY,
  CHAINED_PRIORITY_TXN_HASH_KEY,
  NUMBER_OF_LAYER_1_TXS_KEY,
  BLOB_ONE_HASH_KEY,
  BLOB_TWO_HASH_KEY,
  EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY
}

</code></pre>
<p>In addition to the blob commitments, the hash of the total pubdata is still sent and is used if a batch is committed
with pubdata as calldata vs as blob data. As stated earlier, even when we only have enough pubdata for a single blob, 2
system logs are sent. The hash value in the second log in this case will <code>bytes32(0)</code> .</p>
<p>One important thing is that we don’t try to reason about the data here, that is done in the L1Messenger and Compressor
contracts. The main purpose of this is to commit to blobs and have those commitments travel to L1 via system logs.</p>
<h3 id="l1-executor-facet"><a class="header" href="#l1-executor-facet">L1 Executor Facet</a></h3>
<p>While the function signature for <code>commitBatches</code> and the structure of <code>CommitBatchInfo</code> stays the same, the format of
<code>CommitBatchInfo.pubdataCommitments</code> changes. Before 4844, this field held a byte array of pubdata, now it can hold
either the total pubdata as before or it can hold a list of concatenated info for kzg blob commitments. To differentiate
between the two, a header byte is prepended to the byte array. At the moment we only support 2 values:</p>
<pre><code class="language-solidity">/// @dev Enum used to determine the source of pubdata. At first we will support calldata and blobs but this can be extended.
enum PubdataSource {
    Calldata = 0,
    Blob = 1
}
</code></pre>
<p>We reject all other values in the first byte.</p>
<h3 id="calldata-based-pubdata-processing"><a class="header" href="#calldata-based-pubdata-processing">Calldata Based Pubdata Processing</a></h3>
<p>When using calldata, we want to operate on <code>pubdataCommitments[1:pubdataCommitments.length - 32]</code> as this is the full
pubdata that was committed to via system logs. The reason we don’t operate on the last 32 bytes is that we also include
what the blob commitment for this data would be as a way to make our witness generation more generic. Only a single blob
commitment is needed for this as the max size of calldata is the same size as a single blob. When processing the system
logs in this context, we will check the hash of the supplied pubdata without the 1 byte header for pubdata source
against the value in the corresponding system log with key <code>TOTAL_L2_TO_L1_PUBDATA_KEY</code>. We still require logs for the 2
blob commitments, even if these logs contain values we will substitute them for <code>bytes32(0)</code> when constructing the batch
commitment.</p>
<h3 id="blob-based-pubdata-processing"><a class="header" href="#blob-based-pubdata-processing">Blob Based Pubdata Processing</a></h3>
<p>The format for <code>pubdataCommitments</code> changes when we send pubdata as blobs, containing data we need to verify the blob
contents via the newly introduced point evaluation precompile. The data is <code>pubdataCommitments[1:]</code> is the concatenation
of <code>opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)</code> for each blob
attached to the transaction, lowering our calldata from N → 144 bytes per blob. More on how this is used later on.</p>
<p>Utilizing blobs causes us to process logs in a slightly different way. Similar to how it’s done when pubdata is sent via
calldata, we require a system log with a key of the <code>TOTAL_L2_TO_L1_PUBDATA_KEY</code> , although the value is ignored and
extract the 2 blob hashes from the <code>BLOB_ONE_HASH_KEY</code> and <code>BLOB_TWO_HASH_KEY</code> system logs to be used in the batch
commitment.</p>
<p>While calldata verification is simple, comparing the hash of the supplied calldata versus the value in the system log,
we need to take a few extra steps when verifying the blobs attached to the transaction contain the correct data. After
processing the logs and getting the 2 blob linear hashes, we will have all the data we need to call the
<a href="https://eips.ethereum.org/EIPS/eip-4844#point-evaluation-precompile">point evaluation precompile</a>. Recall that the
contents of <code>pubdataCommitments</code> have the opening point (in its 16 byte form), claimed value, the commitment, and the
proof of this claimed value. The last piece of information we need is the blob’s versioned hash (obtained via <code>BLOBHASH</code>
opcode).</p>
<p>There are checks within <code>_verifyBlobInformation</code> that ensure that we have the correct blob linear hashes and that if we
aren’t expecting a second blob, the linear hash should be equal to <code>bytes32(0)</code>. This is how we signal to our circuits
that we didn’t publish any information in the second blob.</p>
<p>Verifying the commitment via the point evaluation precompile goes as follows (note that we assume the header byte for
pubdataSource has already been removed by this point):</p>
<pre><code class="language-solidity">// The opening point is passed as 16 bytes as that is what our circuits expect and use when verifying the new batch commitment
// PUBDATA_COMMITMENT_SIZE = 144 bytes
pubdata_commitments &lt;- [opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)] from calldata
opening_point = bytes32(pubdata_commitments[:16])
versioned_hash &lt;- from BLOBHASH opcode

// Given that we needed to pad the opening point for the precompile, append the data after.
point_eval_input = versioned_hash || opening_point || pubdataCommitments[16: PUBDATA_COMMITMENT_SIZE]

// this part handles the following:
// verify versioned_hash == hash(commitment)
// verify P(z) = y
res &lt;- point_valuation_precompile(point_eval_input)

assert uint256(res[32:]) == BLS_MODULUS
</code></pre>
<p>Where correctness is validated by checking the latter 32 bytes of output from the point evaluation call is equal to
<code>BLS_MODULUS</code>.</p>
<h3 id="batch-commitment-and-proof-of-equivalence"><a class="header" href="#batch-commitment-and-proof-of-equivalence">Batch Commitment and Proof of Equivalence</a></h3>
<p>With the contents of the blob being verified, we need to add this information to the batch commitment so that it can
further be part of the verification of the overall batch by our proof system. Our batch commitment is the hashing of a
few different values: passthrough data (holding our new state root, and next enumeration index to be used), meta
parameters (flag for if zk porter is available, bootloader bytecode hash, and default account bytecode hash), and
auxiliary output. The auxiliary output changes with 4844, adding in 4 new fields and the new corresponding encoding:</p>
<ul>
<li>2 <code>bytes32</code> fields for linear hashes
<ul>
<li>These are the hashes of the blob’s preimages</li>
</ul>
</li>
<li>2 <code>bytes32</code> for 4844 output commitment hashes
<ul>
<li>These are <code>(versioned hash || opening point || evaluation value)</code></li>
<li>The format of the opening point here is expected to be the 16 byte value passed by calldata</li>
</ul>
</li>
<li>We encode an additional 28 <code>bytes32(0)</code> at the end because with the inclusion of vm 1.5.0, our circuits support a
total of 16 blobs that will be used once the total number of blobs supported by ethereum increase.</li>
</ul>
<pre><code class="language-solidity">abi.encode(
    l2ToL1LogsHash,
    _stateDiffHash,
    _batch.bootloaderHeapInitialContentsHash,
    _batch.eventsQueueStateHash,
    _blob1LinearHash,
    _blob1OutputCommitment,
    _blob2LinearHash,
    _blob2OutputCommitment,
    _encode28Bytes32Zeroes()
);
</code></pre>
<p>There are 3 different scenarios that change the values posted here:</p>
<ol>
<li>We submit pubdata via calldata</li>
<li>We only utilize a single blob</li>
<li>We use both blobs</li>
</ol>
<p>When we use calldata, the values <code>_blob1LinearHash</code>, <code>_blob1OutputCommitment</code>, <code>_blob2LinearHash</code>, and
<code>_blob2OutputCommitment</code> should all be <code>bytes32(0)</code>. If we are using blobs but only have a single blob,
<code>_blob1LinearHash</code> and <code>_blob1OutputCommitment</code> should correspond to that blob, while <code>_blob2LinearHash</code> and
<code>_blob2OutputCommitment</code> will be <code>bytes32(0)</code>. Following this, when we use both blobs, the data for these should be
present in all of the values.</p>
<p>Our circuits will then handle the proof of equivalence, following a method similar to the moderate approach mentioned
<a href="https://notes.ethereum.org/@vbuterin/proto_danksharding_faq#Moderate-approach-works-with-any-ZK-SNARK">here</a>, verifying
that the total pubdata can be repackaged as the blobs we submitted and that the commitments in fact evaluate to the
given value at the computed opening point.</p>
<h2 id="pubdata-contents-and-blobs"><a class="header" href="#pubdata-contents-and-blobs">Pubdata Contents and Blobs</a></h2>
<p>Given how data representation changes on the consensus layer (where blobs live) versus on the execution layer (where
calldata is found), there is some preprocessing that takes place to make it compatible. When calldata is used for
pubdata, we keep it as is and no additional processing is required to transform it. Recalling the above section when
pubdata is sent via calldata it has the format: source byte (1 bytes) || pubdata || blob commitment (32 bytes) and so we
must first trim it of the source byte and blob commitment before decoding it. A more detailed guide on the format can be
found in our documentation. Using blobs requires a few more steps:</p>
<pre><code class="language-python">ZKSYNC_BLOB_SIZE = 31 * 4096

# First we pad the pubdata with the required amount of zeroes to fill
# the nearest blobs
padding_amount = ZKSYNC_BLOB_SIZE - len(pubdata) % ZKSYNC_BLOB_SIZE)
padded_pubdata = pad_right_with_zeroes(pubdata, padding_amount)

# We then chunk them into `ZKSYNC_BLOB_SIZE` sized arrays
blobs = chunk(padded_pubdata, ZKSYNC_BLOB_SIZE)

# Each blob is then encoded to be compatible with the CL
for blob in blobs:
    encoded_blob = zksync_pubdata_into_ethereum_4844_data(blob)
</code></pre>
<p>Now we can apply the encoding formula, with some of the data from the blob commit transaction to move from encoded blobs
back into decodable zksync pubdata:</p>
<pre><code class="language-python"># opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)
BLOB_PUBDATA_COMMITMENT_SIZE = 144

# Parse the kzg commitment from the commit calldata
commit_calldata_without_source = commit_calldata[1:]
for i in range(0, len(commit_calldata_without_source), BLOB_PUBDATA_COMMITMENT_SIZE):
    # We can skip the opening point and claimed value, ignoring the proof
    kzg_commitment = commit_calldata_without_source[48:96]

# We then need to pull the blobs in the correct order, this can be found by matching
# each blob with their kzg_commitment keeping the order from the calldata
encoded_blobs = pull_blob_for_each_kzg_commitment(kzg_commitments)

# Decode each blob into the zksync specific format
for encoded_blob in encoded_blobs:
    decoded_blob = ethereum_4844_data_into_zksync_pubdata(encoded_blob)

reconstructed_pubdata = concat(decoded_blobs)
</code></pre>
<p>The last thing to do depends on the strategy taken, the two approaches are:</p>
<ul>
<li>Remove all trailing zeroes after concatenation</li>
<li>Parse the data and ignore the extra zeroes at the end</li>
</ul>
<p>The second option is a bit messier so going with the first, we can then decode the pubdata and when we get to the last
state diff, if the number of bytes is less than specified we know that the remaining data are zeroes. The needed
functions can be found within the
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/3a973afb3cf2b50b7138c1af61cc6ac3d7d0189f/src/eip_4844/mod.rs#L358">zkevm_circuits code</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bytecode-compression"><a class="header" href="#bytecode-compression">Bytecode Compression</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>As we are a rollup - all the bytecodes that contracts used in our chain must be copied into L1 (so that the chain can be
reconstructed from L1 if needed).</p>
<p>Given the want/need to cutdown on space used, bytecode is compressed prior to being posted to L1. At a high level
bytecode is chunked into opcodes (which have a size of 8 bytes), assigned a 2 byte index, and the newly formed byte
sequence (indexes) are verified and sent to L1. This process is split into 2 different parts: (1)
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/utils/src/bytecode.rs#L31">the server side operator</a>
handling the compression and (2)
<a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/Compressor.sol">the system contract</a> verifying
that the compression is correct before sending to L1.</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p>Original bytecode:</p>
<pre><code>0x000000000000000A000000000000000D000000000000000A000000000000000C000000000000000B000000000000000A000000000000000D000000000000000A000000000000000D000000000000000A000000000000000B000000000000000B
</code></pre>
<p>Split to 8-byte chunks:</p>
<pre><code>000000000000000A 000000000000000D 000000000000000A 000000000000000C
000000000000000B 000000000000000A 000000000000000D 000000000000000A
000000000000000D 000000000000000A 000000000000000B 000000000000000B
</code></pre>
<p>Dictionary would be:</p>
<pre><code>0 -&gt; 0xA (count: 5)
1 -&gt; 0xD (count: 3, first seen: 1)
2 -&gt; 0xB (count: 3, first seen: 4)
3 -&gt; 0xC (count: 1)
</code></pre>
<p>Note that <code>1</code> maps to <code>0xD</code>, as it occurs three times, and first occurrence is earlier than first occurrence of <code>0xB</code>,
that also occurs three times.</p>
<p>Compressed bytecode:</p>
<pre><code>0x0004000000000000000A000000000000000D000000000000000B000000000000000C000000010000000300020000000100000001000000020002
</code></pre>
<p>Split into three parts:</p>
<ol>
<li><code>length_of_dict</code> is stored in the first 2 bytes</li>
<li>dictionary entries are stored in the next <code>8 * length_of_dict</code> bytes</li>
<li>2-byte indices are stored in the rest of the bytes</li>
</ol>
<pre><code>0004

000000000000000A 000000000000000D 000000000000000B 000000000000000C

0000 0001 0000 0003
0002 0000 0001 0000
0001 0000 0002 0002
</code></pre>
<h2 id="server-side-operator"><a class="header" href="#server-side-operator">Server Side Operator</a></h2>
<p>This is the part that is responsible for taking bytecode, that has already been chunked into 8 byte words, performing
validation, and compressing it.</p>
<h3 id="validation-rules"><a class="header" href="#validation-rules">Validation Rules</a></h3>
<p>For bytecode to be considered valid it must satisfy the following:</p>
<ol>
<li>Bytecode length must be less than 2097120 ((2^16 - 1) * 32) bytes.</li>
<li>Bytecode length must be a multiple of 32.</li>
<li>Number of words cannot be even.</li>
</ol>
<p><a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/utils/src/bytecode.rs#L133">Source</a></p>
<h3 id="compression-algorithm"><a class="header" href="#compression-algorithm">Compression Algorithm</a></h3>
<p>At a high level, each 8 byte word from the chunked bytecode is assigned a 2 byte index (constraint on size of dictionary
of chunk → index is 2^16 + 1 elements). The length of the dictionary, dictionary entries (index assumed through order),
and indexes are all concatenated together to yield the final compressed version.</p>
<p>The following is a simplified version of the algorithm:</p>
<pre><code class="language-python">
statistic: Map[chunk, (count, first_pos)]
dictionary: Map[chunk, index]
encoded_data: List[index]

for position, chunk in chunked_bytecode:
    if chunk is in statistic:
        statistic[chunk].count += 1
    else:
        statistic[chunk] = (count=1, first_pos=pos)

statistic.sort(primary=count, secondary=first_pos, order=desc)

for chunk in sorted_statistic:
    dictionary[chunk] = len(dictionary) # length of dictionary used to keep track of index

for chunk in chunked_bytecode:
    encoded_data.append(dictionary[chunk])

return [len(dictionary), dictionary.keys(order=index asc), encoded_data]
</code></pre>
<h2 id="system-contract-compression-verification--publishing"><a class="header" href="#system-contract-compression-verification--publishing">System Contract Compression Verification &amp; Publishing</a></h2>
<p>The <a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/Compressor.sol">Bytecode Compressor</a>
contract performs validation on the compressed bytecode generated on the server side. At the current moment, publishing
bytecode to L1 may only be called by the bootloader but in the future anyone will be able to publish compressed bytecode
with no change to the underlying algorithm.</p>
<h3 id="verification--publication"><a class="header" href="#verification--publication">Verification &amp; Publication</a></h3>
<p>The function <code>publishCompressBytecode</code> takes in both the original <code>_bytecode</code> and the <code>_rawCompressedData</code> , the latter
of which comes from the server’s compression algorithm output. Looping over the encoded data, derived from
<code>_rawCompressedData</code> , the corresponding chunks are retrieved from the dictionary and compared to the original byte
code, reverting if there is a mismatch. After the encoded data has been verified, it is published to L1 and marked
accordingly within the <code>KnownCodesStorage</code> contract.</p>
<p>Pseudo-code implementation:</p>
<pre><code class="language-python">length_of_dict = _rawCompressedData[:2]
dictionary = _rawCompressedData[2:2 + length_of_dict * 8] # need to offset by bytes used to store length (2) and multiply by 8 for chunk size
encoded_data = _rawCompressedData[2 + length_of_dict * 8:]

assert(len(dictionary) % 8 == 0) # each element should be 8 bytes
assert(num_entries(dictionary) &lt;= 2^16)
assert(len(encoded_data) * 4 == len(_bytecode)) # given that each chunk is 8 bytes and each index is 2 bytes they should differ by a factor of 4

for index in encoded_data:
    encoded_chunk = dictionary[index]
    real_chunk = _bytecode.readUint64(index * 4) # need to pull from index * 4 to account for difference in element size
    verify(encoded_chunk == real_chunk)

sendToL1(_rawCompressedBytecode)
markPublished(hash(_bytecode), hash(_rawCompressedData), len(_rawCompressedData))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zkevm-internals"><a class="header" href="#zkevm-internals">zkEVM internals</a></h1>
<h2 id="zkevm-clarifier"><a class="header" href="#zkevm-clarifier">zkEVM clarifier</a></h2>
<p>The ZKsync zkEVM plays a fundamentally different role in the zkStack than the EVM does in Ethereum. The EVM is used to
execute code in Ethereum’s state transition function. This STF needs a client to implement and run it. Ethereum has a
multi-client philosophy, there are multiple clients, and they are written in Go, Rust, and other traditional programming
languages, all running and verifying the same STF.</p>
<p>We have a different set of requirements, we need to produce a proof that some client executed the STF correctly. The
first consequence is that the client needs to be hard-coded, we cannot have the same multi-client philosophy. This
client is the zkEVM, it can run the STF efficiently, including execution of smart contracts similarly to the EVM. The
zkEVM was also designed to be proven efficiently.</p>
<p>For efficiency reasons it the zkEVM is similar to the EVM. This makes executing smart programs inside of it easy. It
also has special features that are not in the EVM but are needed for the rollup’s STF, storage, gas metering,
precompiles and other things. Some of these features are implemented as system contracts while others are built into the
VM. System Contracts are contracts with special permissions, deployed at predefined addresses. Finally, we have the
bootloader, which is also a contract, although it is not deployed at any address. This is the STF that is ultimately
executed by the zkEVM, and executes the transaction against the state.</p>
<!-- kl to do *Add different abstraction levels diagram here:*-->
<p>Full specification of the zkEVM is beyond the scope of this document. However, this section will give you most of the
details needed for understanding the L2 system smart contracts &amp; basic differences between EVM and zkEVM. Note also that
usually understanding the EVM is needed for efficient smart contract development. Understanding the zkEVM goes beyond
this, it is needed for developing the rollup itself.</p>
<h2 id="registers-and-memory-management"><a class="header" href="#registers-and-memory-management">Registers and memory management</a></h2>
<p>On EVM, during transaction execution, the following memory areas are available:</p>
<ul>
<li><code>memory</code> itself.</li>
<li><code>calldata</code> the immutable slice of parent memory.</li>
<li><code>returndata</code> the immutable slice returned by the latest call to another contract.</li>
<li><code>stack</code> where the local variables are stored.</li>
</ul>
<p>Unlike EVM, which is stack machine, zkEVM has 16 registers. Instead of receiving input from <code>calldata</code>, zkEVM starts by
receiving a <em>pointer</em> in its first register <em>(<em>basically a packed struct with 4 elements: the memory page id, start and
length of the slice to which it points to</em>)</em> to the calldata page of the parent. Similarly, a transaction can receive
some other additional data within its registers at the start of the program: whether the transaction should invoke the
constructor
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#contractdeployer--immutablesimulator">more about deployments here</a>,
whether the transaction has <code>isSystem</code> flag, etc. The meaning of each of these flags will be expanded further in this
section.</p>
<p><em>Pointers</em> are separate type in the VM. It is only possible to:</p>
<ul>
<li>Read some value within a pointer.</li>
<li>Shrink the pointer by reducing the slice to which pointer points to.</li>
<li>Receive the pointer to the <code>returndata</code> as a calldata.</li>
<li>Pointers can be stored only on stack/registers to make sure that the other contracts can not read <code>memory/returndata</code>
of contracts they are not supposed to.</li>
<li>A pointer can be converted to the u256 integer representing it, but an integer can not be converted to a pointer to
prevent unallowed memory access.</li>
<li>It is not possible to return a pointer that points to a memory page with id smaller than the one for the current page.
What this means is that it is only possible to <code>return</code> only pointer to the memory of the current frame or one of the
pointers returned by the subcalls of the current frame.</li>
</ul>
<h3 id="memory-areas-in-zkevm"><a class="header" href="#memory-areas-in-zkevm">Memory areas in zkEVM</a></h3>
<p>For each frame, the following memory areas are allocated:</p>
<ul>
<li><em>Heap</em> (plays the same role as <code>memory</code> on Ethereum).</li>
<li><em>AuxHeap</em> (auxiliary heap). It has the same properties as Heap, but it is used for the compiler to encode
calldata/copy the <code>returndata</code> from the calls to system contracts to not interfere with the standard Solidity memory
alignment.</li>
<li><em>Stack</em>. Unlike Ethereum, stack is not the primary place to get arguments for opcodes. The biggest difference between
stack on zkEVM and EVM is that on ZKsync stack can be accessed at any location (just like memory). While users do not
pay for the growth of stack, the stack can be fully cleared at the end of the frame, so the overhead is minimal.</li>
<li><em>Code</em>. The memory area from which the VM executes the code of the contract. The contract itself can not read the code
page, it is only done implicitly by the VM.</li>
</ul>
<p>Also, as mentioned in the previous section, the contract receives the pointer to the calldata.</p>
<h3 id="managing-returndata--calldata"><a class="header" href="#managing-returndata--calldata">Managing returndata &amp; calldata</a></h3>
<p>Whenever a contract finishes its execution, the parent’s frame receives a <em>pointer</em> as <code>returndata</code>. This pointer may
point to the child frame’s Heap/AuxHeap or it can even be the same <code>returndata</code> pointer that the child frame received
from some of its child frames.</p>
<p>The same goes with the <code>calldata</code>. Whenever a contract starts its execution, it receives the pointer to the calldata.
The parent frame can provide any valid pointer as the calldata, which means it can either be a pointer to the slice of
parent’s frame memory (heap or auxHeap) or it can be some valid pointer that the parent frame has received before as
calldata/returndata.</p>
<p>Contracts simply remember the calldata pointer at the start of the execution frame (it is by design of the compiler) and
remembers the latest received returndata pointer.</p>
<p>Some important implications of this is that it is now possible to do the following calls without any memory copying:</p>
<p>A → B → C</p>
<p>where C receives a slice of the calldata received by B.</p>
<p>The same goes for returning data:</p>
<p>A ← B ← C</p>
<p>There is no need to copy returned data if the B returns a slice of the returndata returned by C.</p>
<p>Note, that you can <em>not</em> use the pointer that you received via calldata as returndata (i.e. return it at the end of the
execution frame). Otherwise, it would be possible that returndata points to the memory slice of the active frame and
allow editing the <code>returndata</code>. It means that in the examples above, C could not return a slice of its calldata without
memory copying.</p>
<p>Some of these memory optimizations can be seen utilized in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/EfficientCall.sol">EfficientCall</a>
library that allows to perform a call while reusing the slice of calldata that the frame already has, without memory
copying.</p>
<h3 id="returndata--precompiles"><a class="header" href="#returndata--precompiles">Returndata &amp; precompiles</a></h3>
<p>Some of the operations which are opcodes on Ethereum, have become calls to some of the system contracts. The most
notable examples are <code>Keccak256</code>, <code>SystemContext</code>, etc. Note, that, if done naively, the following lines of code would
work differently on ZKsync and Ethereum:</p>
<pre><code class="language-solidity">pop(call(...))
keccak(...)
returndatacopy(...)
</code></pre>
<p>Since the call to keccak precompile would modify the <code>returndata</code>. To avoid this, our compiler does not override the
latest <code>returndata</code> pointer after calls to such opcode-like precompiles.</p>
<h2 id="zkevm-specific-opcodes"><a class="header" href="#zkevm-specific-opcodes">zkEVM specific opcodes</a></h2>
<p>While some Ethereum opcodes are not supported out of the box, some of the new opcodes were added to facilitate the
development of the system contracts.</p>
<p>Note, that this lists does not aim to be specific about the internals, but rather explain methods in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractHelper.sol">SystemContractHelper.sol</a></p>
<h3 id="only-for-kernel-space"><a class="header" href="#only-for-kernel-space"><strong>Only for kernel space</strong></a></h3>
<p>These opcodes are allowed only for contracts in kernel space (i.e. system contracts). If executed in other places they
result in <code>revert(0,0)</code>.</p>
<ul>
<li><code>mimic_call</code>. The same as a normal <code>call</code>, but it can alter the <code>msg.sender</code> field of the transaction.</li>
<li><code>to_l1</code>. Sends a system L2→L1 log to Ethereum. The structure of this log can be seen
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L47">here</a>.</li>
<li><code>event</code>. Emits an L2 log to ZKsync. Note, that L2 logs are not equivalent to Ethereum events. Each L2 log can emit 64
bytes of data (the actual size is 88 bytes, because it includes the emitter address, etc). A single Ethereum event is
represented with multiple <code>event</code> logs constitute. This opcode is only used by <code>EventWriter</code> system contract.</li>
<li><code>precompile_call</code>. This is an opcode that accepts two parameters: the uint256 representing the packed parameters for
it as well as the ergs to burn. Besides the price for the precompile call itself, it burns the provided ergs and
executes the precompile. The action that it does depend on <code>this</code> during execution:
<ul>
<li>If it is the address of the <code>ecrecover</code> system contract, it performs the ecrecover operation</li>
<li>If it is the address of the <code>sha256</code>/<code>keccak256</code> system contracts, it performs the corresponding hashing operation.</li>
<li>It does nothing (i.e. just burns ergs) otherwise. It can be used to burn ergs needed for L2→L1 communication or
publication of bytecodes onchain.</li>
</ul>
</li>
<li><code>setValueForNextFarCall</code> sets <code>msg.value</code> for the next <code>call</code>/<code>mimic_call</code>. Note, that it does not mean that the value
will be really transferred. It just sets the corresponding <code>msg.value</code> context variable. The transferring of ETH
should be done via other means by the system contract that uses this parameter. Note, that this method has no effect
on <code>delegatecall</code> , since <code>delegatecall</code> inherits the <code>msg.value</code> of the previous frame.</li>
<li><code>increment_tx_counter</code> increments the counter of the transactions within the VM. The transaction counter used mostly
for the VM’s internal tracking of events. Used only in bootloader after the end of each transaction.</li>
</ul>
<p>Note, that currently we do not have access to the <code>tx_counter</code> within VM (i.e. for now it is possible to increment it
and it will be automatically used for logs such as <code>event</code>s as well as system logs produced by <code>to_l1</code>, but we can not
read it). We need to read it to publish the <em>user</em> L2→L1 logs, so <code>increment_tx_counter</code> is always accompanied by the
corresponding call to the
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#systemcontext">SystemContext</a>
contract.</p>
<p>More on the difference between system and user logs can be read
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>. -
<code>set_pubdata_price</code> sets the price (in gas) for publishing a single byte of pubdata.</p>
<h3 id="generally-accessible"><a class="header" href="#generally-accessible"><strong>Generally accessible</strong></a></h3>
<p>Here are opcodes that can be generally accessed by any contract. Note that while the VM allows to access these methods,
it does not mean that this is easy: the compiler might not have convenient support for some use-cases yet.</p>
<ul>
<li><code>near_call</code>. It is basically a “framed” jump to some location of the code of your contract. The difference between the
<code>near_call</code> and ordinary jump are:
<ol>
<li>It is possible to provide an ergsLimit for it. Note, that unlike “<code>far_call</code>”s (i.e. calls between contracts) the
63/64 rule does not apply to them.</li>
<li>If the near call frame panics, all state changes made by it are reversed. Please note, that the memory changes will
<strong>not</strong> be reverted.</li>
</ol>
</li>
<li><code>getMeta</code>. Returns an u256 packed value of
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/libraries/SystemContractHelper.sol#L42">ZkSyncMeta</a>
struct. Note that this is not tight packing. The struct is formed by the
<a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/c7ab62f4c60b27dfc690c3ab3efb5fff1ded1a25/src/definitions/abi/meta.rs#L4">following rust code</a>.</li>
<li><code>getCodeAddress</code> — receives the address of the executed code. This is different from <code>this</code> , since in case of
delegatecalls <code>this</code> is preserved, but <code>codeAddress</code> is not.</li>
</ul>
<h3 id="flags-for-calls"><a class="header" href="#flags-for-calls">Flags for calls</a></h3>
<p>Besides the calldata, it is also possible to provide additional information to the callee when doing <code>call</code> ,
<code>mimic_call</code>, <code>delegate_call</code>. The called contract will receive the following information in its first 12 registers at
the start of execution:</p>
<ul>
<li><em>r1</em> — the pointer to the calldata.</li>
<li><em>r2</em> — the pointer with flags of the call. This is a mask, where each bit is set only if certain flags have been set
to the call. Currently, two flags are supported: 0-th bit: <code>isConstructor</code> flag. This flag can only be set by system
contracts and denotes whether the account should execute its constructor logic. Note, unlike Ethereum, there is no
separation on constructor &amp; deployment bytecode. More on that can be read
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#contractdeployer--immutablesimulator">here</a>.
1-st bit: <code>isSystem</code> flag. Whether the call intends a system contracts’ function. While most of the system contracts’
functions are relatively harmless, accessing some with calldata only may break the invariants of Ethereum, e.g. if the
system contract uses <code>mimic_call</code>: no one expects that by calling a contract some operations may be done out of the
name of the caller. This flag can be only set if the callee is in kernel space.</li>
<li>The rest r3..r12 registers are non-empty only if the <code>isSystem</code> flag is set. There may be arbitrary values passed,
which we call <code>extraAbiParams</code>.</li>
</ul>
<p>The compiler implementation is that these flags are remembered by the contract and can be accessed later during
execution via special
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/overview.md">simulations</a>.</p>
<p>If the caller provides inappropriate flags (i.e. tries to set <code>isSystem</code> flag when callee is not in the kernel space),
the flags are ignored.</p>
<h3 id="onlysystemcall-modifier"><a class="header" href="#onlysystemcall-modifier"><code>onlySystemCall</code> modifier</a></h3>
<p>Some of the system contracts can act on behalf of the user or have a very important impact on the behavior of the
account. That’s why we wanted to make it clear that users can not invoke potentially dangerous operations by doing a
simple EVM-like <code>call</code>. Whenever a user wants to invoke some of the operations which we considered dangerous, they must
provide “<code>isSystem</code>” flag with them.</p>
<p>The <code>onlySystemCall</code> flag checks that the call was either done with the “isSystemCall” flag provided or the call is done
by another system contract (since Matter Labs is fully aware of system contracts).</p>
<h3 id="simulations-via-our-compiler"><a class="header" href="#simulations-via-our-compiler">Simulations via our compiler</a></h3>
<p>In the future, we plan to introduce our “extended” version of Solidity with more supported opcodes than the original
one. However, right now it was beyond the capacity of the team to do, so in order to represent accessing ZKsync-specific
opcodes, we use <code>call</code> opcode with certain constant parameters that will be automatically replaced by the compiler with
zkEVM native opcode.</p>
<p>Example:</p>
<pre><code class="language-solidity">function getCodeAddress() internal view returns (address addr) {
  address callAddr = CODE_ADDRESS_CALL_ADDRESS;
  assembly {
    addr := staticcall(0, callAddr, 0, 0xFFFF, 0, 0)
  }
}

</code></pre>
<p>In the example above, the compiler will detect that the static call is done to the constant <code>CODE_ADDRESS_CALL_ADDRESS</code>
and so it will replace it with the opcode for getting the code address of the current execution.</p>
<p>Full list of opcode simulations can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/call.md">here</a>.</p>
<p>We also use
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/verbatim.md">verbatim-like</a>
statements to access ZKsync-specific opcodes in the bootloader.</p>
<p>All the usages of the simulations in our Solidity code are implemented in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractHelper.sol">SystemContractHelper</a>
library and the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractsCaller.sol">SystemContractsCaller</a>
library.</p>
<p><strong>Simulating</strong> <code>near_call</code> <strong>(in Yul only)</strong></p>
<p>In order to use <code>near_call</code> i.e. to call a local function, while providing a limit of ergs (gas) that this function can
use, the following syntax is used:</p>
<p>The function should contain <code>ZKSYNC_NEAR_CALL</code> string in its name and accept at least 1 input parameter. The first input
parameter is the packed ABI of the <code>near_call</code>. Currently, it is equal to the number of ergs to be passed with the
<code>near_call</code>.</p>
<p>Whenever a <code>near_call</code> panics, the <code>ZKSYNC_CATCH_NEAR_CALL</code> function is called.</p>
<p><em>Important note:</em> the compiler behaves in a way that if there is a <code>revert</code> in the bootloader, the
<code>ZKSYNC_CATCH_NEAR_CALL</code> is not called and the parent frame is reverted as well. The only way to revert only the
<code>near_call</code> frame is to trigger VM’s <em>panic</em> (it can be triggered with either invalid opcode or out of gas error).</p>
<p><em>Important note 2:</em> The 63/64 rule does not apply to <code>near_call</code>. Also, if 0 gas is provided to the near call, then
actually all of the available gas will go to it.</p>
<h3 id="notes-on-security"><a class="header" href="#notes-on-security">Notes on security</a></h3>
<p>To prevent unintended substitution, the compiler requires <code>--system-mode</code> flag to be passed during compilation for the
above substitutions to work.</p>
<h2 id="bytecode-hashes"><a class="header" href="#bytecode-hashes">Bytecode hashes</a></h2>
<p>On ZKsync the bytecode hashes are stored in the following format:</p>
<ul>
<li>The 0th byte denotes the version of the format. Currently the only version that is used is “1”.</li>
<li>The 1st byte is <code>0</code> for deployed contracts’ code and <code>1</code> for the contract code
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#constructing-vs-non-constructing-code-hash">that is being constructed</a>.</li>
<li>The 2nd and 3rd bytes denote the length of the contract in 32-byte words as big-endian 2-byte number.</li>
<li>The next 28 bytes are the last 28 bytes of the sha256 hash of the contract’s bytecode.</li>
</ul>
<p>The bytes are ordered in little-endian order (i.e. the same way as for <code>bytes32</code> ).</p>
<h3 id="bytecode-validity"><a class="header" href="#bytecode-validity">Bytecode validity</a></h3>
<p>A bytecode is valid if it:</p>
<ul>
<li>Has its length in bytes divisible by 32 (i.e. consists of an integer number of 32-byte words).</li>
<li>Has a length of less than 2^16 words (i.e. its length in words fits into 2 bytes).</li>
<li>Has an odd length in words (i.e. the 3rd byte is an odd number).</li>
</ul>
<p>Note, that it does not have to consist of only correct opcodes. In case the VM encounters an invalid opcode, it will
simply revert (similar to how EVM would treat them).</p>
<p>A call to a contract with invalid bytecode can not be proven. That is why it is <strong>essential</strong> that no contract with
invalid bytecode is ever deployed on ZKsync. It is the job of the
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#knowncodestorage">KnownCodesStorage</a>
to ensure that all allowed bytecodes in the system are valid.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intuition-guide-to-zk-in-zkevm"><a class="header" href="#intuition-guide-to-zk-in-zkevm">Intuition guide to ZK in zkEVM</a></h1>
<p><strong>WARNING</strong>: This guide simplifies the complex details of how we use ZK in our systems, just to give you a better
understanding. We’re leaving out a lot of details to keep things brief.</p>
<h2 id="what-is-the-zero-knowledge"><a class="header" href="#what-is-the-zero-knowledge">What is the ‘Zero Knowledge’</a></h2>
<p>In our case, the prover takes public input and witness (which is huge - you’ll see below), and produces a proof, but the
verifier takes (public input, proof) only, without witness. This means that the huge witness doesn’t have to be
submitted to L1. This property can be used for many things, like privacy, but here we use it to implement an efficient
rollup that publishes the least required amount of data to L1.</p>
<h2 id="basic-overview"><a class="header" href="#basic-overview">Basic overview</a></h2>
<p>Let’s break down the basic steps involved when a transaction is made within our ZK system:</p>
<ul>
<li><strong>Execute transaction in State Keeper &amp; Seal the block:</strong> This part has been discussed in other articles.</li>
<li><strong>Generate witness:</strong> What’s that? Let’s find out below!</li>
<li><strong>Generate proof:</strong> This is where some fancy math and computing power comes in.</li>
<li><strong>Verify proof on L1:</strong> This means checking that the fancy math was done right on the Ethereum network (referred to as
L1).</li>
</ul>
<h2 id="what-it-means-to-generate-a-witness"><a class="header" href="#what-it-means-to-generate-a-witness">What It Means to Generate a Witness</a></h2>
<p>When our State Keeper processes a transaction, it carries out a bunch of operations and assumes certain conditions
without openly stating them. However, when it comes to ZK, we need to show clear evidence that these conditions hold.</p>
<p>Take this simple example where we have a command that retrieves some data from storage and assigns it to a variable.</p>
<p><code>a := SLOAD(0x100)</code></p>
<p>In normal circumstances, the system would just read the data from storage and assign it. But in ZK, we need to provide
evidence of what specific data was fetched and that it was indeed present in the storage beforehand.</p>
<p>From the ZK point of view, this looks like:</p>
<pre><code>circuit inputs:
* current_state_hash = 0x1234;
* read_value: 44
* merkle_path proving that (0x100, 44) exists in tree with storage hash 0x1234
circuit outputs:
* new state hash (that includes the leaf saying that variable 'a' has value 44)
</code></pre>
<p><strong>Note</strong>: In reality, we also use multiple Queues with hashes (together with merkle trees), to track all the memory &amp;
storage accesses.</p>
<p>So, in our example, what seems like a simple action actually requires us to create a bunch of hashes and merkle paths.
This is precisely what the Witness Generator does. It processes the transactions, one operation at a time, and generates
the necessary data that will be used later in circuits.</p>
<h3 id="a-closer-look"><a class="header" href="#a-closer-look">A Closer Look</a></h3>
<p>Now let’s dive into a specific example <a href="https://github.com/matter-labs/era-zkevm_test_harness/tree/main/src/witness/individual_circuits/decommit_code.rs#L24">witness_example</a>:</p>
<pre><pre class="playground"><code class="language-rust="><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn compute_decommitter_circuit_snapshots&lt;
    E: Engine,
    R: CircuitArithmeticRoundFunction&lt;E, 2, 3&gt;,
&gt;(
...
) -&gt; (
    Vec&lt;CodeDecommitterCircuitInstanceWitness&lt;E&gt;&gt;,
    CodeDecommitmentsDeduplicatorInstanceWitness&lt;E&gt;,
)
<span class="boring">}</span></code></pre></pre>
<p>In this code snippet, we’re looking at a function named <code>compute_decommitter_circuit_snapshots</code>. It uses some technical
terms and concepts that may seem daunting, but let’s break them down:</p>
<p><strong>Engine:</strong> This is a trait that specifically handles complex mathematical curves, called Elliptic curves. It’s like
your uint64 on steroids!</p>
<p><strong>CircuitArithmeticRoundFunction:</strong> This is a special kind of hashing function that’s more suited for the circuits we
are using than the regular ones like keccak. In our case, we use Franklin and Rescue from <a href="https://github.com/matter-labs/franklin-crypto">franklin repo</a>.</p>
<p>The function returns Witness classes, that contain queues such as <code>FixedWidthEncodingSpongeLikeQueueWitness</code> which hold
the hashes we mentioned earlier. This is similar merkle paths that we discussed above.</p>
<h3 id="where-is-the-code"><a class="header" href="#where-is-the-code">Where is the Code</a></h3>
<p>The job of generating witnesses, which we discussed earlier, is handled by the witness generator. Initially, this was
located in a module [zksync core witness]. However, for the new proof system, the team began to shift this function to a
new location called <a href="https://github.com/matter-labs/zksync-era/blob/main/prover/crates/bin/witness_generator/src/main.rs">separate witness binary</a>.</p>
<p>Inside this new location, after the necessary data is fetched from storage, the witness generator calls another piece of
code from <a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/fb47657ae3b6ff6e4bb5199964d3d37212978200/src/external_calls.rs#L579">zkevm_test_harness witness</a> named <code>run_with_fixed_params</code>. This code is responsible for creating the witnesses
themselves (which can get really HUGE).</p>
<h2 id="generating-the-proof"><a class="header" href="#generating-the-proof">Generating the Proof</a></h2>
<p>Once we have the witness data lined up, it’s time to crunch the numbers and create the proofs.</p>
<p>The main goal of this step is to take an operation (for example, a calculation called <code>ecrecover</code>) and break it down
into smaller pieces. Then, we represent this information as a special mathematical expression called a polynomial.</p>
<p>To construct these polynomials, we use something called a <code>ConstraintSystem</code>. The specific type that we use is called
zkSNARK, and our custom version of it is named bellman. You can find our code for this in the <a href="https://github.com/matter-labs/bellman">bellman repo</a>. Additionally,
we have an optimized version that’s designed to run faster on certain types of hardware (using CUDA technology), which you
can find in the <a href="https://github.com/matter-labs/era-bellman-cuda">bellman cuda repo</a>.</p>
<p>An <a href="https://github.com/matter-labs/era-sync_vm/blob/v1.3.2/src/glue/ecrecover_circuit/mod.rs#L157">example ecrecover circuit</a> might give you a clearer picture of what this looks like in practice.</p>
<p>The proof itself is generated by evaluating this polynomial expression at many different points. Because this involves
heavy calculations, we use GPUs to speed things up.</p>
<h3 id="where-is-the-code-1"><a class="header" href="#where-is-the-code-1">Where is the Code</a></h3>
<p>The main code that utilizes the GPUs to create proofs is located in a repository named <a href="https://github.com/matter-labs/era-heavy-ops-service">heavy_ops_service repo</a>. This code
combines elements from the <a href="https://github.com/matter-labs/era-bellman-cuda">bellman cuda repo</a> that we mentioned earlier, along with a huge amount of data generated by the
witness, to produce the final proofs.</p>
<h2 id="what-does-verify-proof-on-l1-mean"><a class="header" href="#what-does-verify-proof-on-l1-mean">What Does “Verify Proof on L1” Mean</a></h2>
<p>Finally, we reach the stage where we have to verify the proof on L1. But what does that really mean?</p>
<p>We need to ensure that four specific values match:</p>
<ul>
<li><strong>C</strong>: This is a value that represents our circuits, also known as verification keys. It’s like a fingerprint of the
circuit code and is hard-coded into the contract. Whenever the circuit changes, this value changes too.</li>
<li><strong>In</strong>: This represents the root hash before the transaction block.</li>
<li><strong>Out</strong>: This represents the root hash after the transaction block.</li>
<li><strong>P</strong>: This is the proof provided by the prover.</li>
</ul>
<p>The logic behind this is that there can only be a matching proof ‘P’ if <code>C(In) == Out</code>. In simple terms, it means that
the proof ‘P’ will only make sense if the values before and after the transaction block are consistent according to the
circuit represented by ‘C’.</p>
<p>If you’re eager to dive into the nitty-gritty, you can find the code in the <a href="https://github.com/matter-labs/era-contracts/blob/main/l1-contracts/contracts/zksync/Verifier.sol">verifier</a> repository. Also, if you’re
interested in learning even more, you can look up KZG commitments.</p>
<h2 id="a-heads-up-about-code-versions"><a class="header" href="#a-heads-up-about-code-versions">A Heads-Up About Code Versions</a></h2>
<p>Please be aware that there are multiple versions of the proving systems, such as v1.3.1, v1.3.2, and so on. When you’re
looking through the code, make sure you’re checking the version that’s relevant to what you’re working on. At the time
this guide was written, the latest version was 1.3.4, but there was also ongoing development on a new proof system in
version 1.4.0.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="proof-system-deeper-overview"><a class="header" href="#proof-system-deeper-overview">Proof System Deeper Overview</a></h1>
<p>The purpose of this section is to explain our new proof system from an engineering standpoint. We will examine the code
examples and how the libraries communicate.</p>
<p>Let’s begin by discussing our constraint system. In the previous prover, we utilized the Bellman repository. However, in
the new prover, the constraint system is implemented in Boojum.</p>
<h2 id="constraint-system"><a class="header" href="#constraint-system">Constraint system</a></h2>
<p>If you look at boojum repo (src/cs/traits/cs.rs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ConstraintSystem&lt;F: SmallField&gt;: Send + Sync {

    ...
    fn alloc_variable() -&gt; Variable;
    fn alloc_witness_without_value(&amp;mut self) -&gt; Witness;
    fn place_gate&lt;G: Gate&lt;F&gt;&gt;(&amp;mut self, gate: &amp;G, row: usize);
    ...
}
<span class="boring">}</span></code></pre></pre>
<p>We have three main components: <code>Variable</code>, <code>Witness</code>, and <code>Gate</code>.</p>
<p>To understand the constraint system, imagine it as a list of “placeholders” called Variables. We define rules, referred
to as “gates” for these Variables. The Witness represents a specific assignment of values to these Variables, ensuring
that the rules still hold true.</p>
<p>Conceptually, this is similar to how we implement functions. Consider the following example:</p>
<pre><code>fn fun(x) {
  y = x + A;
  z = y * B;
  w = if y { z } else { y }
}
</code></pre>
<p>In this code snippet, <code>A</code>, <code>B</code>, <code>y</code>, <code>z</code>, and <code>w</code> are Variables (with <code>A</code> and <code>B</code> being constants). We establish rules,
or gates, specifying that the Variable <code>z</code> must equal <code>y</code> multiplied by the Variable <code>B</code>.</p>
<p>Example Witness assignment would be:</p>
<pre><code> x = 1; A = 3; y = 3; B = 0; z = 0; w = 3;
</code></pre>
<p>Gates can become more complex. For instance, the <code>w</code> case demonstrates a “selection” gate, which chooses one of two
options depending on a condition.</p>
<p>Now, let’s delve into this gate for a more detailed examination:</p>
<h3 id="selection-gate"><a class="header" href="#selection-gate">Selection gate</a></h3>
<p>The code is in boojum/src/cs/gates/selection_gate.rs</p>
<p>Let’s delve deeper into the concept. Our goal is to create a gate that implements the logic
<code>result = if selector == true a else b;</code>. To accomplish this, we will require four variables.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SelectionGate {
    pub a: Variable,
    pub b: Variable,
    pub selector: Variable,
    pub result: Variable,
}
<span class="boring">}</span></code></pre></pre>
<p>Internally the <code>Variable</code> object is <code>pub struct Variable(pub(crate) u64);</code> - so it is an index to the position within
the constraint system object.</p>
<p>And now let’s see how we can add this gate into the system.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select&lt;F: SmallField, CS: ConstraintSystem&lt;F&gt;&gt;(
    cs: &amp;mut CS,
    a: Variable,
    b: Variable,
    selector: Variable,
) -&gt; Variable {
  //  First, let's allocate the output variable:
  let output_variable = cs.alloc_variable_without_value();
  ...
}
<span class="boring">}</span></code></pre></pre>
<p>And then there is a block of code for witness evaluation (let’s skip it for now), and the final block that adds the gate
to the constraint system <code>cs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    if &lt;CS::Config as CSConfig&gt;::SetupConfig::KEEP_SETUP {
        let gate = Self {
            a,
            b,
            selector,
            result: output_variable,
        };
        gate.add_to_cs(cs);
    }

    output_variable
<span class="boring">}</span></code></pre></pre>
<p>So to recap - we took 3 ‘Variables’, created the output one, created a <code>SelectionGate</code> object out of them, which we
added to the system (by calling <code>add_to_cs</code>) - and the finally returned the output variable.</p>
<p>But where is the ‘logic’? Where do we actually enforce the constraint?</p>
<p>For this, we have to look at the <code>Evaluator</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: SmallField&gt; Gate&lt;F&gt; for SelectionGate {
    type Evaluator = SelectionGateConstraitEvaluator;

    #[inline]
    fn evaluator(&amp;self) -&gt; Self::Evaluator {
        SelectionGateConstraitEvaluator
    }
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: PrimeField&gt; GateConstraitEvaluator&lt;F&gt; for SelectionGateConstraitEvaluator {
  fn evaluate_once{
    let a = trace_source.get_variable_value(0);
    let b = trace_source.get_variable_value(1);
    let selector = trace_source.get_variable_value(2);
    let result = trace_source.get_variable_value(3);

    // contribution = a * selector
    let mut contribution = a;
    contribution.mul_assign(&amp;selector, ctx);

    // tmp = 1 - selector
    let mut tmp = P::one(ctx);
    tmp.sub_assign(&amp;selector, ctx);

    // contribution += tmp * b
    // So:
    // contribution = a*selector + (1-selector) * b
    P::mul_and_accumulate_into(&amp;mut contribution, &amp;tmp, &amp;b, ctx);

    // contribution = a*selector + (1-selector) * b - result
    contribution.sub_assign(&amp;result, ctx);

    // And if we're successful, the contribution == 0.
    // Because result == a * selector + (1-selector) * b
    destination.push_evaluation_result(contribution, ctx);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This evaluator is actually operating on top of the <code>Field</code> objects, trying to build &amp; evaluate the correct polynomials.
The details of it will be covered in a separate article.</p>
<p>Congratulations, you hopefully understood the code for the first gate. To recap - we created the ‘output’ Variable, and
added the Gate to the CS system. Later when CS system ‘computes’ all the dependencies, it will run the constraint
evaluator, to add the ‘raw’ dependency (which is basically an equation) to the list.</p>
<p>You can look into other files in <code>src/cs/gates</code> to see other examples.</p>
<h2 id="structures"><a class="header" href="#structures">Structures</a></h2>
<p>Now, that we handled the basic variables, let’s see what we can do with more complex structures. Boojum has added a
bunch of derive macros, to make development easier.</p>
<p>Let’s look at the example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Derivative, CSSelectable, CSAllocatable, CSVarLengthEncodable, WitnessHookable)]
pub struct VmLocalState&lt;F: SmallField&gt; {
    pub previous_code_word: UInt256&lt;F&gt;,
    pub registers: [VMRegister&lt;F&gt;; REGISTERS_COUNT],
    pub flags: ArithmeticFlagsPort&lt;F&gt;,
    pub timestamp: UInt32&lt;F&gt;,
    pub memory_page_counter: UInt32&lt;F&gt;,
<span class="boring">}</span></code></pre></pre>
<p>First - all the UInt that you see above, are actually implemented in Boojum:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt32&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
}
impl&lt;F: SmallField&gt; CSAllocatable&lt;F&gt; for UInt32&lt;F&gt; {
    // So the 'witness' type (concrete value) for U32 is u32 - no surprises ;-)
    type Witness = u32;
    ...
}

pub struct UInt256&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 8],
}
<span class="boring">}</span></code></pre></pre>
<h3 id="witnesshookable"><a class="header" href="#witnesshookable">WitnessHookable</a></h3>
<p>In the example above, the Witness type for U32, was u32 - easy. But what should we do, when we have more complex struct
(like VmLocalState)?</p>
<p>This derive will automatically create a new struct named XXWitness (in example above <code>VmLocalStateWitness</code>), that can be
filled with concrete values.</p>
<h3 id="csallocatable"><a class="header" href="#csallocatable">CsAllocatable</a></h3>
<p>Implements CsAllocatable - which allows you to directly ‘allocate’ this struct within constraint system (similarly to
how we were operating on regular ‘Variables’ above).</p>
<h3 id="csselectable"><a class="header" href="#csselectable">CSSelectable</a></h3>
<p>Implements the <code>Selectable</code> trait - that allows this struct to participate in operations like conditionally select (so
it can be used as ‘a’ or ‘b’ in the Select gate example above).</p>
<h3 id="csvarlengthencodable"><a class="header" href="#csvarlengthencodable">CSVarLengthEncodable</a></h3>
<p>Implements CircuitVarLengthEncodable - which allows encoding the struct into a vector of variables (think about it as
serializing to Bytes).</p>
<h3 id="summary-5"><a class="header" href="#summary-5">Summary</a></h3>
<p>Now with the tools above, we can do operations on our constraint system using more complex structures. So we have gates
as ‘complex operators’ and structures as complex object. Now we’re ready to start taking it to the next level: Circuits.</p>
<h2 id="circuits"><a class="header" href="#circuits">Circuits</a></h2>
<p>Circuit’s definitions are spread across 2 separate repositories: <code>zkevm_circuits</code> and <code>zkevm_test_harness</code>.</p>
<p>While we have around 9 different circuits (log_sorter, ram_permutation etc) - in this article we’ll focus only on the
one: MainVM - which is responsible for handling almost all of the VM operations (other circuits are used to handle some
of the precompiles, and operations that happen after VM was run - like preparing pubdata etc).</p>
<p>Looking at zkevm_test_harness, we can see the definition:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub type VMMainCircuit&lt;F, W, R&gt; =
    ZkSyncUniformCircuitInstance&lt;F, VmMainInstanceSynthesisFunction&lt;F, W, R&gt;&gt;;
<span class="boring">}</span></code></pre></pre>
<h3 id="so-what-is-a-circuit"><a class="header" href="#so-what-is-a-circuit">So what is a circuit</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ZkSyncUniformCircuitInstance&lt;F: SmallField, S: ZkSyncUniformSynthesisFunction&lt;F&gt;&gt; {
    // Assignment of values to all the Variables.
    pub witness: AtomicCell&lt;Option&lt;S::Witness&gt;&gt;,

    // Configuration - that is circuit specific, in case of MainVM - the configuration
    // is simply the amount of opcodes that we put within 1 circuit.
    pub config: std::sync::Arc&lt;S::Config&gt;,

    // Circuit 'friendly' hash function.
    pub round_function: std::sync::Arc&lt;S::RoundFunction&gt;,

    // Inputs to the circuits.
    pub expected_public_input: Option&lt;[F; INPUT_OUTPUT_COMMITMENT_LENGTH]&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>Notice - that circuit doesn’t have any ‘explicit’ outputs.</p>
<p>Where ZkSyncUniformCircuitInstance is a proxy, so let’s look deeper, into the main function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl VmMainInstanceSynthesisFunction {
    fn synthesize_into_cs_inner&lt;CS: ConstraintSystem&lt;F&gt;&gt;(
        cs: &amp;mut CS,
        witness: Self::Witness,
        round_function: &amp;Self::RoundFunction,
        config: Self::Config,
    ) -&gt; [Num&lt;F&gt;; INPUT_OUTPUT_COMMITMENT_LENGTH] {
        main_vm_entry_point(cs, witness, round_function, config)
    }
}

<span class="boring">}</span></code></pre></pre>
<p>This is the main logic, that takes the witness (remember - Witness is a concrete assignment of values to Variables) -
and returns the public input.</p>
<p>If we look deeper into ‘main_vm_entry_point’ (which is already in zkevm_circuits repo), we can see:</p>
<pre><pre class="playground"><code class="language-rust">pub fn main_vm_entry_point(
    cs: &amp;mut CS,
    witness: VmCircuitWitness&lt;F, W&gt;,
    round_function: &amp;R,
    limit: usize,
) -&gt; [Num&lt;F&gt;; INPUT_OUTPUT_COMMITMENT_LENGTH]</code></pre></pre>
<p>And in this function we do following operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Prepare current 'state'
    //
    // First - unpack the witness
    let VmCircuitWitness {
        closed_form_input,
        witness_oracle,
    } = witness;

    // And add it to the constraint system
    let mut structured_input =
        VmCircuitInputOutput::alloc_ignoring_outputs(cs, closed_form_input.clone());

    let mut state =
        VmLocalState::conditionally_select(cs, start_flag, &amp;bootloader_state, &amp;hidden_fsm_input);

    // Notice, that now state is a VmLocalState object -- which contains 'Variables' inside.

    // And now run the cycles
    for _cycle_idx in 0..limit {
        state = vm_cycle(
            cs,
            state,
            &amp;synchronized_oracle,
            &amp;per_block_context,
            round_function,
        );
    }
<span class="boring">}</span></code></pre></pre>
<p>The <code>vm_cycle</code> method is where the magic is - it takes a given opcode, and creates all the necessary gates, temporary
Variables etc inside the Constraint system. This method is around 800 lines long, so I’d encourage you to take a sneak
peek if you’re interested.</p>
<p>Now that we’ve added all the constraints for the ‘limit’ number of opcodes, we have to do some additional housekeeping -
like storing the Queue hashes (for memory, code decommitment etc).</p>
<p>And then we’re ready to prepare the result of this method (input_commitment).</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Prepare compact form (that contains just the hashes of values, rather than full values).
    let compact_form =
        ClosedFormInputCompactForm::from_full_form(cs, &amp;structured_input, round_function);

    // And serialize it.
    let input_commitment: [_; INPUT_OUTPUT_COMMITMENT_LENGTH] =
        commit_variable_length_encodable_item(cs, &amp;compact_form, round_function);
    input_commitment
<span class="boring">}</span></code></pre></pre>
<h2 id="and-now-putting-it-all-together"><a class="header" href="#and-now-putting-it-all-together">And now putting it all together</a></h2>
<p>Now let’s look at the zkevm_test_harness repo, ‘/src/external_calls.rs’ run method. This is used in many tests, and
tries to execute the whole flow end to end.</p>
<p>And while the signature is quite scary - let’s walk through this together:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn run&lt;
    F: SmallField,
    R: BuildableCircuitRoundFunction&lt;F, 8, 12, 4&gt; + AlgebraicRoundFunction&lt;F, 8, 12, 4&gt; + serde::Serialize + serde::de::DeserializeOwned,
    H: RecursiveTreeHasher&lt;F, Num&lt;F&gt;&gt;,
    EXT: FieldExtension&lt;2, BaseField = F&gt;,
    S: Storage
&gt;(
caller: Address, // for real block must be zero
entry_point_address: Address, // for real block must be the bootloader
entry_point_code: Vec&lt;[u8; 32]&gt;, // for read lobkc must be a bootloader code
initial_heap_content: Vec&lt;u8&gt;, // bootloader starts with non-deterministic heap
    zk_porter_is_available: bool,
    default_aa_code_hash: U256,
used_bytecodes: std::collections::HashMap&lt;U256, Vec&lt;[u8; 32]&gt;&gt;, // auxiliary information to avoid passing a full set of all used codes
ram_verification_queries: Vec&lt;(u32, U256)&gt;, // we may need to check that after the bootloader's memory is filled
    cycle_limit: usize,
round_function: R, // used for all queues implementation
    geometry: GeometryConfig,
    storage: S,
    tree: &amp;mut impl BinarySparseStorageTree&lt;256, 32, 32, 8, 32, Blake2s256, ZkSyncStorageLeaf&gt;,
) -&gt; (
    BlockBasicCircuits&lt;F, R&gt;,
    BlockBasicCircuitsPublicInputs&lt;F&gt;,
    BlockBasicCircuitsPublicCompactFormsWitnesses&lt;F&gt;,
    SchedulerCircuitInstanceWitness&lt;F, H, EXT&gt;,
    BlockAuxilaryOutputWitness&lt;F&gt;,
)
    where [(); &lt;crate::zkevm_circuits::base_structures::log_query::LogQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::base_structures::memory_query::MemoryQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::base_structures::decommit_query::DecommitQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::boojum::gadgets::u256::UInt256&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::boojum::gadgets::u256::UInt256&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN + 1]:,
    [(); &lt;crate::zkevm_circuits::base_structures::vm_state::saved_context::ExecutionContextRecord&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::storage_validity_by_grand_product::TimestampedStorageLogRecord&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,

<span class="boring">}</span></code></pre></pre>
<p>The first section, is adding some decommitments (explain later).</p>
<p>Then we create a vm:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let mut out_of_circuit_vm =
        create_out_of_circuit_vm(&amp;mut tools, &amp;block_properties, caller, entry_point_address);
<span class="boring">}</span></code></pre></pre>
<p>And we’ll run it over all the operands:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    for _cycle in 0..cycle_limit {

        out_of_circuit_vm
            .cycle(&amp;mut tracer)
            .expect("cycle should finish successfully");
    }
<span class="boring">}</span></code></pre></pre>
<p>While doing it, we collect ‘snapshots’ - the detailed information of the state of the system between each operand.</p>
<p>Then we create a <code>Vec&lt;VmInstanceWitness&gt;</code> - let’s see what’s inside:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct VmInstanceWitness&lt;F: SmallField, O: WitnessOracle&lt;F&gt;&gt; {
    // we need everything to start a circuit from this point of time

    // initial state (state of registers etc)
    pub initial_state: VmLocalState,
    pub witness_oracle: O,
    pub auxilary_initial_parameters: VmInCircuitAuxilaryParameters&lt;F&gt;,
    pub cycles_range: std::ops::Range&lt;u32&gt;,

    // final state for test purposes
    pub final_state: VmLocalState,
    pub auxilary_final_parameters: VmInCircuitAuxilaryParameters&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>With this, let’s finally start creating circuits (via <code>create_leaf_level_circuits_and_scheduler_witness</code>)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span>
<span class="boring">fn main() {
</span>    for (instance_idx, vm_instance) in vm_instances_witness.into_iter().enumerate() {
         let instance = VMMainCircuit {
            witness: AtomicCell::new(Some(circuit_input)),
            config: Arc::new(geometry.cycles_per_vm_snapshot as usize),
            round_function: round_function.clone(),
            expected_public_input: Some(proof_system_input),
        };

        main_vm_circuits.push(instance);
    }
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prover-and-keys"><a class="header" href="#prover-and-keys">Prover and keys</a></h1>
<p>You might have come across terms like “prover”, “keys”, and phrases like “regenerating the verification key” or “why is
the key 8GB in size?” or even “the proof failed because the verification key hash was different.” It can all seem a bit
complex, but don’t worry, we’re here to make it simple.</p>
<p>In this article, we’re going to break down the different types of keys, explain their purposes, and show you how they
are created.</p>
<p>Our main focus will be on the boojum, a new proof system. But if you’re familiar with the old proof system, the
principles we’ll discuss apply there as well.</p>
<h2 id="circuits-1"><a class="header" href="#circuits-1">Circuits</a></h2>
<p><img src="https://user-images.githubusercontent.com/128217157/275817097-0a543476-52e5-437b-a7d3-10603d5833fa.png" alt="circuits" /></p>
<p>We offer 13 distinct types of <strong>‘base’ circuits</strong>, including Vm, Decommitter, and others, which you can view in the
<a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/circuit_definitions/base_layer/mod.rs#L77">full list here</a>. In addition, there are 15 <strong>‘recursive’ circuits</strong>. Out of these, 13 are ‘leaves,’
each corresponding to a basic type, while one is a ‘node,’ and another is a ‘scheduler’ that oversees all others. You
can find more details in the <a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/circuit_definitions/recursion_layer/mod.rs#L29">full list here</a>.</p>
<p>In our new proof system, there’s also a final element known as the compressor, or <strong>snark wrapper</strong>, representing an
additional type of circuit.</p>
<p>It’s essential to note that each circuit type requires its unique set of keys.</p>
<p>Also, the base circuits, leaves, node and scheduler are STARK based with FRI commitments, while the snark wrapper is
SNARK based with KZG commitment. This results in slightly different contents of the keys, but their role stays the same.</p>
<h2 id="keys-1"><a class="header" href="#keys-1">Keys</a></h2>
<h3 id="setup-key-big-14gb"><a class="header" href="#setup-key-big-14gb">Setup key (big, 14GB)</a></h3>
<blockquote>
<p>In the following <a href="https://github.com/matter-labs/zksync-era/blob/main/prover/setup-data-cpu-keys.json">CPU</a> and
<a href="https://github.com/matter-labs/zksync-era/blob/main/prover/setup-data-gpu-keys.json">GPU</a> links, you’ll find GCS
buckets containing the latest keys.</p>
</blockquote>
<p>The primary key for a given circuit is called <code>setup key</code>. These keys can be substantial in size - approximately 14GB
for our circuits. Due to their size, we don’t store them directly on GitHub; instead, they need to be generated.</p>
<p>If you’re wondering what these setup keys contain, think of them as the ‘source code of the circuit.’</p>
<p>This implies that any modifications to a circuit necessitate the regeneration of the setup keys to align with the
changes made.</p>
<h3 id="verification-key-small-8kb"><a class="header" href="#verification-key-small-8kb">Verification key (small, 8kb)</a></h3>
<p>To generate the proof, we need the setup key. However, to verify the proof, a much smaller key, known as the
<code>verification key</code>, is required.</p>
<p>These verification keys are available on GitHub, and you can view them <a href="https://github.com/matter-labs/zksync-era/tree/6d18061df4a18803d3c6377305ef711ce60317e1/prover/data/keys">here</a>. Each verification
key is stored in a separate file. They are named in the format <code>verification_X_Y_key.json</code>, for example,
<code>verification_basic_4_key.json</code>.</p>
<p>Comparing these files with the list of circuits mentioned earlier, you’ll notice there are 13 files named
<code>verification_basic_Y</code>, 15 files for leaf, one each for node and scheduler, and an additional one for wrapper.</p>
<p>In simpler terms, each verification key contains multiple ‘hashes’ or commitments, derived from different sections of
the setup key. These hashes enable the proof verification process.</p>
<h3 id="verification-key-hash-very-small-32-bytes"><a class="header" href="#verification-key-hash-very-small-32-bytes">Verification key hash (very small, 32 bytes)</a></h3>
<p>The hash of the verification key serves a quick reference to ensure that both parties involved are using the same keys.
For instance:</p>
<ul>
<li>Our state keeper uses this hash to confirm that the L1 contract possesses the correct key.</li>
<li>The witness generator refers to this hash to determine which jobs it should take on.</li>
</ul>
<p>Typically, we embed these hashes directly into an environment variable for easy access. You can find an example of this
<a href="https://github.com/matter-labs/zksync-era/blob/6d18061df4a18803d3c6377305ef711ce60317e1/etc/env/base/contracts.toml#L61">here for SNARK_WRAPPER_VK_HASH</a>.</p>
<h2 id="crs-files-setup_226key-8gb-files"><a class="header" href="#crs-files-setup_226key-8gb-files">CRS files (setup_2^26.key, 8GB files)</a></h2>
<p>These keys, also referred to as Common Reference Strings (CRS), are essential for KZG commitments and were a crucial
part of our old proving system.</p>
<p>With the introduction of the new prover, CRS is only utilized in the final step, specifically during the snark_wrapper
phase. However, since the computational requirements in this stage are significantly reduced compared to the past, we
can rely on a smaller CRS file, namely the setup_2^24.key.</p>
<h2 id="advanced"><a class="header" href="#advanced">Advanced</a></h2>
<h3 id="whats-inside-the-key"><a class="header" href="#whats-inside-the-key">What’s inside the key</a></h3>
<h4 id="setup-key"><a class="header" href="#setup-key">Setup key</a></h4>
<p>Setup keys house the <a href="https://github.com/matter-labs/zksync-era/blob/d2ca29bf20b4ec2d9ec9e327b4ba6b281d9793de/prover/vk_setup_data_generator_server_fri/src/lib.rs#L61">ProverSetupData object</a>, which in turn contains the full Merkle tree. This is
part of the reason why setup keys can be quite large in size.</p>
<p>To put it in simpler terms, if we consider the circuits as a massive collection of linear equations, the setup key
essentially contains all the parameters for these equations. Every detail that defines and regulates these equations is
stored within the setup key, making it a comprehensive and crucial component in the proving process.</p>
<h4 id="verification-key"><a class="header" href="#verification-key">Verification key</a></h4>
<p>Verification keys are stored in a more accessible format, as JSON files, making it relatively easy to explore their
contents.</p>
<p>Inside, you’ll find numerous configuration fields related to the circuit. These include the size of the circuit, the
number of columns, the locations of constants, where to insert the public input, and the size of the public input, among
other details.</p>
<p>Additionally, at the end of the file, there’s a Merkle tree hash. In our case, there are actually 16 hashes because our
proving system utilizes a ‘Cap’ Merkle tree. Imagine a Merkle tree with 16 roots instead of just one; this design
ensures that each Merkle path is slightly shorter, improving efficiency.</p>
<h4 id="verification-key-hash"><a class="header" href="#verification-key-hash">Verification key hash</a></h4>
<p>As previously stated, the verification key hash is derived from hash function applied to the data contained in the
verification key. You can view the exact process of how the keccak hash is computed in the
<a href="https://github.com/matter-labs/era-contracts/blob/d85a73a1eeb5557343b7b44c6543aaf391d8b984/l1-contracts/contracts/zksync/Verifier.sol#L267">Verifier.sol</a> file.</p>
<p>For SNARK circuits (like snark_wrapper), we use keccak as hash function. For START based circuits, we use more circuit
friendly hash function (currently Poseidon2).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-debugging"><a class="header" href="#advanced-debugging">Advanced debugging</a></h1>
<h2 id="debugging-backend-in-vscode"><a class="header" href="#debugging-backend-in-vscode">Debugging backend in vscode</a></h2>
<p>Our backend takes configuration from environment variables, so before starting the debugging, we must make sure that
they are properly set.</p>
<p>You should create the following file in your <code>$workspaceFolder/.vscode/</code> called <code>prelaunch.py</code>:</p>
<pre><code class="language-python">import os
import lldb

# Read the .env file and store the key-value pairs in a array with format ["key=value"]
env_array = []
with open(os.path.join("etc/env/l2-inits/dev.init.env")) as f:
    for line in f:
        if line.strip() and line.strip()[0] != "#":
            env_array.append(line.strip())

with open(os.path.join("etc/env/targets/dev.env")) as f:
    for line in f:
        if line.strip() and line.strip()[0] != "#":
            env_array.append(line.strip())

target = lldb.debugger.GetSelectedTarget()

launch_info = target.GetLaunchInfo()
launch_info.SetEnvironmentEntries(env_array, True)
target.SetLaunchInfo(launch_info)
</code></pre>
<p>This file will load environment variables from <code>dev.init.env</code> and <code>dev.env</code> before starting the binary (notice that we
do this in a particular order, as values in dev.env should be overwriting the ones in dev.init.env).</p>
<p>Afterwards you need to add something like this to your launch.json:</p>
<pre><code> "configurations": [
        {
            "type": "lldb",
            "request": "launch",
            "name": "Debug executable 'zksync_server' DEV ENV",
            "cargo": {
                "args": [
                    "build",
                    "--bin=zksync_server",
                    "--package=zksync_core"
                ],
                "filter": {
                    "name": "zksync_server",
                    "kind": "bin"
                }
            },
            "args": [],
            "cwd": "${workspaceFolder}",
            "preRunCommands": [
                "command script import ${workspaceFolder}/.vscode/prelaunch.py"
            ]
        },
        ...
    ]
</code></pre>
<h2 id="debugging-contracts-in-vscode-using-hardhat"><a class="header" href="#debugging-contracts-in-vscode-using-hardhat">Debugging contracts in vscode (using hardhat)</a></h2>
<p>Assuming that you created project in hardhat, that you’d normally test with <code>hardhat test</code> - you also also test it with
vscode (which is super powerful - especially as you can have both binaries’ debug sessions running in VSCode at the same
time).</p>
<p>in package.json, make sure to have:</p>
<pre><code class="language-json">"scripts": {
        //...
        "test": "hardhat test",
        //...
    }
</code></pre>
<p>and then in VSCode’s launch.json:</p>
<pre><code class="language-json">{
  "version": "0.2.0",
  "configurations": [
    {
      "type": "node",
      "request": "launch",
      "name": "Launch Program",
      "console": "integratedTerminal",
      "runtimeExecutable": "yarn",
      "runtimeArgs": ["test", "${workspaceFolder}/test/try.js"]
    }
  ]
}
</code></pre>
<p>where <code>test/try.js</code> is your test code.</p>
<h2 id="performance-analysis-of-rust-binaries-flame-graphs"><a class="header" href="#performance-analysis-of-rust-binaries-flame-graphs">Performance analysis of rust binaries (flame graphs)</a></h2>
<p>If you’d like to analyze the CPU performance of your rust binary, you can use ‘perf’ to compute the flame graphs.</p>
<p>First - run the binary with perf enabled (this will make the binary a little bit slower):</p>
<pre><code>sudo perf record -F500 --call-graph=dwarf,65528  /path/to/binary --other --flags
</code></pre>
<p>(you can also connect to already running binary - by providing its PID with <code>-p</code> option)</p>
<p>When you’re done collecting records, you have to convert them into flame-graph friendly format, by running:</p>
<pre><code>sudo perf script -F +pid &gt; perfbench.script
</code></pre>
<p>This will create the perfbench.script file, that you can later upload to <a href="https://profiler.firefox.com/">https://profiler.firefox.com/</a> and see the
detailed flame graph.</p>
<h2 id="debuggingunderstandingtracing-zkevm-assembly"><a class="header" href="#debuggingunderstandingtracing-zkevm-assembly">Debugging/understanding/tracing zkEVM assembly</a></h2>
<p>Currently this is quite a complex process, but we’re working on making it a little bit smoother.</p>
<p>You start by installing the ‘compiler-tester’ repo (see its README.md instructions for details) - it is quite heavy as
it needs the LLVM etc etc.</p>
<p>Afterwards, you can look at one of the tests (for example
<a href="https://github.com/matter-labs/era-compiler-tests/blob/main/solidity/simple/default.sol">tests/solidity/simple/default.sol</a>).</p>
<pre><code class="language-solidity">//! { "cases": [ {
//!     "name": "first",
//!     "inputs": [
//!         {
//!             "method": "first",
//!             "calldata": [
//!             ]
//!         }
//!     ],
//!     "expected": [
//!         "42"
//!     ]
//! }, ] }

// SPDX-License-Identifier: MIT

pragma solidity &gt;=0.4.16;

contract Test {
  function first() public pure returns (uint64) {
    uint64 result = 42;
    return result;
  }
}

</code></pre>
<p>As you can see - it is self-contained - it has the solidity code at the bottom, and the top comments are used to define
the test case - and expected result.</p>
<p>You can run it by calling:</p>
<pre><code class="language-shell">cargo run --release --bin compiler-tester -- -DT \
        --path='tests/solidity/simple/default.sol' \
        --mode='Y+M3B3 0.8.19'
</code></pre>
<p>And then collect the detailed tracing information from trace directory. You’ll notice that you have 2 files for each
test - one covering the deployment, and one covering the test run.</p>
<p>You can take test run one and upload it to <a href="https://explorer.zksync.io/tools/debugger">our debugger</a> to see detailed
zkAssembler and state of memory, heap, stack and registers at each execution step.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="docker-and-ci"><a class="header" href="#docker-and-ci">Docker and CI</a></h1>
<p>How to efficiently debug CI issues locally.</p>
<p>This document will be useful in case you struggle with reproducing some CI issues on your local machine.</p>
<p>In most cases, this is due to the fact that your local machine has some arifacts, configs, files that you might have set
in the past, that are missing from the CI.</p>
<h2 id="basic-docker-commands"><a class="header" href="#basic-docker-commands">Basic docker commands</a></h2>
<ul>
<li><code>docker ps</code> - prints the list of currently running containers</li>
<li><code>docker run</code> - starts a new docker container</li>
<li><code>docker exec</code> - connects to a running container and executes the command.</li>
<li><code>docker kill</code> - stops the container.</li>
<li><code>docker cp</code> - allows copying files between your system and docker container.</li>
</ul>
<p>Usually docker containers have a specific binary that they run, but for debugging we often want to start a bash instead.</p>
<p>The command below starts a new docker containers, and instead of running its binary - runs <code>/bin/bash</code> in interactive
mode.</p>
<pre><code>docker run  -it matterlabs/zk-environment:latest2.0-lightweight-nightly /bin/bash
</code></pre>
<p>Connects to <strong>already running</strong> job, and gets you the interactive shell.</p>
<pre><code>docker exec -i -it local-setup-zksync-1 /bin/bash
</code></pre>
<h2 id="debugging-ci"><a class="header" href="#debugging-ci">Debugging CI</a></h2>
<p>Many of the tests require postgres &amp; reth - you initialize them with:</p>
<pre><code>docker compose up -d

</code></pre>
<p>You should see something like this:</p>
<pre><code>[+] Running 3/3
 ⠿ Network zksync-era_default       Created   0.0s
 ⠿ Container zksync-era-postgres-1  Started   0.3s
 ⠿ Container zksync-era-reth-1      Started   0.3s
</code></pre>
<p>Start the docker with the ‘basic’ imge</p>
<pre><code># We tell it to connect to the same 'subnetwork' as other containers (zksync-era_default).
# the IN_DOCKER variable is changing different urls (like postgres) from localhost to postgres - so that it can connect to those
# containers above.
docker run --network zksync-era_default -e IN_DOCKER=1   -it matterlabs/zk-environment:latest2.0-lightweight-nightly /bin/bash
# and then inside, run:

git clone https://github.com/matter-labs/zksync-era.git .
git checkout YOUR_BRANCH
zk
</code></pre>
<p>After this, you can run any commands you need.</p>
<p>When you see a command like <code>ci_run zkstack dev contracts</code> in the CI - this simply means that it executed
<code>zkstack dev contracts</code> inside that docker container.</p>
<p><strong>IMPORTANT</strong> - by default, docker is running in the mode, where it does NOT persist the changes. So if you exit that
shell, all the changes will be removed (so when you restart, you’ll end up in the same pristine condition). You can
‘commit’ your changes into a new docker image, using <code>docker commit XXX some_name</code>, where XXX is your container id from
<code>docker ps</code>. Afterwards you can ‘start’ this docker image with <code>docker run ... some_name</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-node-documentation"><a class="header" href="#zksync-node-documentation">ZkSync Node Documentation</a></h1>
<p>This documentation explains the basics of the ZKsync Node.</p>
<h2 id="disclaimers"><a class="header" href="#disclaimers">Disclaimers</a></h2>
<ul>
<li>The ZKsync node is in the alpha phase, and should be used with caution.</li>
<li>The ZKsync node is a read-only replica of the main node.</li>
</ul>
<h2 id="what-is-the-zksync-node"><a class="header" href="#what-is-the-zksync-node">What is the ZKsync node</a></h2>
<p>The ZKsync node is a read-replica of the main (centralized) node that can be run by external parties. It functions by
receiving blocks from the ZKsync network and re-applying transactions locally, starting from the genesis block. The
ZKsync node shares most of its codebase with the main node. Consequently, when it re-applies transactions, it does so
exactly as the main node did in the past.</p>
<p><strong>It has two modes of initialization:</strong></p>
<ul>
<li>recovery from a DB dump, in Ethereum terms this corresponds to archival node</li>
<li>recovery from a snapshot, in Ethereum terms this corresponds to light node, such nodes will only have access to
transactions data from after the node was initialized. The database can be pruned on such nodes.</li>
</ul>
<h2 id="high-level-overview-1"><a class="header" href="#high-level-overview-1">High-level overview</a></h2>
<p>At a high level, the ZKsync node can be seen as an application that has the following modules:</p>
<ul>
<li>API server that provides the publicly available Web3 interface.</li>
<li>Consensus layer that interacts with the peer network and retrieves transactions and blocks to re-execute.</li>
<li>Sequencer component that actually executes and persists transactions received from the synchronization layer.</li>
<li>Several checker modules that ensure the consistency of the ZKsync node state.</li>
</ul>
<p>With the EN, you are able to:</p>
<ul>
<li>Locally recreate and verify the ZKsync Era mainnet/testnet state.</li>
<li>Interact with the recreated state in a trustless way (in a sense that the validity is locally verified, and you should
not rely on a third-party API ZKsync Era provides).</li>
<li>Use the Web3 API without having to query the main node.</li>
<li>Send L2 transactions (that will be proxied to the main node).</li>
</ul>
<p>With the EN, you <em>can not</em>:</p>
<ul>
<li>Create L2 blocks or L1 batches on your own.</li>
<li>Generate proofs.</li>
<li>Submit data to L1.</li>
</ul>
<p>A more detailed overview of the EN’s components is provided in the <a href="guides/external-node/06_components.html">components</a> section.</p>
<h2 id="api-overview"><a class="header" href="#api-overview">API overview</a></h2>
<p>API exposed by the ZKsync node strives to be Web3-compliant. If some method is exposed but behaves differently compared
to Ethereum, it should be considered a bug. Please <a href="https://zksync.io/contact">report</a> such cases.</p>
<h3 id="eth-namespace"><a class="header" href="#eth-namespace"><code>eth</code> namespace</a></h3>
<p>Data getters in this namespace operate in the L2 space: require/return L2 block numbers, check balances in L2, etc.</p>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>eth_blockNumber</code></td><td></td></tr>
<tr><td><code>eth_chainId</code></td><td></td></tr>
<tr><td><code>eth_call</code></td><td></td></tr>
<tr><td><code>eth_estimateGas</code></td><td></td></tr>
<tr><td><code>eth_gasPrice</code></td><td></td></tr>
<tr><td><code>eth_newFilter</code></td><td>Maximum amount of installed filters is configurable</td></tr>
<tr><td><code>eth_newBlockFilter</code></td><td>Same as above</td></tr>
<tr><td><code>eth_newPendingTransactionsFilter</code></td><td>Same as above</td></tr>
<tr><td><code>eth_uninstallFilter</code></td><td></td></tr>
<tr><td><code>eth_getLogs</code></td><td>Maximum amount of returned entities can be configured</td></tr>
<tr><td><code>eth_getFilterLogs</code></td><td>Same as above</td></tr>
<tr><td><code>eth_getFilterChanges</code></td><td>Same as above</td></tr>
<tr><td><code>eth_getBalance</code></td><td></td></tr>
<tr><td><code>eth_getBlockByNumber</code></td><td></td></tr>
<tr><td><code>eth_getBlockByHash</code></td><td></td></tr>
<tr><td><code>eth_getBlockTransactionCountByNumber</code></td><td></td></tr>
<tr><td><code>eth_getBlockTransactionCountByHash</code></td><td></td></tr>
<tr><td><code>eth_getCode</code></td><td></td></tr>
<tr><td><code>eth_getStorageAt</code></td><td></td></tr>
<tr><td><code>eth_getTransactionCount</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByHash</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByBlockHashAndIndex</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByBlockNumberAndIndex</code></td><td></td></tr>
<tr><td><code>eth_getTransactionReceipt</code></td><td></td></tr>
<tr><td><code>eth_protocolVersion</code></td><td></td></tr>
<tr><td><code>eth_sendRawTransaction</code></td><td></td></tr>
<tr><td><code>eth_syncing</code></td><td>ZKsync node is considered synced if it’s less than 11 blocks behind the main node.</td></tr>
<tr><td><code>eth_coinbase</code></td><td>Always returns a zero address</td></tr>
<tr><td><code>eth_accounts</code></td><td>Always returns an empty list</td></tr>
<tr><td><code>eth_getCompilers</code></td><td>Always returns an empty list</td></tr>
<tr><td><code>eth_hashrate</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_getUncleCountByBlockHash</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_getUncleCountByBlockNumber</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_mining</code></td><td>Always returns false</td></tr>
</tbody></table>
</div>
<h3 id="pubsub"><a class="header" href="#pubsub">PubSub</a></h3>
<p>Only available on the WebSocket servers.</p>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>eth_subscribe</code></td><td>Maximum amount of subscriptions is configurable</td></tr>
<tr><td><code>eth_subscription</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="net-namespace"><a class="header" href="#net-namespace"><code>net</code> namespace</a></h3>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>net_version</code></td><td></td></tr>
<tr><td><code>net_peer_count</code></td><td>Always returns 0</td></tr>
<tr><td><code>net_listening</code></td><td>Always returns false</td></tr>
</tbody></table>
</div>
<h3 id="web3-namespace"><a class="header" href="#web3-namespace"><code>web3</code> namespace</a></h3>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>web3_clientVersion</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="debug-namespace"><a class="header" href="#debug-namespace"><code>debug</code> namespace</a></h3>
<p>The <code>debug</code> namespace gives access to several non-standard RPC methods, which will allow developers to inspect and debug
calls and transactions.</p>
<p>This namespace is disabled by default and can be configured via setting <code>EN_API_NAMESPACES</code> as described in the
<a href="guides/external-node/prepared_configs/mainnet-config.env">example config</a>.</p>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>debug_traceBlockByNumber</code></td><td></td></tr>
<tr><td><code>debug_traceBlockByHash</code></td><td></td></tr>
<tr><td><code>debug_traceCall</code></td><td></td></tr>
<tr><td><code>debug_traceTransaction</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="zks-namespace"><a class="header" href="#zks-namespace"><code>zks</code> namespace</a></h3>
<p>This namespace contains rollup-specific extensions to the Web3 API. Note that <em>only methods</em> specified in the
<a href="https://docs.zksync.io/build/api-reference/zks-rpc">documentation</a> are considered public. There may be other methods exposed in this namespace, but undocumented
methods come without any kind of stability guarantees and can be changed or removed without notice.</p>
<p>Always refer to the documentation linked above to see the list of stabilized methods in this namespace.</p>
<h3 id="en-namespace"><a class="header" href="#en-namespace"><code>en</code> namespace</a></h3>
<p>This namespace contains methods that ZKsync nodes call on the main node while syncing. If this namespace is enabled,
other ENs can sync from this node.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<h2 id="prerequisites-3"><a class="header" href="#prerequisites-3">Prerequisites</a></h2>
<p>Install <code>docker compose</code> and <code>Docker</code></p>
<h2 id="running-zksync-node-locally"><a class="header" href="#running-zksync-node-locally">Running ZKsync node locally</a></h2>
<p>These commands start ZKsync node locally inside docker.</p>
<p>To start a mainnet instance, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file mainnet-external-node-docker-compose.yml up
</code></pre>
<p>To reset its state, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file mainnet-external-node-docker-compose.yml down --volumes
</code></pre>
<p>To start a testnet instance, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file testnet-external-node-docker-compose.yml up
</code></pre>
<p>To reset its state, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file testnet-external-node-docker-compose.yml down --volumes
</code></pre>
<h3 id="observability"><a class="header" href="#observability">Observability</a></h3>
<p>You can see the status of the node (after recovery) in <a href="http://localhost:3000/dashboards">local grafana dashboard</a>. You
can also access a debug page with more information about the node <a href="http://localhost:5000">here</a>.</p>
<p>The HTTP JSON-RPC API can be accessed on port <code>3060</code> and WebSocket API can be accessed on port <code>3061</code>.</p>
<blockquote>
<p>[!NOTE]</p>
<p>The node will recover from a snapshot on it’s first run, this may take up to 10h. Before the recovery is finished, the
API server won’t serve any requests.</p>
<p>If you need access to historical transaction data, please use recovery from DB dumps (see Advanced setup section)</p>
</blockquote>
<h3 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h3>
<blockquote>
<p>[!NOTE]</p>
<p>Those are requirements for nodes that use snapshots recovery and history pruning (the default for docker-compose
setup).</p>
<p>For requirements for nodes running from DB dump see the <a href="guides/external-node/03_running.html">running</a> section. DB dumps are a way to start
ZKsync node with full historical transactions history.</p>
<p>For nodes with pruning disabled, expect the storage requirements on mainnet to grow at 1TB per month. If you want to
stop historical DB pruning you can read more about this in the <a href="guides/external-node/08_pruning.html">pruning</a> section.</p>
</blockquote>
<ul>
<li>32 GB of RAM and a relatively modern CPU</li>
<li>50 GB of storage for testnet nodes</li>
<li>500 GB of storage for mainnet nodes</li>
<li>100 Mbps connection (1 Gbps+ recommended)</li>
</ul>
<h2 id="advanced-setup"><a class="header" href="#advanced-setup">Advanced setup</a></h2>
<p>If you need monitoring, backups, to recover from DB dump or a more customized PostgreSQL settings, etc, please see:
<a href="https://github.com/matter-labs/ansible-en-role">ansible-en-role repo</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-node-configuration"><a class="header" href="#zksync-node-configuration">ZkSync Node Configuration</a></h1>
<p>This document outlines various configuration options for the EN. Currently, the ZKsync node requires the definition of
numerous environment variables. To streamline this process, we provide prepared configs for the ZKsync Era - for both
<a href="guides/external-node/prepared_configs/mainnet-config.env">mainnet</a> and <a href="guides/external-node/prepared_configs/testnet-sepolia-config.env">testnet</a>. You can use
these files as a starting point and modify only the necessary sections.</p>
<p><strong>You can also see directory docker-compose-examples if you want to run external-node on your machine with recommended
default settings.</strong></p>
<h2 id="database"><a class="header" href="#database">Database</a></h2>
<p>The ZKsync node uses two databases: PostgreSQL and RocksDB.</p>
<p>PostgreSQL serves as the main source of truth in the EN, so all the API requests fetch the state from there. The
PostgreSQL connection is configured by the <code>DATABASE_URL</code>. Additionally, the <code>DATABASE_POOL_SIZE</code> variable defines the
size of the connection pool.</p>
<p>RocksDB is used in components where IO is a bottleneck, such as the State Keeper and the Merkle tree. If possible, it is
recommended to use an NVME SSD for RocksDB. RocksDB requires two variables to be set: <code>EN_STATE_CACHE_PATH</code> and
<code>EN_MERKLE_TREE_PATH</code>, which must point to different directories.</p>
<h2 id="l1-web3-client"><a class="header" href="#l1-web3-client">L1 Web3 client</a></h2>
<p>ZKsync node requires a connection to an Ethereum node. The corresponding env variable is <code>EN_ETH_CLIENT_URL</code>. Make sure
to set the URL corresponding to the correct L1 network (L1 mainnet for L2 mainnet and L1 sepolia for L2 testnet).</p>
<p>Note: Currently, the ZKsync node makes 2 requests to the L1 per L1 batch, so the Web3 client usage for a synced node
should not be high. However, during the synchronization phase the new batches would be persisted on the ZKsync node
quickly, so make sure that the L1 client won’t exceed any limits (e.g. in case you use Infura).</p>
<h2 id="exposed-ports"><a class="header" href="#exposed-ports">Exposed ports</a></h2>
<p>The dockerized version of the server exposes the following ports:</p>
<ul>
<li>HTTP JSON-RPC: 3060</li>
<li>WebSocket JSON-RPC: 3061</li>
<li>Prometheus listener: 3322</li>
<li>Healthcheck server: 3081</li>
</ul>
<p>While the configuration variables for them exist, you are not expected to change them unless you want to use the EN
outside of provided docker environment (not supported at the time of writing).</p>
<p><strong>NOTE</strong>: if the Prometheus port is configured, it must be <a href="https://prometheus.io/docs/introduction/overview/">scraped</a>
periodically to avoid a memory leak due to a
<a href="https://github.com/metrics-rs/metrics/issues/245">bug in an external metrics library</a>. If you are not intending to use
the metrics, leave this port not configured, and the metrics won’t be collected.</p>
<h2 id="api-limits"><a class="header" href="#api-limits">API limits</a></h2>
<p>There are variables that allow you to fine-tune the limits of the RPC servers, such as limits on the number of returned
entries or the limit for the accepted transaction size. Provided files contain sane defaults that are recommended for
use, but these can be edited, e.g. to make the ZKsync node more/less restrictive.</p>
<h2 id="json-rpc-api-namespaces"><a class="header" href="#json-rpc-api-namespaces">JSON-RPC API namespaces</a></h2>
<p>There are 7 total supported API namespaces: <code>eth</code>, <code>net</code>, <code>web3</code>, <code>debug</code> - standard ones; <code>zks</code> - rollup-specific one;
<code>pubsub</code> - a.k.a. <code>eth_subscribe</code>; <code>en</code> - used by ZKsync nodes while syncing. You can configure what namespaces you want
to enable using <code>EN_API_NAMESPACES</code> and specifying namespace names in a comma-separated list. By default, all but the
<code>debug</code> namespace are enabled.</p>
<h2 id="logging-and-observability"><a class="header" href="#logging-and-observability">Logging and observability</a></h2>
<p><code>MISC_LOG_FORMAT</code> defines the format in which logs are shown: <code>plain</code> corresponds to the human-readable format, while
the other option is <code>json</code> (recommended for deployments).</p>
<p><code>RUST_LOG</code> variable allows you to set up the logs granularity (e.g. make the ZKsync node emit fewer logs). You can read
about the format <a href="https://docs.rs/env_logger/0.10.0/env_logger/#enabling-logging">here</a>.</p>
<p><code>MISC_SENTRY_URL</code> and <code>MISC_OTLP_URL</code> variables can be configured to set up Sentry and OpenTelemetry exporters.</p>
<p>If Sentry is configured, you also have to set <code>EN_SENTRY_ENVIRONMENT</code> variable to configure the environment in events
reported to sentry.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-zksync-node"><a class="header" href="#running-the-zksync-node">Running the ZkSync Node</a></h1>
<blockquote>
<p>[!NOTE]</p>
<p>If you want to just run node with recommended default setting, please see the <a href="guides/external-node/00_quick_start.html">quick start</a> page.</p>
</blockquote>
<p>This section assumes that you have prepared a configuration file as described on the
<a href="guides/external-node/02_configuration.html">previous page</a>.</p>
<h2 id="system-requirements-for-nodes-started-from-db-dumps"><a class="header" href="#system-requirements-for-nodes-started-from-db-dumps">System Requirements for nodes started from DB dumps</a></h2>
<p>This configuration is approximate and should be considered as <strong>minimal</strong> requirements.</p>
<ul>
<li>32-core CPU</li>
<li>64GB RAM</li>
<li>SSD storage (NVME recommended):
<ul>
<li>Sepolia Testnet - 10GB ZKsync node + 50GB PostgreSQL (at the time of writing, will grow over time, so should be
constantly monitored)</li>
<li>Mainnet - 3TB ZKsync node + 8TB PostgreSQL (at the time of writing, will grow over time, so should be constantly
monitored)</li>
</ul>
</li>
<li>100 Mbps connection (1 Gbps+ recommended)</li>
</ul>
<h2 id="a-note-about-postgresql-storage"><a class="header" href="#a-note-about-postgresql-storage">A note about PostgreSQL storage</a></h2>
<p>By far, the heaviest table to maintain is the <code>call_traces</code> table. This table is only required for the <code>debug</code>
namespace. If you want to clear some space and aren’t using the <code>debug</code> namespace, you can</p>
<ul>
<li>clear it with a simple query <code>DELETE FROM call_traces;</code></li>
<li>leave the <code>debug</code> namespace disabled via the <code>EN_API_NAMESPACES</code> env var as described in the
<a href="guides/external-node/prepared_configs/mainnet-config.env">example config</a>.</li>
</ul>
<h2 id="infrastructure"><a class="header" href="#infrastructure">Infrastructure</a></h2>
<p>You need to set up a PostgreSQL server, however it is out of the scope of these docs, but the popular choice is to run
it in Docker. There are many of guides on that,
<a href="https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/">here’s one example</a>.</p>
<p>Note however that if you run PostgresSQL as a stand-alone Docker image (e.g. not in Docker-compose with a network shared
between ZKsync node and Postgres), ZKsync node won’t be able to access Postgres via <code>localhost</code> or <code>127.0.0.1</code> URLs. To
make it work, you’ll have to either run it with a <code>--network host</code> (on Linux) or use <code>host.docker.internal</code> instead of
<code>localhost</code> in the ZKsync node configuration ([official docs][host_docker_internal]).</p>
<p>Besides running Postgres, you are expected to have a DB dump from a corresponding env. You can restore it using
<code>pg_restore -O -C &lt;DUMP_PATH&gt; --dbname=&lt;DB_URL&gt;</code>.</p>
<p>You can also refer to
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/external-node/00_quick_start.md#advanced-setup">ZKsync Node configuration management blueprint</a>
for advanced DB instance configurations.</p>
<p><a href="https://docs.docker.com/desktop/networking/#i-want-to-connect-from-a-container-to-a-service-on-the-host">host_docker_internal</a></p>
<h2 id="running"><a class="header" href="#running">Running</a></h2>
<p>Assuming you have the ZKsync node Docker image, an env file with the prepared configuration, and you have restored your
DB with the pg dump, that is all you need.</p>
<p>Sample running command:</p>
<pre><code class="language-sh">docker run --env-file &lt;path_to_env_file&gt; --mount type=bind,source=&lt;local_rocksdb_data_path&gt;,target=&lt;configured_rocksdb_data_path&gt; &lt;image&gt;
</code></pre>
<p>Helm charts and other infrastructure configuration options, if required, would be available later.</p>
<h2 id="first-start"><a class="header" href="#first-start">First start</a></h2>
<p>When you start the node for the first time, the state in PostgreSQL corresponds to the dump you have used, but the state
in RocksDB (mainly the Merkle tree) is absent. Before the node can make any progress, it has to rebuild the state in
RocksDB and verify consistency. The exact time required for that depends on the hardware configuration, but it is
reasonable to expect the state rebuild on the mainnet to take more than 20 hours.</p>
<h2 id="redeploying-the-zksync-node-with-a-new-pg-dump"><a class="header" href="#redeploying-the-zksync-node-with-a-new-pg-dump">Redeploying the ZKsync node with a new PG dump</a></h2>
<p>If you’ve been running the ZKsync node for some time and are going to redeploy it using a new PG dump, you should</p>
<ul>
<li>Stop the EN</li>
<li>Remove SK cache (corresponding to <code>EN_STATE_CACHE_PATH</code>)</li>
<li>Remove your current DB</li>
<li>Restore with the new dump</li>
<li>Start the EN</li>
</ul>
<p>Monitoring the node behavior and analyzing the state it’s in is covered in the
<a href="guides/external-node/04_observability.html">observability section</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-node-observability"><a class="header" href="#zksync-node-observability">ZKsync node Observability</a></h1>
<p>The ZKsync node provides several options for setting up observability. Configuring logs and sentry is described in the
<a href="guides/external-node/02_configuration.html">configuration</a> section, so this section focuses on the exposed metrics.</p>
<p>This section is written with the assumption that you’re familiar with
<a href="https://prometheus.io/docs/introduction/overview/">Prometheus</a> and <a href="https://grafana.com/docs/">Grafana</a>.</p>
<h2 id="buckets"><a class="header" href="#buckets">Buckets</a></h2>
<p>By default, latency histograms are distributed in the following buckets (in seconds):</p>
<pre><code>[0.001, 0.005, 0.025, 0.1, 0.25, 1.0, 5.0, 30.0, 120.0]
</code></pre>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>ZKsync node exposes a lot of metrics, a significant amount of which aren’t interesting outside the development flow.
This section’s purpose is to highlight metrics that may be worth observing in the external setup.</p>
<p>If you are not planning to scrape Prometheus metrics, please unset <code>EN_PROMETHEUS_PORT</code> environment variable to prevent
memory leaking.</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>external_node_synced</code></td><td>Gauge</td><td>-</td><td>1 if synced, 0 otherwise. Matches <code>eth_call</code> behavior</td></tr>
<tr><td><code>external_node_sync_lag</code></td><td>Gauge</td><td>-</td><td>How many blocks behind the main node the ZKsync node is</td></tr>
<tr><td><code>external_node_fetcher_requests</code></td><td>Histogram</td><td><code>stage</code>, <code>actor</code></td><td>Duration of requests performed by the different fetcher components</td></tr>
<tr><td><code>external_node_fetcher_cache_requests</code></td><td>Histogram</td><td>-</td><td>Duration of requests performed by the fetcher cache layer</td></tr>
<tr><td><code>external_node_fetcher_miniblock</code></td><td>Gauge</td><td><code>status</code></td><td>The number of the last L2 block update fetched from the main node</td></tr>
<tr><td><code>external_node_fetcher_l1_batch</code></td><td>Gauge</td><td><code>status</code></td><td>The number of the last batch update fetched from the main node</td></tr>
<tr><td><code>external_node_action_queue_action_queue_size</code></td><td>Gauge</td><td>-</td><td>Amount of fetched items waiting to be processed</td></tr>
<tr><td><code>server_miniblock_number</code></td><td>Gauge</td><td><code>stage</code>=<code>sealed</code></td><td>Last locally applied L2 block number</td></tr>
<tr><td><code>server_block_number</code></td><td>Gauge</td><td><code>stage</code>=<code>sealed</code></td><td>Last locally applied L1 batch number</td></tr>
<tr><td><code>server_block_number</code></td><td>Gauge</td><td><code>stage</code>=<code>tree_lightweight_mode</code></td><td>Last L1 batch number processed by the tree</td></tr>
<tr><td><code>server_processed_txs</code></td><td>Counter</td><td><code>stage</code>=<code>mempool_added, state_keeper</code></td><td>Can be used to show incoming and processing TPS values</td></tr>
<tr><td><code>api_web3_call</code></td><td>Histogram</td><td><code>method</code></td><td>Duration of Web3 API calls</td></tr>
<tr><td><code>sql_connection_acquire</code></td><td>Histogram</td><td>-</td><td>Time to get an SQL connection from the connection pool</td></tr>
</tbody></table>
</div>
<p>Metrics can be used to detect anomalies in configuration, which is described in more detail in the
<a href="guides/external-node/05_troubleshooting.html">next section</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-node-troubleshooting"><a class="header" href="#zksync-node-troubleshooting">ZKsync node Troubleshooting</a></h1>
<p>The ZKsync node tries to follow the fail-fast principle: if an anomaly is discovered, instead of attempting state
recovery, in most cases it will restart. Most of the time it will manifest as crashes, and if it happens once, it
shouldn’t be treated as a problem.</p>
<p>However, if the node enters the crash loop or otherwise behaves unexpectedly, it may indicate either a bug in the
implementation or a problem with configuration. This section tries to cover common problems.</p>
<h2 id="panics"><a class="header" href="#panics">Panics</a></h2>
<p>Panics is the Rust programming language notion of irrecoverable errors, and normally if panic happens, the application
will immediately crash.</p>
<ul>
<li>Panic matching <code>called Result::unwrap() on an Err value: Database(PgDatabaseError</code>: problem communicating with the
PostgreSQL, most likely some of the connections have died.</li>
<li>Panic matching <code>failed to init rocksdb: Error { message: "IO error: No space left on device</code>: more space on SSD is
required.</li>
<li>Anything that mentions “Poison Error”: a “secondary” panic that may occur if one of the components panicked first. If
you see this panic, look for a panic that happened shortly before it to find the real cause.</li>
</ul>
<p>Other kinds of panic aren’t normally expected. While in most cases, the state will be recovered after a restart, please
<a href="https://zksync.io/contact">report</a> such cases to Matter Labs regardless.</p>
<h2 id="genesis-issues"><a class="header" href="#genesis-issues">Genesis Issues</a></h2>
<p>The ZKsync node is supposed to start with an applied DB dump. If you see any genesis-related errors, it probably means
the ZKsync node was started without an applied dump.</p>
<h2 id="logs"><a class="header" href="#logs">Logs</a></h2>
<p><em>Note: logs with the <code>error</code> level are reported to Sentry if it’s configured. If you notice unneeded alerts there that
you don’t consider actionable, you may disable logs for a component by tweaking the configuration.</em></p>
<div class="table-wrapper"><table><thead><tr><th>Level</th><th>Log substring</th><th>Interpretation</th></tr></thead><tbody>
<tr><td>ERROR</td><td>“One of the tokio actors unexpectedly finished”</td><td>One of the components crashed, and the node is restarting.</td></tr>
<tr><td>WARN</td><td>“Stop signal received, <component> is shutting down”</td><td>Satellite log of the message above</td></tr>
<tr><td>ERROR</td><td>“A lot of requests to the remote API failed in a row”</td><td>The remote API used to update token lists is probably down. Logs should disappear once API is available.</td></tr>
<tr><td>WARN</td><td>“Server returned an error status code: 429”</td><td>The main API rate limits are too strict. <a href="https://zksync.io/contact">Contact</a> Matter Labs to discuss the situation.</td></tr>
<tr><td>WARN</td><td>“Following transport error occurred”</td><td>There was a problem with fetching data from the main node.</td></tr>
<tr><td>WARN</td><td>“Unable to get the gas price”</td><td>There was a problem with fetching data from the main node.</td></tr>
<tr><td>WARN</td><td>“Consistency checker error”</td><td>There are problems querying L1, check the Web3 URL you specified in the config.</td></tr>
<tr><td>WARN</td><td>“Reorg detected”</td><td>Reorg was detected on the main node, the ZKsync node will rollback and restart</td></tr>
</tbody></table>
</div>
<p>Same as with panics, normally it’s only a problem if a WARN+ level log appears many times in a row.</p>
<h2 id="metrics-anomalies"><a class="header" href="#metrics-anomalies">Metrics anomalies</a></h2>
<p>The following common anomalies can be discovered by observing metrics <em>after the tree is rebuilt to match the DB
snapshot</em>:</p>
<ul>
<li><code>external_node_sync_lag</code> doesn’t decrease and <code>external_node_action_queue_action_queue_size</code> is near 0. Cause: The
fetcher can’t fetch new blocks quickly enough. Most likely, the network connection is too slow.</li>
<li><code>external_node_sync_lag</code> doesn’t decrease and <code>external_node_action_queue_action_queue_size</code> is at some high level.
Cause: The State Keeper doesn’t process fetched data quickly enough. Most likely, a more powerful CPU is needed.</li>
<li><code>sql_connection_acquire</code> skyrockets. Probably, there are not enough connections in the pool to match the demand.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-node-components"><a class="header" href="#zksync-node-components">ZKsync node components</a></h1>
<p>This section contains an overview of the EN’s main components.</p>
<h2 id="api"><a class="header" href="#api">API</a></h2>
<p>The ZKsync node can serve both the HTTP and the WS Web3 API, as well as PubSub. Whenever possible, it provides data
based on the local state, with a few exceptions:</p>
<ul>
<li>Submitting transactions: Since it is a read replica, submitted transactions are proxied to the main node, and the
response is returned from the main node.</li>
<li>Querying transactions: The ZKsync node is not aware of the main node’s mempool, and it does not sync rejected
transactions. Therefore, if a local lookup for a transaction or its receipt fails, the ZKsync node will attempt the
same query on the main node.</li>
</ul>
<p>Apart from these cases, the API does not depend on the main node. Even if the main node is temporarily unavailable, the
ZKsync node can continue to serve the state it has locally.</p>
<h2 id="fetcher"><a class="header" href="#fetcher">Fetcher</a></h2>
<p>The Fetcher component is responsible for maintaining synchronization between the ZKsync node and the main node. Its
primary task is to fetch new blocks in order to update the local chain state. However, its responsibilities extend
beyond that. For instance, the Fetcher is also responsible for keeping track of L1 batch statuses. This involves
monitoring whether locally applied batches have been committed, proven, or executed on L1.</p>
<p>It is worth noting that in addition to fetching the <em>state</em>, the ZKsync node also retrieves the L1 gas price from the
main node for the purpose of estimating fees for L2 transactions (since this also happens based on the local state).
This information is necessary to ensure that gas estimations are performed in the exact same manner as the main node,
thereby reducing the chances of a transaction not being included in a block.</p>
<h2 id="state-keeper--vm"><a class="header" href="#state-keeper--vm">State Keeper / VM</a></h2>
<p>The State Keeper component serves as the “sequencer” part of the node. It shares most of its functionality with the main
node, with one key distinction. The main node retrieves transactions from the mempool and has the authority to decide
when a specific L2 block or L1 batch should be sealed. On the other hand, the ZKsync node retrieves transactions from
the queue populated by the Fetcher and seals the corresponding blocks/batches based on the data obtained from the
Fetcher queue.</p>
<p>The actual execution of batches takes place within the VM, which is identical in both the Main and ZKsync nodes.</p>
<h2 id="reorg-detector"><a class="header" href="#reorg-detector">Reorg Detector</a></h2>
<p>In ZKsync Era, it is theoretically possible for L1 batches to be reverted before the corresponding “execute” operation
is applied on L1, that is before the block is <a href="https://docs.zksync.io/zk-stack/concepts/finality">final</a>. Such situations are highly uncommon and typically occur
due to significant issues: e.g. a bug in the sequencer implementation preventing L1 batch commitment. Prior to batch
finality, the ZKsync operator can perform a rollback, reverting one or more batches and restoring the blockchain state
to a previous point. Finalized batches cannot be reverted at all.</p>
<p>However, even though such situations are rare, the ZKsync node must handle them correctly.</p>
<p>To address this, the ZKsync node incorporates a Reorg Detector component. This module keeps track of all L1 batches that
have not yet been finalized. It compares the locally obtained state root hashes with those provided by the main node’s
API. If the root hashes for the latest available L1 batch do not match, the Reorg Detector searches for the specific L1
batch responsible for the divergence. Subsequently, it rolls back the local state and restarts the node. Upon restart,
the EN resumes normal operation.</p>
<h2 id="consistency-checker"><a class="header" href="#consistency-checker">Consistency Checker</a></h2>
<p>The main node API serves as the primary source of information for the EN. However, relying solely on the API may not
provide sufficient security since the API data could potentially be incorrect due to various reasons. The primary source
of truth for the rollup system is the L1 smart contract. Therefore, to enhance the security of the EN, each L1 batch
undergoes cross-checking against the L1 smart contract by a component called the Consistency Checker.</p>
<p>When the Consistency Checker detects that a particular batch has been sent to L1, it recalculates a portion of the input
known as the “block commitment” for the L1 transaction. The block commitment contains crucial data such as the state
root and batch number, and is the same commitment that is used for generating a proof for the batch. The Consistency
Checker then compares the locally obtained commitment with the actual commitment sent to L1. If the data does not match,
it indicates a potential bug in either the main node or ZKsync node implementation or that the main node API has
provided incorrect data. In either case, the state of the ZKsync node cannot be trusted, and the ZKsync node enters a
crash loop until the issue is resolved.</p>
<h2 id="health-check-server"><a class="header" href="#health-check-server">Health check server</a></h2>
<p>The ZKsync node also exposes an additional server that returns HTTP 200 response when the ZKsync node is operating
normally, and HTTP 503 response when some of the health checks don’t pass (e.g. when the ZKsync node is not fully
initialized yet). This server can be used, for example, to implement the readiness probe in an orchestration solution
you use.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="snapshots-recovery"><a class="header" href="#snapshots-recovery">Snapshots Recovery</a></h1>
<p>Instead of initializing a node using a Postgres dump, it’s possible to configure a node to recover from a protocol-level
snapshot. This process is much faster and requires much less storage. Postgres database of a mainnet node recovered from
a snapshot is less than 500GB. Note that without <a href="guides/external-node/08_pruning.html">pruning</a> enabled, the node state will continuously grow
at a rate about 15GB per day.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<p>A snapshot is effectively a point-in-time snapshot of the VM state at the end of a certain L1 batch. Snapshots are
created for the latest L1 batches periodically (roughly twice a day) and are stored in a public GCS bucket.</p>
<p>Recovery from a snapshot consists of several parts.</p>
<ul>
<li><strong>Postgres</strong> recovery is the initial stage. The node API is not functioning during this stage. The stage is expected
to take about 1 hour on the mainnet.</li>
<li><strong>Merkle tree</strong> recovery starts once Postgres is fully recovered. Merkle tree recovery can take about 3 hours on the
mainnet. Ordinarily, Merkle tree recovery is a blocker for node synchronization; i.e., the node will not process
blocks newer than the snapshot block until the Merkle tree is recovered. If the <a href="guides/external-node/10_treeless_mode.html">treeless mode</a>
is enabled, tree recovery is not performed, and the node will start catching up blocks immediately after Postgres
recovery. This is still true if the tree data fetcher is enabled <em>together</em> with a Merkle tree; tree recovery is
asynchronous in this case.</li>
<li>Recovering RocksDB-based <strong>VM state cache</strong> is concurrent with Merkle tree recovery and also depends on Postgres
recovery. It takes about 1 hour on the mainnet. Unlike Merkle tree recovery, VM state cache is not necessary for node
operation (the node will get the state from Postgres is if it is absent), although it considerably speeds up VM
execution.</li>
</ul>
<p>After Postgres recovery is completed, the node becomes operational, providing Web3 API etc. It still needs some time to
catch up executing blocks after the snapshot (i.e, roughly several hours worth of blocks / transactions). This may take
order of 1–2 hours on the mainnet. In total, recovery process and catch-up thus should take roughly 5–6 hours with a
Merkle tree, or 3–4 hours in the treeless mode / with a tree data fetcher.</p>
<h2 id="current-limitations"><a class="header" href="#current-limitations">Current limitations</a></h2>
<p>Nodes recovered from snapshot don’t have any historical data from before the recovery. There is currently no way to
back-fill this historic data. E.g., if a node has recovered from a snapshot for L1 batch 500,000; then, it will not have
data for L1 batches 499,999, 499,998, etc. The relevant Web3 methods, such as <code>eth_getBlockByNumber</code>, will return an
error mentioning the first locally retained block or L1 batch if queried this missing data. The same error messages are
used for <a href="guides/external-node/08_pruning.html">pruning</a> because logically, recovering from a snapshot is equivalent to pruning node storage to
the snapshot L1 batch.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>To enable snapshot recovery on mainnet, you need to set environment variables for a node before starting it for the
first time:</p>
<pre><code class="language-yaml">EN_SNAPSHOTS_RECOVERY_ENABLED: 'true'
EN_SNAPSHOTS_OBJECT_STORE_BUCKET_BASE_URL: 'zksync-era-mainnet-external-node-snapshots'
EN_SNAPSHOTS_OBJECT_STORE_MODE: 'GCSAnonymousReadOnly'
</code></pre>
<p>For the Sepolia testnet, use:</p>
<pre><code class="language-yaml">EN_SNAPSHOTS_RECOVERY_ENABLED: 'true'
EN_SNAPSHOTS_OBJECT_STORE_BUCKET_BASE_URL: 'zksync-era-boojnet-external-node-snapshots'
EN_SNAPSHOTS_OBJECT_STORE_MODE: 'GCSAnonymousReadOnly'
</code></pre>
<p>For a working examples of a fully configured Nodes recovering from snapshots, see
<a href="guides/external-node/docker-compose-examples">Docker Compose examples</a> and <a href="guides/external-node/00_quick_start.html"><em>Quick Start</em></a>.</p>
<p>If a node is already recovered (does not matter whether from a snapshot or from a Postgres dump), setting these env
variables will have no effect; the node will never reset its state.</p>
<h2 id="monitoring-recovery"><a class="header" href="#monitoring-recovery">Monitoring recovery</a></h2>
<p>Snapshot recovery information is logged with the following targets:</p>
<ul>
<li><strong>Recovery orchestration:</strong> <code>zksync_external_node::init</code></li>
<li><strong>Postgres recovery:</strong> <code>zksync_snapshots_applier</code></li>
<li><strong>Merkle tree recovery:</strong> <code>zksync_metadata_calculator::recovery</code>, <code>zksync_merkle_tree::recovery</code></li>
</ul>
<p>An example of snapshot recovery logs during the first node start:</p>
<pre><code class="language-text">2024-06-20T07:25:32.466926Z  INFO zksync_external_node::init: Node has neither genesis L1 batch, nor snapshot recovery info
2024-06-20T07:25:32.466946Z  INFO zksync_external_node::init: Chosen node initialization strategy: SnapshotRecovery
2024-06-20T07:25:32.466951Z  WARN zksync_external_node::init: Proceeding with snapshot recovery. This is an experimental feature; use at your own risk
2024-06-20T07:25:32.475547Z  INFO zksync_snapshots_applier: Found snapshot with data up to L1 batch #7, L2 block #27, version 0, storage logs are divided into 10 chunk(s)
2024-06-20T07:25:32.516142Z  INFO zksync_snapshots_applier: Applied factory dependencies in 27.768291ms
2024-06-20T07:25:32.527363Z  INFO zksync_snapshots_applier: Recovering storage log chunks with 10 max concurrency
2024-06-20T07:25:32.608539Z  INFO zksync_snapshots_applier: Recovered 3007 storage logs in total; checking overall consistency...
2024-06-20T07:25:32.612967Z  INFO zksync_snapshots_applier: Retrieved 2 tokens from main node
2024-06-20T07:25:32.616142Z  INFO zksync_external_node::init: Recovered Postgres from snapshot in 148.523709ms
2024-06-20T07:25:32.645399Z  INFO zksync_metadata_calculator::recovery: Recovering Merkle tree from Postgres snapshot in 1 chunks with max concurrency 10
2024-06-20T07:25:32.650478Z  INFO zksync_metadata_calculator::recovery: Filtered recovered key chunks; 1 / 1 chunks remaining
2024-06-20T07:25:32.681327Z  INFO zksync_metadata_calculator::recovery: Recovered 1/1 Merkle tree chunks, there are 0 left to process
2024-06-20T07:25:32.784597Z  INFO zksync_metadata_calculator::recovery: Recovered Merkle tree from snapshot in 144.040125ms
</code></pre>
<p>(Obviously, timestamps and numbers in the logs will differ.)</p>
<p>Recovery logic also exports some metrics, the main of which are as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>snapshots_applier_storage_logs_chunks_left_to_process</code></td><td>Gauge</td><td>-</td><td>Number of storage log chunks left to process during Postgres recovery</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="pruning"><a class="header" href="#pruning">Pruning</a></h1>
<p>It is possible to configure a ZKsync node to periodically prune all data from L1 batches older than a configurable
threshold. Data is pruned both from Postgres and from tree (RocksDB). Pruning happens continuously (i.e., does not
require stopping the node) in the background during normal node operation. It is designed to not significantly impact
node performance.</p>
<p>Types of pruned data in Postgres include:</p>
<ul>
<li>Block and L1 batch headers</li>
<li>Transactions</li>
<li>EVM logs aka events</li>
<li>Overwritten storage logs</li>
<li>Transaction traces</li>
</ul>
<p>Pruned data is no longer available via Web3 API of the node. The relevant Web3 methods, such as <code>eth_getBlockByNumber</code>,
will return an error mentioning the first retained block or L1 batch if queried pruned data.</p>
<h2 id="interaction-with-snapshot-recovery"><a class="header" href="#interaction-with-snapshot-recovery">Interaction with snapshot recovery</a></h2>
<p>Pruning and <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> are independent features. Pruning works both for archival
nodes restored from a Postgres dump, and nodes recovered from a snapshot. Conversely, a node recovered from a snapshot
may have pruning disabled; this would mean that it retains all data starting from the snapshot indefinitely (but not
earlier data, see <a href="guides/external-node/07_snapshots_recovery.html#current-limitations">snapshot recovery limitations</a>).</p>
<p>A rough guide whether to choose the recovery option and/or pruning is as follows:</p>
<ul>
<li>If you need a node with data retention period of up to a few days, set up a node from a snapshot with pruning enabled
and wait for it to have enough data.</li>
<li>If you need a node with the entire rollup history, using a Postgres dump is the only option, and pruning should be
disabled.</li>
<li>If you need a node with significant data retention (order of months), the best option right now is using a Postgres
dump. You may enable pruning for such a node, but beware that full pruning may take significant amount of time (order
of weeks or months). In the future, we intend to offer pre-pruned Postgres dumps with a few months of data.</li>
</ul>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>You can enable pruning by setting the environment variable</p>
<pre><code class="language-yaml">EN_PRUNING_ENABLED: 'true'
</code></pre>
<p>By default, the node will keep L1 batch data for 7 days determined by the batch timestamp (always equal to the timestamp
of the first block in the batch). You can configure the retention period using:</p>
<pre><code class="language-yaml">EN_PRUNING_DATA_RETENTION_SEC: '259200' # 3 days
</code></pre>
<p>The retention period can be set to any value, but for mainnet values under 24h will be ignored because a batch can only
be pruned after it has been executed on Ethereum.</p>
<p>Pruning can be disabled or enabled and the data retention period can be freely changed during the node lifetime.</p>
<blockquote>
<p>[!WARNING]</p>
<p>Pruning should be disabled when recovering the Merkle tree (e.g., if a node ran in
<a href="guides/external-node/09_treeless_mode.html">the treeless mode</a> before, or if its tree needs a reset for whatever reason). Otherwise, tree
recovery will with almost definitely result in an error, or worse, in a corrupted tree.</p>
</blockquote>
<h2 id="storage-requirements-for-pruned-nodes"><a class="header" href="#storage-requirements-for-pruned-nodes">Storage requirements for pruned nodes</a></h2>
<p>The storage requirements depend on how long you configure to retain the data, but are roughly:</p>
<ul>
<li><strong>40GB + ~5GB/day of retained data</strong> of disk space needed on machine that runs the node</li>
<li><strong>300GB + ~15GB/day of retained data</strong> of disk space for Postgres</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p>When pruning an existing archival node, Postgres will be unable to reclaim disk space automatically. To reclaim disk
space, you need to manually run <code>VACUUM FULL</code>, which requires an <code>ACCESS EXCLUSIVE</code> lock. You can read more about it
in <a href="https://www.postgresql.org/docs/current/sql-vacuum.html">Postgres docs</a>.</p>
</blockquote>
<h2 id="monitoring-pruning"><a class="header" href="#monitoring-pruning">Monitoring pruning</a></h2>
<p>Pruning information is logged with the following targets:</p>
<ul>
<li><strong>Postgres pruning:</strong> <code>zksync_node_db_pruner</code></li>
<li><strong>Merkle tree pruning:</strong> <code>zksync_metadata_calculator::pruning</code>, <code>zksync_merkle_tree::pruning</code>.</li>
</ul>
<p>To check whether Postgres pruning works as intended, you should look for logs like this:</p>
<pre><code class="language-text">2024-06-20T07:26:03.415382Z  INFO zksync_node_db_pruner: Soft pruned db l1_batches up to 8 and L2 blocks up to 29, operation took 14.850042ms
2024-06-20T07:26:04.433574Z  INFO zksync_node_db_pruner::metrics: Performed pruning of database, deleted 1 L1 batches, 2 L2 blocks, 68 storage logs, 383 events, 27 call traces, 12 L2-to-L1 logs
2024-06-20T07:26:04.436516Z  INFO zksync_node_db_pruner: Hard pruned db l1_batches up to 8 and L2 blocks up to 29, operation took 18.653083ms
</code></pre>
<p>(Obviously, timestamps and numbers in the logs will differ.)</p>
<p>Pruning logic also exports some metrics, the main of which are as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>db_pruner_not_pruned_l1_batches_count</code></td><td>Gauge</td><td>-</td><td>Number of retained L1 batches</td></tr>
<tr><td><code>db_pruner_pruning_chunk_duration_seconds</code></td><td>Histogram</td><td><code>prune_type</code></td><td>Latency of a single pruning iteration</td></tr>
<tr><td><code>merkle_tree_pruning_deleted_stale_key_versions</code></td><td>Gauge</td><td><code>bound</code></td><td>Versions (= L1 batches) pruned from the Merkle tree</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="treeless-operation-mode"><a class="header" href="#treeless-operation-mode">Treeless Operation Mode</a></h1>
<p>Normally, a ZKsync node needs to run the Merkle tree component (aka <em>metadata calculator</em>) in order to compute L1 batch
state root hashes. A state root hash from the previous batch can be accessed by L2 contracts, so processing transactions
in an L1 batch cannot start until the state root hash of the previous L1 batch is computed. Merkle tree requires
non-trivial storage space and RAM (roughly 3 TB and 32 GB respectively for an archival mainnet node as of July 2024).
While storage and RAM requirements can be significantly lowered with <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> and
<a href="guides/external-node/08_pruning.html">pruning</a>, <strong>treeless operation mode</strong> allows to run a node without a local Merkle tree instance at all.</p>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How it works</a></h2>
<p>The relevant logic is encapsulated in the <em>tree fetcher</em> component that can be run instead, or concurrently with the
Merkle tree component. Tree fetcher continuously loads L1 batch state root hashes for batches persisted to the node
storage. It uses the following 2 sources of hashes:</p>
<ul>
<li>The rollup contract on L1 (e.g. on the Ethereum mainnet for the Era mainnet). The state root hash for a batch is
submitted to this contract as a part of the batch commitment.</li>
<li>Main L2 node (or more generally, the L2 node that the current node is configured to sync from). Only used if the L1
data source does not work (e.g., very recent L1 batches may be not yet committed to L1).</li>
</ul>
<p>If the tree fetcher run concurrently to the Merkle tree, the tree will still compute state root hashes for all batches.
If the tree is slower than the fetcher (which is expected in most cases), it will compare the computed hash against the
state root hash from the tree fetcher and crash on a mismatch.</p>
<h2 id="tradeoffs"><a class="header" href="#tradeoffs">Tradeoffs</a></h2>
<ul>
<li>Tree fetcher requires limited trust to the L1 Web3 provider and the main L2 node (note that trust in them is required
for other node components, such as <a href="guides/external-node/06_components.html#consistency-checker">the consistency checker</a> and
<a href="guides/external-node/06_components.html#reorg-detector">reorg detector</a>). This trust is limited in time; mismatched L1 batch root hashes
will eventually be detected by the 2 aforementioned components and the Merkle tree (if it is run concurrently).</li>
<li>Tree fetcher only loads root hashes of the Merkle tree, not other tree data. That is, it cannot replace the Merkle
tree if a node needs to serve the <code>zks_getProof</code> endpoint, since it fetches proofs from the Merkle tree.</li>
</ul>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<p>The tree fetcher is disabled by default. You can enable it by specifying <code>tree_fetcher</code> in the list of components that a
node should run in the <code>--components</code> command-line arg. For example, to run all standard components and the tree
fetcher:</p>
<pre><code class="language-shell"># Assume that the node binary in in $PATH
zksync_external_node --components=all,tree_fetcher
</code></pre>
<p>To run all standard components without the Merkle tree and the tree fetcher:</p>
<pre><code class="language-shell">zksync_external_node --components=core,api,tree_fetcher
</code></pre>
<p>The tree fetcher currently does not have configurable parameters.</p>
<p>The tree fetcher can be freely switched on or off during the node lifetime; i.e., it’s not required to commit to running
or not running it when initializing a node.</p>
<blockquote>
<p>[!TIP]</p>
<p>Switching on the tree fetcher during <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> can significantly speed it up
(order of 2–3 hours for the mainnet) because the node no longer needs to recover the Merkle tree before starting
catching up.</p>
</blockquote>
<h2 id="monitoring-tree-fetcher"><a class="header" href="#monitoring-tree-fetcher">Monitoring tree fetcher</a></h2>
<p>Tree fetcher information is logged with the <code>zksync_node_sync::tree_data_fetcher</code> target.</p>
<p>Tree fetcher exports some metrics:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>external_node_tree_data_fetcher_last_updated_batch_number</code></td><td>Gauge</td><td>-</td><td>Last L1 batch with tree data updated by the fetcher</td></tr>
<tr><td><code>external_node_tree_data_fetcher_step_outcomes</code></td><td>Counter</td><td><code>kind</code></td><td>Number of times a fetcher step resulted in a certain outcome (e.g., update, no-op, or transient error)</td></tr>
<tr><td><code>external_node_tree_data_fetcher_root_hash_sources</code></td><td>Counter</td><td><code>source</code></td><td>Number of root hashes fetched from a particular source (L1 or L2).</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="decentralization"><a class="header" href="#decentralization">Decentralization</a></h1>
<p>In the default setup, the ZKsync node will fetch data from the ZKsync API endpoint maintained by Matter Labs. To reduce
the reliance on this centralized endpoint we have developed a decentralized p2p networking stack (aka gossipnet) which
will eventually be used instead of ZKsync API for synchronizing data.</p>
<p>On the gossipnet, the data integrity will be protected by the BFT (byzantine fault-tolerant) consensus algorithm
(currently data is signed just by the main node though).</p>
<h2 id="enabling-gossipnet-on-your-node"><a class="header" href="#enabling-gossipnet-on-your-node">Enabling gossipnet on your node</a></h2>
<blockquote>
<p>[!NOTE]</p>
<p>The minimal supported server version for this is
<a href="https://github.com/matter-labs/zksync-era/releases/tag/core-v24.11.0">24.11.0</a></p>
</blockquote>
<h3 id="generating-secrets"><a class="header" href="#generating-secrets">Generating secrets</a></h3>
<p>Each participant node of the gossipnet has to have an identity (a public/secret key pair). When running your node for
the first time, generate the secrets by running:</p>
<pre><code>docker run --entrypoint /usr/bin/zksync_external_node "matterlabs/external-node:2.0-v25.1.0" generate-secrets &gt; consensus_secrets.yaml
chmod 600 consensus_secrets.yaml
</code></pre>
<blockquote>
<p>[!NOTE]</p>
<p>NEVER reveal the secret keys used by your node. Otherwise, someone can impersonate your node on the gossipnet. If you
suspect that your secret key has been leaked, you can generate fresh keys using the same tool.</p>
<p>If you want someone else to connect to your node, give them your PUBLIC key instead. Both public and secret keys are
present in the <code>consensus_secrets.yaml</code> (public keys are in comments).</p>
</blockquote>
<h3 id="preparing-configuration-file"><a class="header" href="#preparing-configuration-file">Preparing configuration file</a></h3>
<p>Copy the template of the consensus configuration file (for
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/external-node/prepared_configs/mainnet_consensus_config.yaml">mainnet</a>
or
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/external-node/prepared_configs/testnet_consensus_config.yaml">testnet</a>
).</p>
<blockquote>
<p>[!NOTE]</p>
<p>You need to fill in the <code>public_addr</code> field. This is the address that will (not implemented yet) be advertised over
gossipnet to other nodes, so that they can establish connections to your node. If you don’t want to expose your node
to the public internet, you can use IP in your local network.</p>
</blockquote>
<p>Currently the config contains the following fields (refer to config
<a href="https://github.com/matter-labs/zksync-era/blob/990676c5f84afd2ff8cd337f495c82e8d1f305a4/core/lib/protobuf_config/src/proto/core/consensus.proto#L66">schema</a>
for more details):</p>
<ul>
<li><code>server_addr</code> - local TCP socket address that the node should listen on for incoming connections. Note that this is an
additional TCP port that will be opened by the node.</li>
<li><code>public_addr</code> - the public address of your node that will be advertised over the gossipnet.</li>
<li><code>max_payload_size</code> - limit (in bytes) on the sized of the ZKsync ERA block received from the gossipnet. This protects
your node from getting DoS`ed by too large network messages. Use the value from the template.</li>
<li><code>gossip_dynamic_inbound_limit</code> - maximal number of unauthenticated concurrent inbound connections that can be
established to your node. This is a DDoS protection measure.</li>
</ul>
<h3 id="setting-environment-variables"><a class="header" href="#setting-environment-variables">Setting environment variables</a></h3>
<p>Uncomment (or add) the following lines in your <code>.env</code> config:</p>
<pre><code>EN_CONSENSUS_CONFIG_PATH=...
EN_CONSENSUS_SECRETS_PATH=...
</code></pre>
<p>These variables should point to your consensus config and secrets files that we have just created. Tweak the paths to
the files if you have placed them differently.</p>
<h3 id="add---enable-consensus-flag-to-your-entry-point-command"><a class="header" href="#add---enable-consensus-flag-to-your-entry-point-command">Add <code>--enable-consensus</code> flag to your entry point command</a></h3>
<p>For the consensus configuration to take effect you have to add <code>--enable-consensus</code> flag to the command line when
running the node, for example:</p>
<pre><code>docker run "matterlabs/external-node:2.0-v24.12.0" &lt;all the other flags&gt; --enable-consensus
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h1>
<p>The goal of the ZK Stack is to power the internet of value. Value needs to be secured, and only blockchains are able
provide the level of security that the internet needs. The ZK Stack can be used to launch zero-knowledge rollups, which
are extra secure blockchains.</p>
<p>ZK Rollups use advanced mathematics called zero-knowledge proofs to show that the execution of the rollup was done
correctly. They also send (“roll up”) their data to another chain, in our case this is Ethereum. The ZK Stack uses the
zkEVM to execute transactions, making it Ethereum compatible.</p>
<p>These two techniques allow the rollup to be verified externally. Unlike traditional blockchains, where you have to run a
node to verify all transactions, the state of the rollup can be easily checked by external participants by validating
the proof.</p>
<p>These external validators of a rollup can be other rollups. This means we can connect rollups trustlessly, and create a
network of rollups. This network is called the ZK Chain ecosystem.</p>
<p>These specs will provide a high level overview of the zkEVM and a full specification of its more technical components,
such as the prover, compiler, and the VM itself. We also specify the foundations of the ZK Chain ecosystem.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-2"><a class="header" href="#overview-2">Overview</a></h1>
<p>As stated in the introduction, the ZK Stack can be used to launch rollups. These rollups have some operators that are
needed to run it, these are the sequencer and the prover, they create blocks and proofs, and submit them to the L1
contract.</p>
<p>A user submits their transaction to the sequencer. The job of the sequencer is to collect transactions and execute them
using the zkEVM, and to provide a soft confirmation to the user that their transaction was executed. If the user chooses
they can force the sequencer to include their transaction by submitting it via L1. After the sequencer executes the
block, it sends it over to the prover, who creates a cryptographic proof of the block’s execution. This proof is then
sent to the L1 contract alongside the necessary data. On the L1 a <a href="specs/./l1_smart_contracts.html">smart contract</a> verifies
that the proof is valid and all the data has been submitted, and the rollup’s state is also updated in the contract.</p>
<p><img src="specs/./img/L2_Components.png" alt="Components" /></p>
<p>The core of this mechanism was the execution of transactions. The ZK Stack uses the <a href="specs/./zk_evm/README.html">zkEVM</a> for
this, which is similar to the EVM, but its role is different than the EVM’s role in Ethereum.</p>
<p>Transactions can also be submitted via L1. This happens via the same process that allows
<a href="specs/./l1_l2_communication/README.html">L1&lt;&gt;L2 communication</a>. This method provides the rollup with censorship resistance, and
allows trustless bridges to the L1.</p>
<p>The sequencer collects transactions into blocks <a href="specs/./blocks_batches.html">blocks</a>, similarly to Ethereum. To provide the
best UX the protocol has small blocks with quick soft confirmations for the users. Unlike Ethereum, the zkEVM does not
just have blocks, but also batches, which are just a collection of blocks. A batch is the unit that the prover
processes.</p>
<p>Before we submit a proof we send the <a href="specs/./data_availability/README.html">data</a> to L1. Instead of submitting the data of each
transaction, we submit how the state of the blockchain changes, this change is called the state diff. This approach
allows the transactions that change the same storage slots to be very cheap, since these transactions don’t incur
additional data costs.</p>
<p>Finally at the end of the process, we <a href="specs/./data_availability/README.html">create the proofs</a> and send them to L1. Our Boojum
proof system provides excellent performance, and can be run on just 16Gb of GPU RAM. This will enable the proof
generation to be truly decentralized.</p>
<p>Up to this point we have only talked about a single chain. We will connect these chains into a single ecosystem, called
<a href="specs/./zk_chains/README.html">ZK Chain ecosystem</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blocks--batches---how-we-package-transactions"><a class="header" href="#blocks--batches---how-we-package-transactions">Blocks &amp; Batches - How we package transactions</a></h1>
<p>In this article, we will explore the processing of transactions, how we group them into blocks, what it means to “seal”
a block, and why it is important to have rollbacks in our virtual machine (VM).</p>
<p>At the basic level, we have individual transactions. However, to execute them more efficiently, we group them together
into blocks &amp; batches.</p>
<h2 id="l1-batch-vs-l2-block-aka-miniblock-vs-transaction"><a class="header" href="#l1-batch-vs-l2-block-aka-miniblock-vs-transaction">L1 Batch vs L2 Block (a.k.a MiniBlock) vs Transaction</a></h2>
<p>To help visualize the concept, here are two images:</p>
<p><img src="https://user-images.githubusercontent.com/128217157/236494232-aeed380c-78f6-4fda-ab2a-8de26c1089ff.png" alt="Block layout" title="block layout" /></p>
<p>You can refer to the Block layout image to see how the blocks are organized. It provides a graphical representation of
how transactions are arranged within the blocks and the arrangement of L2 blocks within L1 “batches.”</p>
<p><img src="https://user-images.githubusercontent.com/128217157/236500717-165470ad-30b8-4ad6-97ed-fc29c8eb1fe0.png" alt="Explorer example" title="explorer example" /></p>
<h3 id="l2-blocks-aka-miniblocks"><a class="header" href="#l2-blocks-aka-miniblocks">L2 blocks (aka Miniblocks)</a></h3>
<p>Currently, the L2 blocks do not have a major role in the system, until we transition to a decentralized sequencer. We
introduced them mainly as a “compatibility feature” to accommodate various tools, such as Metamask, which expect a block
that changes frequently. This allows these tools to provide feedback to users, confirming that their transaction has
been added.</p>
<p>As of now, an L2 block is created every 2 seconds (controlled by StateKeeper’s config <code>miniblock_commit_deadline_ms</code>),
and it includes all the transactions received during that time period. This periodic creation of L2 blocks ensures that
transactions are processed and included in the blocks regularly.</p>
<h3 id="l1-batches"><a class="header" href="#l1-batches">L1 batches</a></h3>
<p>L1 batches play a crucial role because they serve as the fundamental unit for generating proofs. From the perspective of
the virtual machine (VM), each L1 batch represents the execution of a single program, specifically the Bootloader. The
Bootloader internally processes all the transactions belonging to that particular batch. Therefore, the L1 batch serves
as the container for executing the program and handling the transactions within it.</p>
<h4 id="so-how-large-can-l1-batch-be"><a class="header" href="#so-how-large-can-l1-batch-be">So how large can L1 batch be</a></h4>
<p>Most blockchains use factors like time and gas usage to determine when a block should be closed or sealed. However, our
case is a bit more complex because we also need to consider prover capacity and limits related to publishing to L1.</p>
<p>The decision of when to seal the block is handled by the code in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/seal_criteria/conditional_sealer.rs#20" title="Conditional Sealer">conditional_sealer</a> module. It
maintains a list of <code>SealCriterion</code> and at the time of writing this article, <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/seal_criteria/mod.rs#L106" title="Reasons for Sealing">we have 9 reasons to seal the
block</a>, which include:</p>
<ul>
<li>Transaction slots limit (currently set to 750 transactions in <code>StateKeeper</code>’s config - <code>transaction_slots</code>).</li>
<li>Gas limit (currently set to <code>MAX_L2_TX_GAS_LIMIT</code> = 80M).</li>
<li>Published data limit (as each L1 batch must publish information about the changed slots to L1, so all the changes must
fit within the L1 transaction limit, currently set to <code>MAX_PUBDATA_PER_L1_BATCH</code>= 120k).</li>
<li>zkEVM Geometry limits - For certain operations like merklelization, there is a maximum number of circuits that can be
included in a single L1 batch. If this limit is exceeded, we wouldn’t be able to generate the proof.</li>
</ul>
<p>We also have a <code>TimeoutCriterion</code> - but it is not enabled.</p>
<p>However, these sealing criteria pose a significant challenge because it is difficult to predict in advance whether
adding a given transaction to the current batch will exceed the limits or not. This unpredictability adds complexity to
the process of determining when to seal the block.</p>
<h4 id="what-if-a-transaction-doesnt-fit"><a class="header" href="#what-if-a-transaction-doesnt-fit">What if a transaction doesn’t fit</a></h4>
<p>To handle situations where a transaction exceeds the limits of the currently active L1 batch, we employ a “try and
rollback” approach. This means that we attempt to add the transaction to the active L1 batch, and if we receive a
<code>ExcludeAndSeal</code> response indicating that it doesn’t fit, we roll back the virtual machine (VM) to the state before the
transaction was attempted.</p>
<p>Implementing this approach introduces a significant amount of complexity in the <code>oracles</code> (also known as interfaces) of
the VM. These oracles need to support snapshotting and rolling back operations to ensure consistency when handling
transactions that don’t fit.</p>
<p>In a separate article, we will delve into more details about how these oracles and the VM work, providing a
comprehensive understanding of their functionality and interactions.</p>
<h2 id="deeper-dive"><a class="header" href="#deeper-dive">Deeper dive</a></h2>
<h3 id="glossary"><a class="header" href="#glossary">Glossary</a></h3>
<ul>
<li>Batch - a set of transactions that the bootloader processes (<code>commitBatches</code>, <code>proveBatches</code>,
and <code>executeBatches</code> work with it). A batch consists of multiple transactions.</li>
<li>L2 block - a non-intersecting sub-set of consecutively executed transactions. This is the kind of block you see in the
API. This is the one that will <em>eventually</em> be used for <code>block.number</code>/<code>block.timestamp</code>/etc. This will happen
<em>eventually</em>, since at the time of this writing the virtual block migration is being
<a href="specs/blocks_batches.html#migration--virtual-blocks-logic">run</a>.</li>
<li>Virtual block — blocks the data of which will be returned in the contract execution environment during the migration.
They are called “virtual”, since they have no trace in our API, i.e. it is not possible to query information about
them in any way.</li>
</ul>
<h3 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h3>
<p>Before the recent upgrade, <code>block.number</code>, <code>block.timestamp</code>, as well as <code>blockhash</code> in Solidity, returned information
about <em>batches</em>, i.e. large blocks that are proven on L1 and which consist of many small L2 blocks. At the same time,
API returns <code>block.number</code> and <code>block.timestamp</code> as for L2 blocks.</p>
<p>L2 blocks were created for fast soft confirmation on wallets and block explorer. For example, MetaMask shows
transactions as confirmed only after the block in which transaction execution was mined. So if the user needs to wait
for the batch confirmation it would take at least minutes (for soft confirmation) and hours for full confirmation which
is very bad UX. But API could return soft confirmation much earlier through L2 blocks.</p>
<p>There was a huge outcry in the community for us to return the information for L2 blocks in <code>block.number</code>,
<code>block.timestamp</code>, as well as <code>blockhash</code>, because of discrepancy of runtime execution and returned data by API.</p>
<p>However, there were over 15mln L2 blocks, while less than 200k batches, meaning that if we simply “switched” from
returning L1 batches’ info to L2 block’s info, some contracts (especially those that use <code>block.number</code> for measuring
time intervals instead of <code>block.timestamp</code>) would break. For that, we decided to have an accelerated migration process,
i.e. the <code>block.number</code> will grow faster and faster, until it becomes roughly 8x times the L2 block production speed,
allowing it to gradually reach the L2 block number, after which the information on the L2 <code>block.number</code> will be
returned. The blocks the info of which will be returned during this process are called “virtual blocks”. Their
information will never be available in any of our APIs, which should not be a major breaking change, since our API
already mostly works with L2 blocks, while L1 batches’s information is returned in the runtime.</p>
<h3 id="adapting-for-solidity"><a class="header" href="#adapting-for-solidity">Adapting for Solidity</a></h3>
<p>In order to get the returned value for <code>block.number</code>, <code>block.timestamp</code>, <code>blockhash</code> our compiler used the following
functions:</p>
<ul>
<li><code>getBlockNumber</code></li>
<li><code>getBlockTimestamp</code></li>
<li><code>getBlockHashEVM</code></li>
</ul>
<p>During the migration process, these will return the values of the virtual blocks. After the migration is complete, they
will return values for L2 blocks.</p>
<h3 id="migration-status"><a class="header" href="#migration-status">Migration status</a></h3>
<p>At the time of this writing, the migration has been complete on testnet, i.e. there we already have only the L2 block
information returned. However, the <a href="https://github.com/zkSync-Community-Hub/zksync-developers/discussions/87">migration</a>
on mainnet is still ongoing and most likely will end on late October / early November.</p>
<h2 id="blocks-processing-and-consistency-checks"><a class="header" href="#blocks-processing-and-consistency-checks">Blocks’ processing and consistency checks</a></h2>
<p>Our <code>SystemContext</code> contract allows to get information about batches and L2 blocks. Some of the information is hard to
calculate onchain. For instance, time. The timing information (for both batches and L2 blocks) are provided by the
operator. In order to check that the operator provided some realistic values, certain checks are done on L1. Generally
though, we try to check as much as we can on L2.</p>
<h2 id="initializing-l1-batch"><a class="header" href="#initializing-l1-batch">Initializing L1 batch</a></h2>
<p>At the start of the batch, the operator
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3636">provides</a>
the timestamp of the batch, its number and the hash of the previous batch.. The root hash of the Merkle tree serves as
the root hash of the batch.</p>
<p>The SystemContext can immediately check whether the provided number is the correct batch number. It also immediately
sends the previous batch hash to L1, where it will be checked during the commit operation. Also, some general
consistency checks are performed. This logic can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L416">here</a>.</p>
<h2 id="l2-blocks-processing-and-consistency-checks"><a class="header" href="#l2-blocks-processing-and-consistency-checks">L2 blocks processing and consistency checks</a></h2>
<h3 id="setl2block"><a class="header" href="#setl2block"><code>setL2Block</code></a></h3>
<p>Before each transaction, we call <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2605">method</a>.
There we will provide some data about the L2 block that the transaction belongs to:</p>
<ul>
<li><code>_l2BlockNumber</code> The number of the new L2 block.</li>
<li><code>_l2BlockTimestamp</code> The timestamp of the new L2 block.</li>
<li><code>_expectedPrevL2BlockHash</code> The expected hash of the previous L2 block.</li>
<li><code>_isFirstInBatch</code> Whether this method is called for the first time in the batch.</li>
<li><code>_maxVirtualBlocksToCreate</code> The maximum number of virtual block to create with this L2 block.</li>
</ul>
<p>If two transactions belong to the same L2 block, only the first one may have non-zero <code>_maxVirtualBlocksToCreate</code>. The
rest of the data must be same.</p>
<p>The <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L312">performs</a>
a lot of similar consistency checks to the ones for the L1 batch.</p>
<h3 id="l2-blockhash-calculation-and-storage"><a class="header" href="#l2-blockhash-calculation-and-storage">L2 blockhash calculation and storage</a></h3>
<p>Unlike L1 batch’s hash, the L2 blocks’ hashes can be checked on L2.</p>
<p>The hash of an L2 block is
<code>keccak256(abi.encode(_blockNumber, _blockTimestamp, _prevL2BlockHash, _blockTxsRollingHash))</code>. Where
<code>_blockTxsRollingHash</code> is defined in the following way:</p>
<p><code>_blockTxsRollingHash = 0</code> for an empty block.</p>
<p><code>_blockTxsRollingHash = keccak(0, tx1_hash)</code> for a block with one tx.</p>
<p><code>_blockTxsRollingHash = keccak(keccak(0, tx1_hash), tx2_hash)</code> for a block with two txs, etc.</p>
<p>To add a transaction hash to the current miniblock we use the <code>appendTransactionToCurrentL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L373">function</a>.</p>
<p>Since ZKsync is a state-diff based rollup, there is no way to deduce the hashes of the L2 blocks based on the
transactions’ in the batch (because there is no access to the transaction’s hashes). At the same time, in order to
server <code>blockhash</code> method, the VM requires the knowledge of some of the previous L2 block hashes. In order to save up on
pubdata (by making sure that the same storage slots are reused, i.e. we only have repeated writes) we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L70">store</a>
only the last 257 block hashes. You can read more on what are the repeated writes and how the pubdata is processed
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>We store only the last 257 blocks, since the EVM requires only 256 previous ones and we use 257 as a safe margin.</p>
<h3 id="legacy-blockhash"><a class="header" href="#legacy-blockhash">Legacy blockhash</a></h3>
<p>When initializing L2 blocks that do not have their hashes stored on L2 (basically these are blocks before the migration
upgrade), we use the following formula for their hash:</p>
<p><code>keccak256(abi.encodePacked(uint32(_blockNumber)))</code></p>
<h3 id="timing-invariants"><a class="header" href="#timing-invariants">Timing invariants</a></h3>
<p>While the timestamp of each L2 block is provided by the operator, there are some timing invariants that the system
preserves:</p>
<ul>
<li>For each L2 block its timestamp should be &gt; the timestamp of the previous L2 block</li>
<li>For each L2 block its timestamp should be ≥ timestamp of the batch it belongs to</li>
<li>Each batch must start with a new L2 block (i.e. an L2 block can not span across batches).</li>
<li>The timestamp of a batch must be ≥ the timestamp of the latest L2 block which belonged to the previous batch.</li>
<li>The timestamp of the last miniblock in batch can not go too far into the future. This is enforced by publishing an
L2→L1 log, with the timestamp which is then checked on L1.</li>
</ul>
<h2 id="fictive-l2-block--finalizing-the-batch"><a class="header" href="#fictive-l2-block--finalizing-the-batch">Fictive L2 block &amp; finalizing the batch</a></h2>
<p>At the end of the batch, the bootloader calls the <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3812">one more time</a>
to allow the operator to create a new empty block. This is done purely for some of the technical reasons inside the
node, where each batch ends with an empty L2 block.</p>
<p>We do not enforce that the last block is empty explicitly as it complicates the development process and testing, but in
practice, it is, and either way, it should be secure.</p>
<p>Also, at the end of the batch we send the timestamps of the batch as well as the timestamp of the last miniblock in
order to check on L1 that both of these are realistic. Checking any other L2 block’s timestamp is not required since all
of them are enforced to be between those two.</p>
<h2 id="migration--virtual-blocks-logic"><a class="header" href="#migration--virtual-blocks-logic">Migration &amp; virtual blocks’ logic</a></h2>
<p>As already explained above, for a smoother upgrade for the ecosystem, there is a migration being performed during which
instead of returning either batch information or L2 block information, we will return the virtual block information
until they catch up with the L2 block’s number.</p>
<h3 id="production-of-the-virtual-blocks"><a class="header" href="#production-of-the-virtual-blocks">Production of the virtual blocks</a></h3>
<ul>
<li>In each batch, there should be at least one virtual block created.</li>
<li>Whenever a new L2 block is created, the operator can select how many virtual blocks it wants to create. This can be
any number, however, if the number of the virtual block exceeds the L2 block number, the migration is considered
complete and we switch to the mode where the L2 block information will be returned.</li>
</ul>
<h2 id="additional-note-on-blockhashes"><a class="header" href="#additional-note-on-blockhashes">Additional note on blockhashes</a></h2>
<p>Note, that if we used some complex formula for virtual blocks’ hashes (like we do for L2 blocks), we would have to put
all of these into storage for the data availability. Even if we used the same storage trick that we used for the L2
blocks, where we store only the last 257’s block’s hashes under the current load/migration plans it would be expected
that we have roughly ~250 virtual blocks per batch, practically meaning that we will publish all of these anyway. This
would be too expensive. That is why we have to use a simple formula of <code>keccak(uint256(number))</code> for now. Note, that
they do not collide with the legacy miniblock hash, since legacy miniblock hashes are calculated as
<code>keccak(uint32(number))</code>.</p>
<p>Also, we need to keep the consistency of previous blockhashes, i.e. if <code>blockhash(X)</code> returns a non-zero value, it
should be consistent among the future blocks. For instance, let’s say that the hash of batch <code>1000</code> is <code>1</code>,
i.e. <code>blockhash(1000) = 1</code>. Then, when we migrate to virtual blocks, we need to ensure that <code>blockhash(1000)</code> will
return either 0 (if and only if the block is more than 256 blocks old) or <code>1</code>. Because of that for <code>blockhash</code> we will
have the following complex
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L103">logic</a>:</p>
<ul>
<li>For blocks that were created before the virtual block upgrade, use the batch hashes</li>
<li>For blocks that were created during the virtual block upgrade, use <code>keccak(uint256(number))</code>.</li>
<li>For blocks that were created after the virtual blocks have caught up with the L2 blocks, use L2 block hashes.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l1-smart-contracts"><a class="header" href="#l1-smart-contracts">L1 Smart contracts</a></h1>
<p>This document presumes familiarity with Rollups. For a better understanding, consider reading the overview
<a href="specs/./overview.html">here</a>.</p>
<p>Rollups inherit security and decentralization guarantees from Ethereum, on which they store information about changes in
their own state, providing validity proofs for state transition, implementing a communication mechanism, etc. In
practice, all this is achieved by Smart Contracts built on top of Ethereum. This document details the architecture of
the L2 contracts on Ethereum Layer 1. We also have contracts that support the ZK Chain ecosystem, we cover those in the
<a href="specs/./zk_chains/shared_bridge.html">Shared Bridge</a> section. The Shared Bridge relies on these individual contracts.</p>
<h2 id="diamond"><a class="header" href="#diamond">Diamond</a></h2>
<p>Technically, this L1 smart contract acts as a connector between Ethereum (L1) and a single L2. It checks the validity
proof and data availability, handles L2 &lt;-&gt; L1 communication, finalizes L2 state transition, and more.</p>
<p><img src="specs/./img/diamondProxy.jpg" alt="diamondProxy.png" /></p>
<h3 id="diamondproxy"><a class="header" href="#diamondproxy">DiamondProxy</a></h3>
<p>The main contract uses <a href="https://eips.ethereum.org/EIPS/eip-2535">EIP-2535</a> diamond proxy pattern. It is an in-house
implementation that is inspired by the <a href="https://github.com/mudgen/Diamond">mudgen reference implementation</a>. It has no
external functions, only the fallback that delegates a call to one of the facets (target/implementation contract). So
even an upgrade system is a separate facet that can be replaced.</p>
<p>One of the differences from the reference implementation is access freezable. Each of the facets has an associated
parameter that indicates if it is possible to freeze access to the facet. Privileged actors can freeze the <strong>diamond</strong>
(not a specific facet!) and all facets with the marker <code>isFreezable</code> should be inaccessible until the governor or admin
unfreezes the diamond. Note that it is a very dangerous thing since the diamond proxy can freeze the upgrade system and
then the diamond will be frozen forever.</p>
<p>The diamond proxy pattern is very flexible and extendable. For now, it allows splitting implementation contracts by
their logical meaning, removes the limit of bytecode size per contract and implements security features such as
freezing. In the future, it can also be viewed as <a href="https://eips.ethereum.org/EIPS/eip-6900">EIP-6900</a> for
<a href="https://blog.matter-labs.io/introducing-the-zk-stack-c24240c2532a">ZK Stack</a>, where each ZK Chain can implement a
sub-set of allowed implementation contracts.</p>
<h3 id="gettersfacet"><a class="header" href="#gettersfacet">GettersFacet</a></h3>
<p>Separate facet, whose only function is providing <code>view</code> and <code>pure</code> methods. It also implements
<a href="https://eips.ethereum.org/EIPS/eip-2535#diamond-loupe">diamond loupe</a> which makes managing facets easier. This contract
must never be frozen.</p>
<h3 id="adminfacet"><a class="header" href="#adminfacet">AdminFacet</a></h3>
<p>Controls changing the privileged addresses such as governor and validators or one of the system parameters (L2
bootloader bytecode hash, verifier address, verifier parameters, etc), and it also manages the freezing/unfreezing and
execution of upgrades in the diamond proxy.</p>
<p>The admin facet is controlled by two entities:</p>
<ul>
<li>Governance - Separate smart contract that can perform critical changes to the system as protocol upgrades. This
contract controlled by two multisigs, one managed by Matter Labs team and another will be multisig with well-respected
contributors in the crypto space. Only together they can perform an instant upgrade, the Matter Labs team can only
schedule an upgrade with delay.</li>
<li>Admin - Multisig smart contract managed by Matter Labs that can perform non-critical changes to the system such as
granting validator permissions. Note, that the Admin is the same multisig as the owner of the governance.</li>
</ul>
<h3 id="mailboxfacet"><a class="header" href="#mailboxfacet">MailboxFacet</a></h3>
<p>The facet that handles L2 &lt;-&gt; L1 communication, an overview for which can be found in
<a href="https://docs.zksync.io/build/developer-reference/l1-l2-interoperability">docs</a>.</p>
<p>The Mailbox performs three functions:</p>
<ul>
<li>L1 &lt;-&gt; L2 communication.</li>
<li>Bridging native Ether to the L2 (with the launch of the Shared Bridge this will be moved)</li>
<li>Censorship resistance mechanism (in the research stage).</li>
</ul>
<p>L1 -&gt; L2 communication is implemented as requesting an L2 transaction on L1 and executing it on L2. This means a user
can call the function on the L1 contract to save the data about the transaction in some queue. Later on, a validator can
process it on L2 and mark it as processed on the L1 priority queue. Currently, it is used for sending information from
L1 to L2 or implementing multi-layer protocols.</p>
<p><em>NOTE</em>: While user requests the transaction from L1, the initiated transaction on L2 will have such a <code>msg.sender</code>:</p>
<pre><code class="language-solidity">  address sender = msg.sender;
  if (sender != tx.origin) {
      sender = AddressAliasHelper.applyL1ToL2Alias(msg.sender);
  }
</code></pre>
<p>where</p>
<pre><code class="language-solidity">uint160 constant offset = uint160(0x1111000000000000000000000000000000001111);

function applyL1ToL2Alias(address l1Address) internal pure returns (address l2Address) {
  unchecked {
    l2Address = address(uint160(l1Address) + offset);
  }
}

</code></pre>
<p>For most of the rollups the address aliasing needs to prevent cross-chain exploits that would otherwise be possible if
we simply reused the same L1 addresses as the L2 sender. In zkEVM address derivation rule is different from the
Ethereum, so cross-chain exploits are already impossible. However, the zkEVM may add full EVM support in the future, so
applying address aliasing leaves room for future EVM compatibility.</p>
<p>The L1 -&gt; L2 communication is also used for bridging ether. The user should include a <code>msg.value</code> when initiating a
transaction request on the L1 contract. Before executing a transaction on L2, the specified address will be credited
with the funds. To withdraw funds user should call <code>withdraw</code> function on the <code>L2EtherToken</code> system contracts. This will
burn the funds on L2, allowing the user to reclaim them through the <code>finalizeEthWithdrawal</code> function on the
<code>MailboxFacet</code>.</p>
<p>More about L1-&gt;L2 operations can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>L2 -&gt; L1 communication, in contrast to L1 -&gt; L2 communication, is based only on transferring the information, and not on
the transaction execution on L1. The full description of the mechanism for sending information from L2 to L1 can be
found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</p>
<h3 id="executorfacet"><a class="header" href="#executorfacet">ExecutorFacet</a></h3>
<p>A contract that accepts L2 batches, enforces data availability and checks the validity of zk-proofs.</p>
<p>The state transition is divided into three stages:</p>
<ul>
<li><code>commitBatches</code> - check L2 batch timestamp, process the L2 logs, save data for a batch, and prepare data for zk-proof.</li>
<li><code>proveBatches</code> - validate zk-proof.</li>
<li><code>executeBatches</code> - finalize the state, marking L1 -&gt; L2 communication processing, and saving Merkle tree with L2 logs.</li>
</ul>
<p>Each L2 -&gt; L1 system log will have a key that is part of the following:</p>
<pre><code class="language-solidity">enum SystemLogKey {
  L2_TO_L1_LOGS_TREE_ROOT_KEY,
  TOTAL_L2_TO_L1_PUBDATA_KEY,
  STATE_DIFF_HASH_KEY,
  PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY,
  PREV_BATCH_HASH_KEY,
  CHAINED_PRIORITY_TXN_HASH_KEY,
  NUMBER_OF_LAYER_1_TXS_KEY,
  EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY
}

</code></pre>
<p>When a batch is committed, we process L2 -&gt; L1 system logs. Here are the invariants that are expected there:</p>
<ul>
<li>In a given batch there will be either 7 or 8 system logs. The 8th log is only required for a protocol upgrade.</li>
<li>There will be a single log for each key that is contained within <code>SystemLogKey</code></li>
<li>Three logs from the <code>L2_TO_L1_MESSENGER</code> with keys:</li>
<li><code>L2_TO_L1_LOGS_TREE_ROOT_KEY</code></li>
<li><code>TOTAL_L2_TO_L1_PUBDATA_KEY</code></li>
<li><code>STATE_DIFF_HASH_KEY</code></li>
<li>Two logs from <code>L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR</code> with keys:
<ul>
<li><code>PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY</code></li>
<li><code>PREV_BATCH_HASH_KEY</code></li>
</ul>
</li>
<li>Two or three logs from <code>L2_BOOTLOADER_ADDRESS</code> with keys:
<ul>
<li><code>CHAINED_PRIORITY_TXN_HASH_KEY</code></li>
<li><code>NUMBER_OF_LAYER_1_TXS_KEY</code></li>
<li><code>EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY</code></li>
</ul>
</li>
<li>None logs from other addresses (may be changed in the future).</li>
</ul>
<h3 id="diamondinit"><a class="header" href="#diamondinit">DiamondInit</a></h3>
<p>It is a one-function contract that implements the logic of initializing a diamond proxy. It is called only once on the
diamond constructor and is not saved in the diamond as a facet.</p>
<p>Implementation detail - function returns a magic value just like it is designed in
<a href="https://eips.ethereum.org/EIPS/eip-1271">EIP-1271</a>, but the magic value is 32 bytes in size.</p>
<h2 id="bridges"><a class="header" href="#bridges">Bridges</a></h2>
<p>Bridges are completely separate contracts from the Diamond. They are a wrapper for L1 &lt;-&gt; L2 communication on contracts
on both L1 and L2. Upon locking assets on L1, a request is sent to mint these bridged assets on L2. Upon burning assets
on L2, a request is sent to unlock them on L2.</p>
<p>Unlike the native Ether bridging, all other assets can be bridged by the custom implementation relying on the trustless
L1 &lt;-&gt; L2 communication.</p>
<h3 id="l1erc20bridge"><a class="header" href="#l1erc20bridge">L1ERC20Bridge</a></h3>
<p>The “legacy” implementation of the ERC20 token bridge. Works only with regular ERC20 tokens, i.e. not with
fee-on-transfer tokens or other custom logic for handling user balances.</p>
<ul>
<li><code>deposit</code> - lock funds inside the contract and send a request to mint bridged assets on L2.</li>
<li><code>claimFailedDeposit</code> - unlock funds if the deposit was initiated but then failed on L2.</li>
<li><code>finalizeWithdrawal</code> - unlock funds for the valid withdrawal request from L2.</li>
</ul>
<p>The owner of the L1ERC20Bridge is the Governance contract.</p>
<h3 id="l1sharedbridge"><a class="header" href="#l1sharedbridge">L1SharedBridge</a></h3>
<p>The main bridge implementation handles transfers Ether, ERC20 tokens and of WETH tokens between the two domains. It is
designed to streamline and enhance the user experience for bridging WETH tokens by minimizing the number of transactions
required and reducing liquidity fragmentation thus improving efficiency and user experience.</p>
<p>This contract accepts WETH deposits on L1, unwraps them to ETH, and sends the ETH to the L2 WETH bridge contract, where
it is wrapped back into WETH and delivered to the L2 recipient.</p>
<p>Thus, the deposit is made in one transaction, and the user receives L2 WETH that can be unwrapped to ETH.</p>
<p>For withdrawals, the contract receives ETH from the L2 WETH bridge contract, wraps it into WETH, and sends the WETH to
the L1 recipient.</p>
<p>The owner of the L1WethBridge contract is the Governance contract.</p>
<h3 id="l2sharedbridge"><a class="header" href="#l2sharedbridge">L2SharedBridge</a></h3>
<p>The L2 counterpart of the L1 Shared bridge.</p>
<ul>
<li><code>withdraw</code> - initiate a withdrawal by burning funds on the contract and sending a corresponding message to L1.</li>
<li><code>finalizeDeposit</code> - finalize the deposit and mint funds on L2. The function is only callable by L1 bridge.</li>
</ul>
<p>The owner of the L2SharedBridge and the contracts related to it is the Governance contract.</p>
<h2 id="governance"><a class="header" href="#governance">Governance</a></h2>
<p>This contract manages calls for all governed zkEVM contracts on L1 and L2. Mostly, it is used for upgradability an
changing critical system parameters. The contract has minimum delay settings for the call execution.</p>
<p>Each upgrade consists of two steps:</p>
<ul>
<li>Scheduling - The owner can schedule upgrades in two different manners:
<ul>
<li>Fully transparent data. All the targets, calldata, and upgrade conditions are known to the community before upgrade
execution.</li>
<li>Shadow upgrade. The owner only shows the commitment to the upgrade. This upgrade type is mostly useful for fixing
critical issues in the production environment.</li>
</ul>
</li>
<li>Upgrade execution - the Owner or Security council can perform the upgrade with previously scheduled parameters.
<ul>
<li>Upgrade with delay. Scheduled operations should elapse the delay period. Both the owner and Security Council can
execute this type of upgrade.</li>
<li>Instant upgrade. Scheduled operations can be executed at any moment. Only the Security Council can perform this type
of upgrade.</li>
</ul>
</li>
</ul>
<p>Please note, that both the Owner and Security council can cancel the upgrade before its execution.</p>
<p>The diagram below outlines the complete journey from the initiation of an operation to its execution.</p>
<p><img src="specs/./img/governance.jpg" alt="governance.png" /></p>
<h2 id="validatortimelock"><a class="header" href="#validatortimelock">ValidatorTimelock</a></h2>
<p>An intermediate smart contract between the validator EOA account and the ZKsync smart contract. Its primary purpose is
to provide a trustless means of delaying batch execution without modifying the main ZKsync contract. ZKsync actively
monitors the chain activity and reacts to any suspicious activity by freezing the chain. This allows time for
investigation and mitigation before resuming normal operations.</p>
<p>It is a temporary solution to prevent any significant impact of the validator hot key leakage, while the network is in
the Alpha stage.</p>
<p>This contract consists of four main functions <code>commitBatches</code>, <code>proveBatches</code>, <code>executeBatches</code>, and <code>revertBatches</code>,
which can be called only by the validator.</p>
<p>When the validator calls <code>commitBatches</code>, the same calldata will be propagated to the ZKsync contract (<code>DiamondProxy</code>
through <code>call</code> where it invokes the <code>ExecutorFacet</code> through <code>delegatecall</code>), and also a timestamp is assigned to these
batches to track the time these batches are committed by the validator to enforce a delay between committing and
execution of batches. Then, the validator can prove the already committed batches regardless of the mentioned timestamp,
and again the same calldata (related to the <code>proveBatches</code> function) will be propagated to the ZKsync contract. After
the <code>delay</code> is elapsed, the validator is allowed to call <code>executeBatches</code> to propagate the same calldata to ZKsync
contract.</p>
<p>The owner of the ValidatorTimelock contract is the same as the owner of the Governance contract - Matter Labs multisig.</p>
<h2 id="allowlist"><a class="header" href="#allowlist">Allowlist</a></h2>
<p>The auxiliary contract controls the permission access list. It is used in bridges and diamond proxies to control which
addresses can interact with them in the Alpha release. Currently, it is supposed to set all permissions to public.</p>
<p>The owner of the Allowlist contract is the Governance contract.</p>
<h2 id="deposit-limitation"><a class="header" href="#deposit-limitation">Deposit Limitation</a></h2>
<p>The amount of deposit can be limited. This limitation is applied on an account level and is not time-based. In other
words, each account cannot deposit more than the cap defined. The tokens and the cap can be set through governance
transactions. Moreover, there is an allow listing mechanism as well (only some allow listed accounts can call some
specific functions). So, the combination of deposit limitation and allow listing leads to limiting the deposit of the
allow listed account to be less than the defined cap.</p>
<pre><code class="language-solidity">struct Deposit {
  bool depositLimitation;
  uint256 depositCap;
}

</code></pre>
<p>Currently, the limit is used only for blocking deposits of the specific token (turning on the limitation and setting the
limit to zero). And on the near future, this functionality will be completely removed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-3"><a class="header" href="#overview-3">Overview</a></h1>
<p>To support being a rollup, the ZK Stack needs to post the data of the chain on L1. Instead of submitting the data of
each transaction, we submit how the state of the blockchain changes, this change is called the state diff. This approach
allows the transactions that change the same storage slots to be very cheap, since these transactions don’t incur
additional data costs.</p>
<p>Besides the state diff we also <a href="specs/data_availability/./pubdata.html">post additional</a> data to L1, such as the L2-&gt;L1 messages, the L2-&gt;L1 logs,
the bytecodes of the deployed smart contracts.</p>
<p>We also <a href="specs/data_availability/./compression.html">compress</a> all the data that we send to L1, to reduce the costs of posting it.</p>
<p>By posting all the data to L1, we can <a href="specs/data_availability/./reconstruction.html">reconstruct</a> the state of the chain from the data on L1.
This is a key security property of the rollup.</p>
<p>The the chain chooses not to post this data, they become a validium. This makes transactions there much cheaper, but
less secure. Because we use state diffs to post data, we can combine the rollup and validium features, by separating
storage slots that need to post data from the ones that don’t. This construction combines the benefits of rollups and
validiums, and it is called a <a href="specs/data_availability/./validium_zk_porter.html">zkPorter</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-pubdata-in-boojum"><a class="header" href="#handling-pubdata-in-boojum">Handling pubdata in Boojum</a></h1>
<p>Pubdata in ZKsync can be divided up into 4 different categories:</p>
<ol>
<li>L2 to L1 Logs</li>
<li>L2 to L1 Messages</li>
<li>Smart Contract Bytecodes</li>
<li>Storage writes</li>
</ol>
<p>Using data corresponding to these 4 facets, across all executed batches, we’re able to reconstruct the full state of L2.
With the upgrade to our new proof system, Boojum, the way this data is represented will change. At a high level, in the
pre-Boojum system these are represented as separate fields while for boojum they will be packed into a single bytes
array. Once 4844 gets integrated this bytes array will move from being part of the calldata to blob data.</p>
<p>While the structure of the pubdata changes, the way in which one can go about pulling the information will remain the
same. Basically, we just need to filter all of the transactions to the L1 ZKsync contract for only the <code>commitBatches</code>
transactions where the proposed block has been referenced by a corresponding <code>executeBatches</code> call (the reason for this
is that a committed or even proven block can be reverted but an executed one cannot). Once we have all the committed
batches that have been executed, we then will pull the transaction input and the relevant fields, applying them in order
to reconstruct the current state of L2.</p>
<h2 id="l2l1-communication"><a class="header" href="#l2l1-communication">L2→L1 communication</a></h2>
<h3 id="l2l1-communication-before-boojum"><a class="header" href="#l2l1-communication-before-boojum">L2→L1 communication before Boojum</a></h3>
<p>While there were quite some changes during Boojum upgrade, most of the scheme remains the same and so explaining how it
worked before gives some background on why certain decisions are made and kept for backward compatibility.</p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum/L2%E2%86%92L1%20communication%20before%20Boojum.md">L2→L1 communication before Boojum</a></p>
<p>The most important feature that we’ll need to maintain in Boojum for backward compatibility is to provide a similar
Merkle tree of L2→L1 logs with the long L2→L1 messages and priority operations’ status.</p>
<p>Before Boojum, whenever we sent an L2→L1 long message, a <em>log</em> was appended to the Merkle tree of L2→L1 messages on L1
due to necessity. In Boojum we’ll have to maintain this fact. Having the priority operations’ statuses is important to
enable
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/bridge/L1ERC20Bridge.sol#L255">proving</a>
failed deposits for bridges.</p>
<h3 id="changes-with-boojum"><a class="header" href="#changes-with-boojum">Changes with Boojum</a></h3>
<h4 id="problems-with-the-previous-approach"><a class="header" href="#problems-with-the-previous-approach">Problems with the previous approach</a></h4>
<ul>
<li>There was a limit of 512 L2→L1 logs per batch, which is very limiting. It causes our block to be forcefully closed
based on the number of these messages instead of having the pubdata as the only limit.</li>
<li>In the ideal world, we would like to have the tree adapt to the requirements of the batch, with any number of leaves
possible (in practice, a maximum of 2048 would likely be enough for the foreseeable future).</li>
<li>Extending the tree in the circuits will be hard to do and hard to maintain.</li>
<li>The hash of the contents of the L2→L1 messages needs to be rehashed to support the danksharding blobs, so we want to
keep only the essential logs as parts of calldata and the rest should be separated so that they could be moved the
EIP4844 blob in the future.</li>
</ul>
<h4 id="solution-1"><a class="header" href="#solution-1">Solution</a></h4>
<p>We will implement the calculation of the Merkle root of the L2→L1 messages via a system contract as part of the
<code>L1Messenger</code>. Basically, whenever a new log emitted by users that needs to be Merklized is created, the <code>L1Messenger</code>
contract will append it to its rolling hash and then at the end of the batch, during the formation of the blob it will
receive the original preimages from the operator, verify, and include the logs to the blob.</p>
<p>We will now call the logs that are created by users and are Merklized <em>user</em> logs and the logs that are emitted by
natively by VM <em>system</em> logs. Here is a short comparison table for better understanding:</p>
<div class="table-wrapper"><table><thead><tr><th>System logs</th><th>User logs</th></tr></thead><tbody>
<tr><td>Emitted by VM via an opcode.</td><td>VM knows nothing about them.</td></tr>
<tr><td>Consistency and correctness is enforced by the verifier on L1 (i.e. their hash is part of the block commitment.</td><td>Consistency and correctness is enforced by the L1Messenger system contract. The correctness of the behavior of the L1Messenger is enforced implicitly by prover in a sense that it proves the correctness of the execution overall.</td></tr>
<tr><td>We don’t calculate their Merkle root.</td><td>We calculate their Merkle root on the L1Messenger system contract.</td></tr>
<tr><td>We have constant small number of those.</td><td>We can have as much as possible as long as the commitBatches function on L1 remains executable (it is the job of the operator to ensure that only such transactions are selected)</td></tr>
<tr><td>In EIP4844 they will remain part of the calldata.</td><td>In EIP4844 they will become part of the blobs.</td></tr>
</tbody></table>
</div>
<h4 id="backwards-compatibility"><a class="header" href="#backwards-compatibility">Backwards-compatibility</a></h4>
<p>Note, that to maintain a unified interface with the previous version of the protocol, the leaves of the Merkle tree will
have to maintain the following structure:</p>
<pre><code class="language-solidity">struct L2Log {
  uint8 l2ShardId;
  bool isService;
  uint16 txNumberInBlock;
  address sender;
  bytes32 key;
  bytes32 value;
}

</code></pre>
<p>While the leaf will look the following way:</p>
<pre><code class="language-solidity">bytes32 hashedLog = keccak256(
    abi.encodePacked(_log.l2ShardId, _log.isService, _log.txNumberInBlock, _log.sender, _log.key, _log.value)
);
</code></pre>
<p><code>keccak256</code> will continue being the function for the merkle tree.</p>
<p>To put it shortly, the proofs for L2→L1 log inclusion will continue having exactly the same format as they did in the
pre-Boojum system, which avoids breaking changes for SDKs and bridges alike.</p>
<h4 id="implementation-of-l1messenger"><a class="header" href="#implementation-of-l1messenger">Implementation of <code>L1Messenger</code></a></h4>
<p>The L1Messenger contract will maintain a rolling hash of all the L2ToL1 logs <code>chainedLogsHash</code> as well as the rolling
hashes of messages <code>chainedMessagesHash</code>. Whenever a contract wants to send an L2→L1 log, the following operation will
be
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/L1Messenger.sol#L110">applied</a>:</p>
<p><code>chainedLogsHash = keccak256(chainedLogsHash, hashedLog)</code>. L2→L1 logs have the same 88-byte format as in the current
version of ZKsync.</p>
<p>Note, that the user is charged for necessary future the computation that will be needed to calculate the final merkle
root. It is roughly 4x higher than the cost to calculate the hash of the leaf, since the eventual tree might have be 4x
times the number nodes. In any case, this will likely be a relatively negligible part compared to the cost of the
pubdata.</p>
<p>At the end of the execution, the bootloader will
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2470">provide</a>
a list of all the L2ToL1 logs as well as the messages in this block to the L1Messenger (this will be provided by the
operator in the memory of the bootloader). The L1Messenger checks that the rolling hash from the provided logs is the
same as in the <code>chainedLogsHash</code> and calculate the merkle tree of the provided messages. Right now, we always build the
Merkle tree of size <code>2048</code>, but we charge the user as if the tree was built dynamically based on the number of leaves in
there. The implementation of the dynamic tree has been postponed until the later upgrades.</p>
<h4 id="long-l2l1-messages--bytecodes"><a class="header" href="#long-l2l1-messages--bytecodes">Long L2→L1 messages &amp; bytecodes</a></h4>
<p>Before, the fact that the correct preimages for L2→L1 messages as bytecodes were provided was checked on the L1 side.
Now, it will be done on L2.</p>
<p>If the user wants to send an L2→L1 message, its preimage is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/L1Messenger.sol#L125">appended</a>
to the message’s rolling hash too <code>chainedMessagesHash = keccak256(chainedMessagesHash, keccak256(message))</code>.</p>
<p>A very similar approach for bytecodes is used, where their rolling hash is calculated and then the preimages are
provided at the end of the batch to form the full pubdata for the batch.</p>
<p>Note, that in for backward compatibility, just like before any long message or bytecode is accompanied by the
corresponding user L2→L1 log.</p>
<h4 id="using-system-l2l1-logs-vs-the-user-logs"><a class="header" href="#using-system-l2l1-logs-vs-the-user-logs">Using system L2→L1 logs vs the user logs</a></h4>
<p>The content of the L2→L1 logs by the L1Messenger will go to the blob of EIP4844. Meaning, that all the data that belongs
to the tree by L1Messenger’s L2→L1 logs should not be needed during block commitment. Also, note that in the future we
will remove the calculation of the Merkle root of the built-in L2→L1 messages.</p>
<p>The only places where the built-in L2→L1 messaging should continue to be used:</p>
<ul>
<li>Logs by SystemContext (they are needed on commit to check the previous block hash).</li>
<li>Logs by L1Messenger for the merkle root of the L2→L1 tree as well as the hash of the <code>totalPubdata</code>.</li>
<li><code>chainedPriorityTxsHash</code> and <code>numberOfLayer1Txs</code> from the bootloader (read more about it below).</li>
</ul>
<h4 id="obtaining-txnumberinblock"><a class="header" href="#obtaining-txnumberinblock">Obtaining <code>txNumberInBlock</code></a></h4>
<p>To have the same log format, the <code>txNumberInBlock</code> must be obtained. While it is internally counted in the VM, there is
currently no opcode to retrieve this number. We will have a public variable <code>txNumberInBlock</code> in the <code>SystemContext</code>,
which will be incremented with each new transaction and retrieve this variable from there. It is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L458">zeroed out</a>
at the end of the batch.</p>
<h3 id="bootloader-implementation"><a class="header" href="#bootloader-implementation">Bootloader implementation</a></h3>
<p>The bootloader has a memory segment dedicated to the ABI-encoded data of the L1ToL2Messenger to perform the
<code>publishPubdataAndClearState</code> call.</p>
<p>At the end of the execution of the batch, the operator should provide the corresponding data into the bootloader memory,
i.e user L2→L1 logs, long messages, bytecodes, etc. After that, the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2484">call</a>
is performed to the <code>L1Messenger</code> system contract, that should validate the adherence of the pubdata to the required
format</p>
<h2 id="bytecode-publishing"><a class="header" href="#bytecode-publishing">Bytecode Publishing</a></h2>
<p>Within pubdata, bytecodes are published in 1 of 2 ways: (1) uncompressed via <code>factoryDeps</code> (pre-boojum this is within
its own field, and post-boojum as part of the <code>totalPubdata</code>) and (2) compressed via long l2 → l1 messages.</p>
<h3 id="uncompressed-bytecode-publishing"><a class="header" href="#uncompressed-bytecode-publishing">Uncompressed Bytecode Publishing</a></h3>
<p>With Boojum, <code>factoryDeps</code> are included within the <code>totalPubdata</code> bytes and have the following format:
<code>number of bytecodes || forEachBytecode (length of bytecode(n) || bytecode(n))</code> .</p>
<h3 id="compressed-bytecode-publishing"><a class="header" href="#compressed-bytecode-publishing">Compressed Bytecode Publishing</a></h3>
<p>This part stays the same in a pre and post boojum ZKsync. Unlike uncompressed bytecode which are published as part of
<code>factoryDeps</code>, compressed bytecodes are published as long l2 → l1 messages which can be seen
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/Compressor.sol#L80">here</a>.</p>
<h4 id="bytecode-compression-algorithm--server-side"><a class="header" href="#bytecode-compression-algorithm--server-side">Bytecode Compression Algorithm — Server Side</a></h4>
<p>This is the part that is responsible for taking bytecode, that has already been chunked into 8 byte words, performing
validation, and compressing it.</p>
<p>Each 8 byte word from the chunked bytecode is assigned a 2 byte index (constraint on size of dictionary of chunk → index
is 2^16 - 1 elements). The length of the dictionary, dictionary entries (index assumed through order), and indexes are
all concatenated together to yield the final compressed version.</p>
<p>For bytecode to be considered valid it must satisfy the following:</p>
<ol>
<li>Bytecode length must be less than 2097120 ((2^16 - 1) * 32) bytes.</li>
<li>Bytecode length must be a multiple of 32.</li>
<li>Number of 32-byte words cannot be even.</li>
</ol>
<p>The following is a simplified version of the algorithm:</p>
<pre><code class="language-python">statistic: Map[chunk, (count, first_pos)]
dictionary: Map[chunk, index]
encoded_data: List[index]

for position, chunk in chunked_bytecode:
 if chunk is in statistic:
  statistic[chunk].count += 1
 else:
  statistic[chunk] = (count=1, first_pos=pos)

# We want the more frequently used bytes to have smaller ids to save on calldata (zero bytes cost less)
statistic.sort(primary=count, secondary=first_pos, order=desc)

for index, chunk in enumerated(sorted_statistics):
  dictionary[chunk] = index

for chunk in chunked_bytecode:
 encoded_data.append(dictionary[chunk])

return [len(dictionary), dictionary.keys(order=index asc), encoded_data]
</code></pre>
<h4 id="verification-and-publishing--l2-contract"><a class="header" href="#verification-and-publishing--l2-contract">Verification And Publishing — L2 Contract</a></h4>
<p>The function <code>publishCompressBytecode</code> takes in both the original <code>_bytecode</code> and the <code>_rawCompressedData</code> , the latter
of which comes from the output of the server’s compression algorithm. Looping over the encoded data, derived from
<code>_rawCompressedData</code> , the corresponding chunks are pulled from the dictionary and compared to the original byte code,
reverting if there is a mismatch. After the encoded data has been verified, it is published to L1 and marked accordingly
within the <code>KnownCodesStorage</code> contract.</p>
<p>Pseudo-code implementation:</p>
<pre><code class="language-python">length_of_dict = _rawCompressedData[:2]
dictionary = _rawCompressedData[2:2 + length_of_dict * 8] # need to offset by bytes used to store length (2) and multiply by 8 for chunk size
encoded_data = _rawCompressedData[2 + length_of_dict * 8:]

assert(len(dictionary) % 8 == 0) # each element should be 8 bytes
assert(num_entries(dictionary) &lt;= 2^16)
assert(len(encoded_data) * 4 == len(_bytecode)) # given that each chunk is 8 bytes and each index is 2 bytes they should differ by a factor of 4

for (index, dict_index) in list(enumerate(encoded_data)):
 encoded_chunk = dictionary[dict_index]
 real_chunk = _bytecode.readUint64(index * 8) # need to pull from index * 8 to account for difference in element size
 verify(encoded_chunk == real_chunk)

# Sending the compressed bytecode to L1 for data availability
sendToL1(_rawCompressedBytecode)
markAsPublished(hash(_bytecode))
</code></pre>
<h2 id="storage-diff-publishing"><a class="header" href="#storage-diff-publishing">Storage diff publishing</a></h2>
<p>ZKsync is a statediff-based rollup and so publishing the correct state diffs plays an integral role in ensuring data
availability.</p>
<h3 id="how-publishing-of-storage-diffs-worked-before-boojum"><a class="header" href="#how-publishing-of-storage-diffs-worked-before-boojum">How publishing of storage diffs worked before Boojum</a></h3>
<p>As always in order to understand the new system better, some information about the previous one is important.</p>
<p>Before, the system contracts had no clue about storage diffs. It was the job of the operator to provide the
<code>initialStorageChanges</code> and <code>reapeatedStorageWrites</code> (more on the differences will be explained below). The information
to commit the block looked the following way:</p>
<pre><code class="language-solidity">struct CommitBlockInfo {
  uint64 blockNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 l2LogsTreeRoot;
  bytes32 priorityOperationsHash;
  bytes initialStorageChanges;
  bytes repeatedStorageChanges;
  bytes l2Logs;
  bytes[] l2ArbitraryLengthMessages;
  bytes[] factoryDeps;
}

</code></pre>
<p>These two fields would be then included into the block commitment and checked by the verifier.</p>
<h3 id="difference-between-initial-and-repeated-writes"><a class="header" href="#difference-between-initial-and-repeated-writes">Difference between initial and repeated writes</a></h3>
<p>ZKsync publishes state changes that happened within the batch instead of transactions themselves. Meaning, that for
instance some storage slot <code>S</code> under account <code>A</code> has changed to value <code>V</code>, we could publish a triple of <code>A,S,V</code>. Users
by observing all the triples could restore the state of ZKsync. However, note that our tree unlike Ethereum’s one is not
account based (i.e. there is no first layer of depth 160 of the merkle tree corresponding to accounts and second layer
of depth 256 of the merkle tree corresponding to users). Our tree is “flat”, i.e. a slot <code>S</code> under account <code>A</code> is just
stored in the leaf number <code>H(S,A)</code>. Our tree is of depth 256 + 8 (the 256 is for these hashed account/key pairs and 8 is
for potential shards in the future, we currently have only one shard and it is irrelevant for the rest of the document).</p>
<p>We call this <code>H(S,A)</code> <em>derived key</em>, because it is derived from the address and the actual key in the storage of the
account. Since our tree is flat, whenever a change happens, we can publish a pair <code>DK, V</code>, where <code>DK=H(S,A)</code>.</p>
<p>However, these is an optimization that could be done:</p>
<ul>
<li>Whenever a change to a key is used for the first time, we publish a pair of <code>DK,V</code> and we assign some sequential id to
this derived key. This is called an <em>initial write</em>. It happens for the first time and that’s why we must publish the
full key.</li>
<li>If this storage slot is published in some of the subsequent batches, instead of publishing the whole <code>DK</code>, we can use
the sequential id instead. This is called a <em>repeated write</em>.</li>
</ul>
<p>For instance, if the slots <code>A</code>,<code>B</code> (I’ll use latin letters instead of 32-byte hashes for readability) changed their
values to <code>12</code>,<code>13</code> accordingly, in the batch it happened they will be published in the following format:</p>
<ul>
<li><code>(A, 12), (B, 13)</code>. Let’s say that the last sequential id ever used is 6. Then, <code>A</code> will receive the id of <code>7</code> and B
will receive the id of <code>8</code>.</li>
</ul>
<p>Let’s say that in the next block, they changes their values to <code>13</code>,<code>14</code>. Then, their diff will be published in the
following format:</p>
<ul>
<li><code>(7, 13), (8,14)</code>.</li>
</ul>
<p>The id is permanently assigned to each storage key that was ever published. While in the description above it may not
seem like a huge boost, however, each <code>DK</code> is 32 bytes long and id is at most 8 bytes long.</p>
<p>We call this id <em>enumeration_index</em>.</p>
<p>Note, that the enumeration indexes are assigned in the order of sorted array of (address, key), i.e. they are internally
sorted. The enumeration indexes are part of the state merkle tree, it is <strong>crucial</strong> that the initial writes are
published in the correct order, so that anyone could restore the correct enum indexes for the storage slots. In
addition, an enumeration index of <code>0</code> indicates that the storage write is an initial write.</p>
<h3 id="state-diffs-after-boojum-upgrade"><a class="header" href="#state-diffs-after-boojum-upgrade">State diffs after Boojum upgrade</a></h3>
<p>Firstly, let’s define what we’ll call the <code>stateDiffs</code>. A <em>state diff</em> is an element of the following structure.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/encodings/state_diff_record.rs#L8">https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/encodings/state_diff_record.rs#L8</a>.</p>
<p>Basically, it contains all the values which might interest us about the state diff:</p>
<ul>
<li><code>address</code> where the storage has been changed.</li>
<li><code>key</code> (the original key inside the address)</li>
<li><code>derived_key</code> — <code>H(key, address)</code> as described in the previous section.
<ul>
<li>Note, the hashing algorithm currently used here is <code>Blake2s</code></li>
</ul>
</li>
<li><code>enumeration_index</code> — Enumeration index as explained above. It is equal to 0 if the write is initial and contains the
non-zero enumeration index if it is the repeated write (indexes are numerated starting from 1).</li>
<li><code>initial_value</code> — The value that was present in the key at the start of the batch</li>
<li><code>final_value</code> — The value that the key has changed to by the end of the batch.</li>
</ul>
<p>We will consider <code>stateDiffs</code> an array of such objects, sorted by (address, key).</p>
<p>This is the internal structure that is used by the circuits to represent the state diffs. The most basic “compression”
algorithm is the one described above:</p>
<ul>
<li>For initial writes, write the pair of (<code>derived_key</code>, <code>final_value</code>)</li>
<li>For repeated writes write the pair of (<code>enumeration_index</code>, <code>final_value</code>).</li>
</ul>
<p>Note, that values like <code>initial_value</code>, <code>address</code> and <code>key</code> are not used in the “simplified” algorithm above, but they
will be helpful for the more advanced compression algorithms in the future. The
<a href="specs/data_availability/pubdata.html#state-diff-compression-format">algorithm</a> for Boojum will already utilize the difference between the <code>initial_value</code>
and <code>final_value</code> for saving up on pubdata.</p>
<h3 id="how-the-new-pubdata-verification-would-work"><a class="header" href="#how-the-new-pubdata-verification-would-work">How the new pubdata verification would work</a></h3>
<h4 id="l2"><a class="header" href="#l2">L2</a></h4>
<ol>
<li>The operator provides both full <code>stateDiffs</code> (i.e. the array of the structs above) and the compressed state diffs
(i.e. the array which contains the state diffs, compressed by the algorithm explained
<a href="specs/data_availability/pubdata.html#state-diff-compression-format">below</a>).</li>
<li>The L1Messenger must verify that the compressed version is consistent with the original stateDiffs.</li>
<li>Once verified, the L1Messenger will publish the <em>hash</em> of the original state diff via a system log. It will also
include the compressed state diffs into the totalPubdata to be published onto L1.</li>
</ol>
<h4 id="l1"><a class="header" href="#l1">L1</a></h4>
<ol>
<li>During committing the block, the L1
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L139">verifies</a>
that the operator has provided the full preimage for the totalPubdata (which includes L2→L1 logs, L2→L1 messages,
bytecodes as well as the compressed state diffs).</li>
<li>The block commitment
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L462">includes</a>
*the hash of the <code>stateDiffs</code>. Thus, during ZKP verification will fail if the provided stateDiff hash is not
correct.</li>
</ol>
<p>It is a secure construction because the proof can be verified only if both the execution was correct and the hash of the
provided hash of the <code>stateDiffs</code> is correct. This means that the L1Messenger indeed received the array of correct
<code>stateDiffs</code> and, assuming the L1Messenger is working correctly, double-checked that the compression is of the correct
format, while L1 contracts on the commit stage double checked that the operator provided the preimage for the compressed
state diffs.</p>
<h3 id="state-diff-compression-format"><a class="header" href="#state-diff-compression-format">State diff compression format</a></h3>
<p>The following algorithm is used for the state diff compression:</p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum/State%20diff%20compression%20v1%20spec.md">State diff compression v1 spec</a></p>
<h2 id="general-pubdata-format"><a class="header" href="#general-pubdata-format">General pubdata format</a></h2>
<p>At the end of the execution of the batch, the bootloader provides the <code>L1Messenger</code> with the preimages for the user
L2→L1 logs, L2→L1 long messages as well as uncompressed bytecodes. It also provides with compressed state diffs as well
as the original expanded state diff entries.</p>
<p>It will check that the preimages are correct as well as the fact that the compression is correct. It will output the
following three values via system logs:</p>
<ul>
<li>The root of the L2→L1 log Merkle tree. It will be stored and used for proving withdrawals.</li>
<li>The hash of the <code>totalPubdata</code> (i.e. the pubdata that contains the preimages above as well as packed state diffs).</li>
<li>The hash of the state diffs provided by the operator (it later on be included in the block commitment and its will be
enforced by the circuits).</li>
</ul>
<p>The <code>totalPubdata</code> has the following structure:</p>
<ol>
<li>First 4 bytes — the number of user L2→L1 logs in the batch</li>
<li>Then, the concatenation of packed L2→L1 user logs.</li>
<li>Next, 4 bytes — the number of long L2→L1 messages in the batch.</li>
<li>Then, the concatenation of L2→L1 messages, each in the format of <code>&lt;4 byte length || actual_message&gt;</code>.</li>
<li>Next, 4 bytes — the number of uncompressed bytecodes in the batch.</li>
<li>Then, the concatenation of uncompressed bytecodes, each in the format of <code>&lt;4 byte length || actual_bytecode&gt;</code>.</li>
<li>Next, 4 bytes — the length of the compressed state diffs.</li>
<li>Then, state diffs are compressed by the spec <a href="specs/data_availability/pubdata.html#state-diff-compression-format">above</a>.</li>
</ol>
<p>With Boojum, the interface for committing batches is the following one:</p>
<pre><code class="language-solidity">/// @notice Data needed to commit new batch
/// @param batchNumber Number of the committed batch
/// @param timestamp Unix timestamp denoting the start of the batch execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param priorityOperationsHash Hash of all priority operations from this batch
/// @param bootloaderHeapInitialContentsHash Hash of the initial contents of the bootloader heap. In practice it serves as the commitment to the transactions in the batch.
/// @param eventsQueueStateHash Hash of the events queue state. In practice it serves as the commitment to the events in the batch.
/// @param systemLogs concatenation of all L2 -&gt; L1 system logs in the batch
/// @param totalL2ToL1Pubdata Total pubdata committed to as part of bootloader run. Contents are: l2Tol1Logs &lt;&gt; l2Tol1Messages &lt;&gt; publishedBytecodes &lt;&gt; stateDiffs
struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes totalL2ToL1Pubdata;
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="state-diff-compression"><a class="header" href="#state-diff-compression">State diff Compression</a></h1>
<p>The most basic strategy to publish state diffs is to publish those in either of the following two forms:</p>
<ul>
<li>When a key is updated for the first time — <code>&lt;key, value&gt;</code>, where key is 32-byte derived key and the value is new
32-byte value of the slot.</li>
<li>When a key is updated for the second time and more — <code>&lt;enumeration_index, value&gt;</code>, where the <code>enumeration_index</code> is an
8-byte id of the slot and the value is the new 32-byte value of the slot.</li>
</ul>
<p>This compression strategy will utilize a similar idea for treating keys and values separately and it will be focused on
the efficient compression of keys and values separately.</p>
<h2 id="keys-2"><a class="header" href="#keys-2">Keys</a></h2>
<p>Keys will be packed in the same way as they were before. The only change is that we’ll avoid using the 8-byte
enumeration index and will pack it to the minimal necessary number of bytes. This number will be part of the pubdata.
Once a key has been used, it can already use the 4 or 5 byte enumeration index and it is very hard to have something
cheaper for keys that has been used already. The opportunity comes when remembering the ids for accounts to spare some
bytes on nonce/balance key, but ultimately the complexity may not be worth it.</p>
<p>There is some room for optimization of the keys that are being written for the first time, however, optimizing those is
more complex and achieves only a one-time effect (when the key is published for the first time), so they may be in scope
of the future upgrades.</p>
<h2 id="values-1"><a class="header" href="#values-1">Values</a></h2>
<p>Values are much easier to compress since they usually contain only zeroes. Also, we can leverage the nature of how those
values are changed. For instance, if nonce has been increased only by 1, we do not need to write the entire 32-byte new
value, we can just tell that the slot has been <em>increased</em> and then supply only the 1-byte value by which it was
increased. This way instead of 32 bytes we need to publish only 2 bytes: first byte to denote which operation has been
applied and the second by to denote the number by which the addition has been made.</p>
<p>We have the following 4 types of changes: <code>Add</code>, <code>Sub,</code> <code>Transform</code>, <code>NoCompression</code> where:</p>
<ul>
<li><code>NoCompression</code> denotes that the whole 32 byte will be provided.</li>
<li><code>Add</code> denotes that the value has been increased. (modulo 2^256)</li>
<li><code>Sub</code> denotes that the value has been decreased. (modulo 2^256)</li>
<li><code>Transform</code> denotes the value just has been changed (i.e. we disregard any potential relation between the previous and
the new value, though the new value might be small enough to save up on the number of bytes).</li>
</ul>
<p>Where the byte size of the output can be anywhere from 0 to 31 (also 0 makes sense for <code>Transform</code>, since it denotes
that it has been zeroed out). For <code>NoCompression</code> the whole 32 byte value is used.</p>
<p>So the format of the pubdata is the following:</p>
<p><strong>Part 1. Header.</strong></p>
<ul>
<li><code>&lt;version = 1 byte&gt;</code> — this will enable easier automated unpacking in the future. Currently, it will be only equal to
<code>1</code>.</li>
<li><code>&lt;total_logs_len = 3 bytes&gt;</code> — we need only 3 bytes to describe the total length of the L2→L1 logs.</li>
<li><code>&lt;the number of bytes used for derived keys = 1 byte&gt;</code>. It should be equal to the minimal required bytes to represent
the enum indexes for repeated writes.</li>
</ul>
<p><strong>Part 2. Initial writes.</strong></p>
<ul>
<li><code>&lt;num_of_initial_writes = 2 bytes&gt;</code> - the number of initial writes. Since each initial write publishes at least 32
bytes for key, then <code>2^16 * 32 = 2097152</code> will be enough for a lot of time (right now with the limit of 120kb it will
take more than 15 L1 txs to use up all the space there).</li>
<li>Then for each <code>&lt;key, value&gt;</code> pair for each initial write:
<ul>
<li>print key as 32-byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<p><strong>Part 3. Repeated writes.</strong></p>
<p>Note, that there is no need to write the number of repeated writes, since we know that until the end of the pubdata, all
the writes will be repeated ones.</p>
<ul>
<li>For each <code>&lt;key, value&gt;</code> pair for each repeated write:
<ul>
<li>print key as derived key by using the number of bytes provided in the header.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<h2 id="impact"><a class="header" href="#impact">Impact</a></h2>
<p>This setup allows us to achieve nearly 75% packing for values, and 50% gains overall in terms of the storage logs based
on historical data.</p>
<h2 id="encoding-of-packing-type"><a class="header" href="#encoding-of-packing-type">Encoding of packing type</a></h2>
<p>Since we have <code>32 * 3 + 1</code> ways to pack a state diff, we need at least 7 bits to present the packing type. To make
parsing easier, we will use 8 bits, i.e. 1 byte.</p>
<p>We will use the first 5 bits to represent the length of the bytes (from 0 to 31 inclusive) to be used. The other 3 bits
will be used to represent the type of the packing: <code>Add</code>, <code>Sub</code> , <code>Transform</code>, <code>NoCompression</code>.</p>
<h2 id="worst-case-scenario"><a class="header" href="#worst-case-scenario">Worst case scenario</a></h2>
<p>The worst case scenario for such packing is when we have to pack a completely random new value, i.e. it will take us 32
bytes to pack + 1 byte to denote which type it is. However, for such a write the user will anyway pay at least for 32
bytes. Adding an additional byte is roughly 3% increase, which will likely be barely felt by users, most of which use
storage slots for balances, etc, which will consume only 7-9 bytes for packed value.</p>
<h2 id="why-do-we-need-to-repeat-the-same-packing-method-id"><a class="header" href="#why-do-we-need-to-repeat-the-same-packing-method-id">Why do we need to repeat the same packing method id</a></h2>
<p>You might have noticed that for each pair <code>&lt;key, value&gt;</code> to describe value we always first write the packing type and
then write the packed value. However, the reader might ask, it is more efficient to just supply the packing id once and
then list all the pairs <code>&lt;key, value&gt;</code> which use such packing.</p>
<p>I.e. instead of listing</p>
<p>(key = 0, type = 1, value = 1), (key = 1, type = 1, value = 3), (key = 2, type = 1, value = 4), …</p>
<p>Just write:</p>
<p>type = 1, (key = 0, value = 1), (key = 1, value = 3), (key = 2, value = 4), …</p>
<p>There are two reasons for it:</p>
<ul>
<li>A minor reason: sometimes it is less efficient in case the packing is used for very few slots (since for correct
unpacking we need to provide the number of slots for each packing type).</li>
<li>A fundamental reason: currently enum indices are stored directly in the merkle tree &amp; have very strict order of
incrementing enforced by the circuits and (they are given in order by pairs <code>(address, key)</code>), which are generally not
accessible from pubdata.</li>
</ul>
<p>All this means that we are not allowed to change the order of “first writes” above, so indexes for them are directly
recoverable from their order, and so we can not permute them. If we were to reorder keys without supplying the new
enumeration indices for them, the state would be unrecoverable. Always supplying the new enum index may add additional 5
bytes for each key, which might negate the compression benefits in a lot of cases. Even if the compression will still be
beneficial, the added complexity may not be worth it.</p>
<p>That being said, we <em>could</em> rearrange those for <em>repeated</em> writes, but for now we stick to the same value compression
format for simplicity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l2-state-reconstruction-tool"><a class="header" href="#l2-state-reconstruction-tool">L2 State Reconstruction Tool</a></h1>
<p>Given that we post all data to L1, there is a tool, created by the <a href="https://equilibrium.co/">Equilibrium Team</a> that
solely uses L1 pubdata for reconstructing the state and verifying that the state root on L1 can be created using
pubdata. A link to the repo can be found <a href="https://github.com/eqlabs/zksync-state-reconstruct">here</a>. The way the tool
works is by parsing out all the L1 pubdata for an executed batch, comparing the state roots after each batch is
processed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validium-and-zkporter"><a class="header" href="#validium-and-zkporter">Validium and zkPorter</a></h1>
<p>The may choose not to post their data to L1, in which case they become a validium. This makes transactions there much
cheaper, but less secure. Because the ZK Stack uses state diffs to post data, it can combine the rollup and validium
features, by separating storage slots that need to post data from the ones that don’t. This construction combines the
benefits of rollups and validiums, and it is called a
<a href="https://blog.matter-labs.io/zkporter-composable-scalability-in-l2-beyond-zkrollup-2a30c4d69a75">zkPorter</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview---deposits-and-withdrawals"><a class="header" href="#overview---deposits-and-withdrawals">Overview - Deposits and Withdrawals</a></h1>
<p>The zkEVM supports general message passing for L1&lt;-&gt;L2 communication. Proofs are settled on L1, so core of this process
is the [L2-&gt;L1] message passing process. [L1-&gt;L2] messages are recorded on L1 inside a priority queue, the sequencer
picks it up from here and executes it in the zkEVM. The zkEVM sends an L2-&gt;L1 message of the L1 transactions that it
processed, and the rollup’s proof is only valid if the processed transactions were exactly right.</p>
<p>There is an asymmetry in the two directions however, in the L1-&gt;L2 direction we support starting message calls by having
a special transaction type called L1 transactions. In the L2-&gt;L1 direction we only support message passing.</p>
<p>In particular, deposits and withdrawals of ether also use the above methods. For deposits the L1-&gt;L2 transaction is sent
with empty calldata, the recipients address and the deposited value. When withdrawing, an L2-&gt;L1 message is sent. This
is then processed by the smart contract holding the ether on L1, which releases the funds.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-l1l2-ops"><a class="header" href="#handling-l1l2-ops">Handling L1→L2 ops</a></h1>
<p>The transactions on ZKsync can be initiated not only on L2, but also on L1. There are two types of transactions that can
be initiated on L1:</p>
<ul>
<li>Priority operations. These are the kind of operations that any user can create.</li>
<li>Upgrade transactions. These can be created only during upgrades.</li>
</ul>
<h3 id="prerequisites-4"><a class="header" href="#prerequisites-4">Prerequisites</a></h3>
<p>Please read the full
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">article</a>
on the general system contracts / bootloader structure as well as the pubdata structure with Boojum system to understand
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">the difference</a>
between system and user logs.</p>
<h2 id="priority-operations"><a class="header" href="#priority-operations">Priority operations</a></h2>
<h3 id="initiation"><a class="header" href="#initiation">Initiation</a></h3>
<p>A new priority operation can be appended by calling the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Mailbox.sol#L236">requestL2Transaction</a>
method on L1. This method will perform several checks for the transaction, making sure that it is processable and
provides enough fee to compensate the operator for this transaction. Then, this transaction will be
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Mailbox.sol#L369C1-L369C1">appended</a>
to the priority queue.</p>
<h3 id="bootloader"><a class="header" href="#bootloader">Bootloader</a></h3>
<p>Whenever an operator sees a priority operation, it can include the transaction into the batch. While for normal L2
transaction the account abstraction protocol will ensure that the <code>msg.sender</code> has indeed agreed to start a transaction
out of this name, for L1→L2 transactions there is no signature verification. In order to verify that the operator
includes only transactions that were indeed requested on L1, the bootloader
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L970">maintains</a>
two variables:</p>
<ul>
<li><code>numberOfPriorityTransactions</code> (maintained at <code>PRIORITY_TXS_L1_DATA_BEGIN_BYTE</code> of bootloader memory)</li>
<li><code>priorityOperationsRollingHash</code> (maintained at <code>PRIORITY_TXS_L1_DATA_BEGIN_BYTE + 32</code> of the bootloader memory)</li>
</ul>
<p>Whenever a priority transaction is processed, the <code>numberOfPriorityTransactions</code> gets incremented by 1, while
<code>priorityOperationsRollingHash</code> is assigned to <code>keccak256(priorityOperationsRollingHash, processedPriorityOpHash)</code>,
where <code>processedPriorityOpHash</code> is the hash of the priority operations that has been just processed.</p>
<p>Also, for each priority transaction, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L966">emit</a>
a user L2→L1 log with its hash and result, which basically means that it will get Merklized and users will be able to
prove on L1 that a certain priority transaction has succeeded or failed (which can be helpful to reclaim your funds from
bridges if the L2 part of the deposit has failed).</p>
<p>Then, at the end of the batch, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3819">submit</a>
and 2 L2→L1 log system log with these values.</p>
<h3 id="batch-commit"><a class="header" href="#batch-commit">Batch commit</a></h3>
<p>During block commit, the contract will remember those values, but not validate them in any way.</p>
<h3 id="batch-execution"><a class="header" href="#batch-execution">Batch execution</a></h3>
<p>During batch execution, we would pop <code>numberOfPriorityTransactions</code> from the top of priority queue and
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L282">verify</a>
that their rolling hash does indeed equal to <code>priorityOperationsRollingHash</code>.</p>
<h2 id="upgrade-transactions"><a class="header" href="#upgrade-transactions">Upgrade transactions</a></h2>
<h3 id="initiation-1"><a class="header" href="#initiation-1">Initiation</a></h3>
<p>Upgrade transactions can only be created during a system upgrade. It is done if the <code>DiamondProxy</code> delegatecalls to the
implementation that manually puts this transaction into the storage of the DiamondProxy. Note, that since it happens
during the upgrade, there is no “real” checks on the structure of this transaction. We do have
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/upgrades/BaseZkSyncUpgrade.sol#L175">some validation</a>,
but it is purely on the side of the implementation which the <code>DiamondProxy</code> delegatecalls to and so may be lifted if the
implementation is changed.</p>
<p>The hash of the currently required upgrade transaction is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L138">stored</a>
under <code>l2SystemContractsUpgradeTxHash</code>.</p>
<p>We will also track the batch where the upgrade has been committed in the <code>l2SystemContractsUpgradeBatchNumber</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L141">variable</a>.</p>
<p>We can not support multiple upgrades in parallel, i.e. the next upgrade should start only after the previous one has
been complete.</p>
<h3 id="bootloader-1"><a class="header" href="#bootloader-1">Bootloader</a></h3>
<p>The upgrade transactions are processed just like with priority transactions, with only the following differences:</p>
<ul>
<li>We can have only one upgrade transaction per batch &amp; this transaction must be the first transaction in the batch.</li>
<li>The system contracts upgrade transaction is not appended to <code>priorityOperationsRollingHash</code> and doesn’t increment
<code>numberOfPriorityTransactions</code>. Instead, its hash is calculated via a system L2→L1 log <em>before</em> it gets executed.
Note, that it is an important property. More on it <a href="specs/l1_l2_communication/l1_to_l2.html#security-considerations">below</a>.</li>
</ul>
<h3 id="commit"><a class="header" href="#commit">Commit</a></h3>
<p>After an upgrade has been initiated, it will be required that the next commit batches operation already contains the
system upgrade transaction. It is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L157">checked</a>
by verifying the corresponding L2→L1 log.</p>
<p>We also remember that the upgrade transaction has been processed in this batch (by amending the
<code>l2SystemContractsUpgradeBatchNumber</code> variable).</p>
<h3 id="revert"><a class="header" href="#revert">Revert</a></h3>
<p>In a very rare event when the team needs to revert the batch with the upgrade on ZKsync, the
<code>l2SystemContractsUpgradeBatchNumber</code> is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L412">reset</a>.</p>
<p>Note, however, that we do not “remember” that certain batches had a version before the upgrade, i.e. if the reverted
batches will have to be re-executed, the upgrade transaction must still be present there, even if some of the deleted
batches were committed before the upgrade and thus didn’t contain the transaction.</p>
<h3 id="execute"><a class="header" href="#execute">Execute</a></h3>
<p>Once batch with the upgrade transaction has been executed, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L304">delete</a>
them from storage for efficiency to signify that the upgrade has been fully processed and that a new upgrade can be
initiated.</p>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security considerations</a></h2>
<p>Since the operator can put any data into the bootloader memory and for L1→L2 transactions the bootloader has to blindly
trust it and rely on L1 contracts to validate it, it may be a very powerful tool for a malicious operator. Note, that
while the governance mechanism is generally trusted, we try to limit our trust for the operator as much as possible,
since in the future anyone would be able to become an operator.</p>
<p>Some time ago, we <em>used to</em> have a system where the upgrades could be done via L1→L2 transactions, i.e. the
implementation of the <code>DiamondProxy</code> upgrade would
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/upgrade-initializers/DIamondUpgradeInit2.sol#L27">include</a>
a priority transaction (with <code>from</code> equal to for instance <code>FORCE_DEPLOYER</code>) with all the upgrade params.</p>
<p>In the Boojum though having such logic would be dangerous and would allow for the following attack:</p>
<ul>
<li>Let’s say that we have at least 1 priority operations in the priority queue. This can be any operation, initiated by
anyone.</li>
<li>The operator puts a malicious priority operation with an upgrade into the bootloader memory. This operation was never
included in the priority operations queue / and it is not an upgrade transaction. However, as already mentioned above
the bootloader has no idea what priority / upgrade transactions are correct and so this transaction will be processed.</li>
</ul>
<p>The most important caveat of this malicious upgrade is that it may change implementation of the <code>Keccak256</code> precompile
to return any values that the operator needs.</p>
<ul>
<li>When the<code>priorityOperationsRollingHash</code> will be updated, instead of the “correct” rolling hash of the priority
transactions, the one which would appear with the correct topmost priority operation is returned. The operator can’t
amend the behaviour of <code>numberOfPriorityTransactions</code>, but it won’t help much, since the
the<code>priorityOperationsRollingHash</code> will match on L1 on the execution step.</li>
</ul>
<p>That’s why the concept of the upgrade transaction is needed: this is the only transaction that can initiate transactions
out of the kernel space and thus change bytecodes of system contracts. That’s why it must be the first one and that’s
why
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L587">emit</a>
its hash via a system L2→L1 log before actually processing it.</p>
<h3 id="why-it-doesnt-break-on-the-previous-version-of-the-system"><a class="header" href="#why-it-doesnt-break-on-the-previous-version-of-the-system">Why it doesn’t break on the previous version of the system</a></h3>
<p>This section is not required for Boojum understanding but for those willing to analyze the production system that is
deployed at the time of this writing.</p>
<p>Note that the hash of the transaction is calculated before the transaction is executed:
<a href="https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1055">https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1055</a></p>
<p>And then we publish its hash on L1 via a <em>system</em> L2→L1 log:
<a href="https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1133">https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1133</a></p>
<p>In the new upgrade system, the <code>priorityOperationsRollingHash</code> is calculated on L2 and so if something in the middle
changes the implementation of <code>Keccak256</code>, it may lead to the full <code>priorityOperationsRollingHash</code> be maliciously
crafted. In the pre-Boojum system, we publish all the hashes of the priority transactions via system L2→L1 and then the
rolling hash is calculated on L1. This means that if at least one of the hash is incorrect, then the entire rolling hash
will not match also.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l2l1-communication-1"><a class="header" href="#l2l1-communication-1">L2→L1 communication</a></h1>
<p>The L2→L1 communication is more fundamental than the L1→L2 communication, as the second relies on the first. L2→L1
communication happens by the L1 smart contract verifying messages alongside the proofs. The only “provable” part of the
communication from L2 to L1 are native L2→L1 logs emitted by VM. These can be emitted by the <code>to_l1</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">opcode</a>.
Each log consists of the following fields:</p>
<pre><code class="language-solidity">struct L2Log {
  uint8 l2ShardId;
  bool isService;
  uint16 txNumberInBatch;
  address sender;
  bytes32 key;
  bytes32 value;
}

</code></pre>
<p>Where:</p>
<ul>
<li><code>l2ShardId</code> is the id of the shard the opcode was called (it is currently always 0).</li>
<li><code>isService</code> a boolean flag that is not used right now</li>
<li><code>txNumberInBatch</code> the number of the transaction in the batch where the log has happened. This number is taken from the
internal counter which is incremented each time the <code>increment_tx_counter</code> is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">called</a>.</li>
<li><code>sender</code> is the value of <code>this</code> in the frame where the L2→L1 log was emitted.</li>
<li><code>key</code> and <code>value</code> are just two 32-byte values that could be used to carry some data with the log.</li>
</ul>
<p>The hashed array of these opcodes is then included into the
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/facets/Executor.sol#L493">batch commitment</a>.
Because of that we know that if the proof verifies, then the L2→L1 logs provided by the operator were correct, so we can
use that fact to produce more complex structures. Before Boojum such logs were also Merklized within the circuits and so
the Merkle tree’s root hash was included into the batch commitment also.</p>
<h2 id="important-system-values"><a class="header" href="#important-system-values">Important system values</a></h2>
<p>Two <code>key</code> and <code>value</code> fields are enough for a lot of system-related use-cases, such as sending timestamp of the batch,
previous batch hash, etc. They were and are used
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L438">used</a>
to verify the correctness of the batch’s timestamps and hashes. You can read more about block processing
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20&amp;%20L2%20blocks%20on%20zkSync.md">here</a>.</p>
<h2 id="long-l2l1-messages--bytecodes-1"><a class="header" href="#long-l2l1-messages--bytecodes-1">Long L2→L1 messages &amp; bytecodes</a></h2>
<p>However, sometimes users want to send long messages beyond 64 bytes which <code>key</code> and <code>value</code> allow us. But as already
said, these L2→L1 logs are the only ways that the L2 can communicate with the outside world. How do we provide long
messages?</p>
<p>Let’s add an <code>sendToL1</code> method in L1Messenger, where the main idea is the following:</p>
<ul>
<li>Let’s submit an L2→L1 log with <code>key = msg.sender</code> (the actual sender of the long message) and
<code>value = keccak256(message)</code>.</li>
<li>Now, during batch commitment the operator will have to provide an array of such long L2→L1 messages and it will be
checked on L1 that indeed for each such log the correct preimage was provided.</li>
</ul>
<p>A very similar idea is used to publish uncompressed bytecodes on L1 (the compressed bytecodes were sent via the long
L1→L2 messages mechanism as explained above).</p>
<p>Note, however, that whenever someone wants to prove that a certain message was present, they need to compose the L2→L1
log and prove its presence.</p>
<h2 id="priority-operations-1"><a class="header" href="#priority-operations-1">Priority operations</a></h2>
<p>Also, for each priority operation, we would send its hash and it status via an L2→L1 log. On L1 we would then
reconstruct the rolling hash of the processed priority transactions, allowing to correctly verify during the
<code>executeBatches</code> method that indeed the batch contained the correct priority operations.</p>
<p>Importantly, the fact that both hash and status were sent, it made it possible to
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/bridge/L1ERC20Bridge.sol#L255">prove</a>
that the L2 part of a deposit has failed and ask the bridge to release funds.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intro-to-zksyncs-zk"><a class="header" href="#intro-to-zksyncs-zk">Intro to ZKsync’s ZK</a></h1>
<p>This page is specific to our cryptography. For a general introduction, please read:
<a href="https://docs.zksync.io/build/developer-reference/rollups.html">https://docs.zksync.io/build/developer-reference/rollups.html</a></p>
<p>As a ZK rollup, we want everything to be verified by cryptography and secured by Ethereum. The power of ZK allows for
transaction compression, reducing fees for users while inheriting the same security.</p>
<p>ZK Proofs allow a verifier to easily check whether a prover has done a computation correctly. For ZKsync, the prover
will prove the correct execution of ZKsync’s EVM, and a smart contract on Ethereum will verify the proof is correct.</p>
<p>In more detail, there are several steps.</p>
<ul>
<li>Witness generation: witness generation can be perceived as part of the process where the user (prover) generates proof
of transaction validity. For instance, when a user initiates a transaction, a corresponding witness is generated,
which serves as proof that the transaction is valid and adheres to the network’s consensus rules. The zero-knowledge
aspect ensures that the witness reveals no information about the transaction’s specifics, maintaining user privacy and
data security. New transactions are proved in batches. These batches will be processed and sent to the circuits.</li>
<li>Circuits: Our virtual machine needs to prove that the execution was completed correctly to generate proofs correctly.
This is accomplished using circuits. In order for proofs to work, normal code logic must be transformed into a format
readable by the proof system. The virtual machine reads the code that will be executed and sorts the parts into
various circuits. These circuits then break down the parts of code, which can then be sent to the proof system.</li>
<li>Proof system: We need a proof system to process the ZK circuit. Our proving system is called Boojum.</li>
</ul>
<p>Here are the different repositories we use:</p>
<ul>
<li><strong>Boojum</strong>: Think of this as the toolbox. It holds essential tools and parts like the prover (which helps confirm the
circuit’s functionality), verifier (which double-checks everything), and various other backend components. These are
the technical bits and pieces, like defining Booleans, Nums, and Variables that will be used in the circuits.</li>
<li><strong>zkevm_circuits</strong>: This is where we build and store the actual circuits. The circuits are built from Boojum and
designed to replicate the behavior of the EVM.</li>
<li><strong>zkevm_test_harness</strong>: It’s like our testing ground. Here, we have different tests to ensure our circuits work
correctly. Additionally, it has the necessary code that helps kickstart and run these circuits smoothly.</li>
</ul>
<h3 id="what-is-a-circuit"><a class="header" href="#what-is-a-circuit">What is a circuit</a></h3>
<p>ZK circuits get their name from Arithmetic Circuits, which look like this (see picture). You can read the circuit by
starting at the bottom with the inputs, and following the arrows, computing each operation as you go.</p>
<p><img src="specs/prover/./img/intro_to_zkSync%E2%80%99s_ZK/circuit.png" alt="Untitled" /></p>
<p>The prover will prove that the circuit is “satisfied” by the inputs, meaning every step is computed correctly, leading
to a correct output.</p>
<p>It is very important that every step is actually “constrained”. The prover must be forced to compute the correct values.
If the circuit is missing a constraint, then a malicious prover can create proofs that will pass verification but not be
valid. The ZK terminology for this is that an underconstrained circuit could lead to a soundness error.</p>
<h3 id="what-do-zksyncs-circuits-prove"><a class="header" href="#what-do-zksyncs-circuits-prove">What do ZKsync’s circuits prove</a></h3>
<p>The main goal of our circuits is to prove correct execution of our VM. This includes proving each opcode run within the
VM, as well as other components such as precompiles, storage, and circuits that connect everything else together. This
is described in more detail in
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits.md">Circuits</a></p>
<h3 id="more-details"><a class="header" href="#more-details">More details</a></h3>
<p>The process of turning code into constraints is called arithmetization. Our arithmetization is based on a variation of
“Plonk”. The details are abstracted away from the circuits, but if you’d like to learn more, read about Plonk in
<a href="https://vitalik.eth.limo/general/2019/09/22/plonk.html">Vitalik’s blog</a> or the
<a href="https://github.com/mir-protocol/plonky2/blob/main/plonky2/plonky2.pdf">Plonky2 paper</a>.</p>
<p>More details of our proving system can be found in the <a href="https://eprint.iacr.org/2019/1400.pdf">Redshift Paper</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h1>
<p>Our ZK code is spread across three repositories:</p>
<p><a href="https://github.com/matter-labs/era-boojum/tree/main">Boojum</a> contains the low level ZK details.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_circuits/tree/main">zkevm_circuits</a> contains the code for the circuits.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_test_harness/tree/v1.4.0">zkevm_test_harness</a> contains the tests for the
circuits.</p>
<p>To get started, run the basic_test from the era-zkevm_test_harness:</p>
<pre><code class="language-bash">rustup default nightly-2023-08-23
cargo update
cargo test basic_test  --release -- --nocapture

</code></pre>
<p>This test may take several minutes to run, but you will see lot’s of information along the way!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zk-terminology"><a class="header" href="#zk-terminology">ZK Terminology</a></h1>
<h3 id="arithmetization"><a class="header" href="#arithmetization">Arithmetization</a></h3>
<p>Arithmetization refers to a technique used in zero-knowledge proof systems, where computations are represented in such a
way that they can be efficiently verified by a prover and a verifier. In simpler terms, it is a way to convert
computations into polynomial equations so that they can be used in cryptographic proofs.</p>
<h3 id="builder"><a class="header" href="#builder">Builder</a></h3>
<p>The builder helps set up the constraint system. The builder knows the placement strategy for each gate, as well as the
geometry and other information needed for building the constraint system.</p>
<h3 id="circuit"><a class="header" href="#circuit">Circuit</a></h3>
<p>An arithmetic circuit is a mathematical construct used in cryptography to encode a computational problem. It is
comprised of gates, with each gate performing an arithmetic operation, for example, such as addition or multiplication.
These circuits are essential for encoding statements or computations that a prover wants to prove knowledge of without
revealing the actual information.</p>
<h3 id="constraint"><a class="header" href="#constraint">Constraint</a></h3>
<p>A constraint is a rule or restriction that a specific operation or set of operations must follow. ZKsync uses
constraints to verify the validity of certain operations, and in the generation of proofs. Constraints can be missing,
causing bugs, or there could be too many constraints, leading to restricted operations.</p>
<h3 id="constraint-degree"><a class="header" href="#constraint-degree">Constraint degree</a></h3>
<p>The “constraint degree” of a constraint system refers to the maximum degree of the polynomial gates in the system. In
simpler terms, it’s the highest power of polynomial equations of the constraint system. At ZKsync, we allow gates with
degree 8 or lower.</p>
<h3 id="constraint-system-1"><a class="header" href="#constraint-system-1">Constraint system</a></h3>
<p>Constraint system is a mathematical representation consisting of a set of equations or polynomial constraints that are
constructed to represent the statements being proved. The system is deemed satisfied if there exist specific assignments
to the variables that make all the equations or constraints true. Imagine it as a list of “placeholders” called
Variables. Then we add gates to the variables which enforce a specific constraint. The Witness represents a specific
assignment of values to these Variables, ensuring that the rules still hold true.</p>
<h3 id="geometry"><a class="header" href="#geometry">Geometry</a></h3>
<p>The geometry defines the number of rows and columns in the constraint system. As part of PLONK arithmetization, the
witness data is arranged into a grid, where each row defines a gate (or a few gates), and the columns are as long as
needed to hold all of the witness data. At ZKsync, we have ~164 base witness columns.</p>
<h3 id="log"><a class="header" href="#log">Log</a></h3>
<p>We use the word “log” in the sense of a database log: a log stores a list of changes.</p>
<h3 id="lookup-table"><a class="header" href="#lookup-table">Lookup table</a></h3>
<p>Lookup table is a predefined table used to map input values to corresponding output values efficiently, assisting in
validating certain relations or computations without revealing any extra information about the inputs or the internal
computations. ****Lookup ****tables are particularly useful in ZK systems to optimize and reduce the complexity of
computations and validations, enabling the prover to construct proofs more efficiently and the verifier to validate
relationships or properties with minimal computational effort. For example, if you want to prove a certain number is
between 0 and 2^8, it is common to use a lookup table.</p>
<h3 id="proof"><a class="header" href="#proof">Proof</a></h3>
<p>A proof can refer generally to the entire proving process, or a proof can refer specifically to the data sent from the
prover to the verifier.</p>
<h3 id="prover"><a class="header" href="#prover">Prover</a></h3>
<p>In our ZKsync zk-rollup context, the prover is used to process a set of transactions executing smart contracts in a
succinct and efficient manner. It computes proofs that all the transactions are correct and ensures a valid transition
from one state to another. The proof will be sent to a Verifier smart contract on Ethereum. At ZKsync, we prove state
diffs of a block of transactions, in order to prove the new state root state is valid.</p>
<h3 id="satisfiable"><a class="header" href="#satisfiable">Satisfiable</a></h3>
<p>In the context of ZK, satisfiability refers to whether the witness passes - or “satisfies” - all of the constraints in a
circuit.</p>
<h3 id="state-diffs"><a class="header" href="#state-diffs">State Diffs</a></h3>
<p>State Diffs, or State Differentials, are the differences in accounts before and after processing transactions contained
in a block. For example, if my ETH Balance changes from 5 ETH to 6 ETH, then the state diff for my account is +1 ETH.</p>
<h3 id="variables"><a class="header" href="#variables">Variables</a></h3>
<p>Variables are placeholders in the constraint system until we know the specific witness data. The reason we would want
placeholders is because we might want to fill in which constraints we need, before knowing the actual input data. For
example, we might know we need to add two numbers and constrain the sum, before we know exactly which two numbers will
be in the witness.</p>
<h3 id="verifier"><a class="header" href="#verifier">Verifier</a></h3>
<p>The Verifier is a smart contract on Ethereum. It will receive a proof, check to make sure the proof is valid, and then
update the state root.</p>
<h3 id="witness"><a class="header" href="#witness">Witness</a></h3>
<p>Witness refers to the private, secret information or set of values that the prover possesses and aims to demonstrate
knowledge of, without revealing the actual information to the verifier. The witness is the input to the circuit. When we
have a circuit, the valid “witness” is the input that meets all the constraints and satisfies everything.</p>
<h3 id="worker"><a class="header" href="#worker">Worker</a></h3>
<p>A worker refers to our multi-threaded proving system. Proving may be “worked” in parallel, meaning that we can execute
some operations, like polynomial addition, in parallel threads.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boojum-function-check_if_satisfied"><a class="header" href="#boojum-function-check_if_satisfied">Boojum function: check_if_satisfied</a></h1>
<p>Note: Please read our other documentation and tests first before reading this page.</p>
<p>Our circuits (and tests) depend on a function from Boojum called
<a href="https://github.com/matter-labs/era-boojum/blob/main/src/cs/implementations/satisfiability_test.rs#L11">check_if_satisfied</a>.
You don’t need to understand it to run circuit tests, but it can be informative to learn more about Boojum and our proof
system.</p>
<p>First we prepare the constants, variables, and witness. As a reminder, the constants are just constant numbers, the
variables circuit columns that are under PLONK copy-permutation constraints (so they are close in semantics to variables
in programming languages), and the witness ephemeral values that can be used to prove certain constraints, for example
by providing an inverse if the variable must be non-zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied.png" alt="Check_if_satisfied.png" /></p>
<p>Next we prepare a view. Instead of working with all of the columns at once, it can be helpful to work with only a
subset.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(1).png" alt="Check_if_satisfied(1).png" /></p>
<p>Next we create the paths_mappings. For each gate in the circuit, we create a vector of booleans in the correct shape.
Later, when we traverse the gates with actual inputs, we’ll be able to remember which gates should be satisfied at
particular rows by computing the corresponding selector using constant columns and the paths_mappings.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(2).png" alt="Check_if_satisfied(2).png" /></p>
<p>Now, we have to actually check everything. The checks for the rows depend on whether they are under general purpose
columns, or under special purpose columns.</p>
<p><strong>General purpose rows:</strong></p>
<p>For each row and gate, we need several things.</p>
<ul>
<li>Evaluator for the gate, to compute the result of the gate</li>
<li>Path for the gate from the paths_mappings, to locate the gate</li>
<li>Constants_placement_offset, to find the constants</li>
<li>Num_terms in the evaluator
<ul>
<li>If this is zero, we can skip the row since there is nothing to do</li>
</ul>
</li>
<li>Gate_debug_name</li>
<li>num_constants_used</li>
<li>this_view</li>
<li>placement (described below)</li>
<li>evaluation function</li>
</ul>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(3).png" alt="Check_if_satisfied(3).png" /></p>
<p>Placement is either UniqueOnRow or MultipleOnRow. UniqueOnRow means there is only one gate on the row (typically because
the gate is larger / more complicated). MultipleOnRow means there are multiple gates within the same row (typically
because the gate is smaller). For example, if a gate only needs 30 columns, but we have 150 columns, we could include
five copies fo that gate in the same row.</p>
<p>Next, if the placement is UniqueOnRow, we call evaluate_over_general_purpose_columns. All of the evaluations should be
equal to zero, or we panic.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(4).png" alt="Check_if_satisfied(4).png" /></p>
<p>If the placement is MultipleOnRow, we again call evaluate_over_general_purpose_columns. If any of the evaluations are
non-zero, we log some extra debug information, and then panic.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(7).png" alt="Check_if_satisfied(7).png" /></p>
<p>This concludes evaluating and checking the generalized rows. Now we will check the specialized rows.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(8).png" alt="Check_if_satisfied(8).png" /></p>
<p>We start by initializing vectors for specialized_placement_data, evaluation_functions, views, and evaluator_names. Then,
we iterate over each gate_type_id and evaluator.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(9).png" alt="Check_if_satisfied(9).png" /></p>
<p>If gate_type_id is a LookupFormalGate, we don’t need to do anything in this loop because it is handled by the lookup
table. For all other cases, we need to check the evaluator’s total_quotient_terms_over_all_repetitions is non-zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(11).png" alt="Check_if_satisfied(11).png" /></p>
<p>Next, we get num_terms, num_repetitions, and share_constants, total_terms, initial_offset, per_repetition_offset, and
total_constants_available. All of these together form our placement data.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(12).png" alt="Check_if_satisfied(12).png" /></p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(13).png" alt="Check_if_satisfied(13).png" /></p>
<p>Once we know the placement_data, we can keep it for later, as well as the evaluator for this gate.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(14).png" alt="Check_if_satisfied(14).png" /></p>
<p>We also will keep the view and evaluator name. This is all the data we need from our specialized columns.</p>
<p>To complete the satisfiability test on the special columns, we just need to loop through and check that each of the
evaluations are zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(16).png" alt="Check_if_satisfied(16).png" /></p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(17).png" alt="Check_if_satisfied(17).png" /></p>
<p>Now we have checked every value on every row, so the satisfiability test is passed, and we can return true.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boojum-gadgets"><a class="header" href="#boojum-gadgets">Boojum gadgets</a></h1>
<p>Boojum gadgets are low-level implementations of tools for constraint systems. They consist of various types: curves,
hash functions, lookup tables, and different circuit types. These gadgets are mostly a reference from
<a href="https://github.com/matter-labs/franklin-crypto">franklin-crypto</a>, with additional hash functions added. These gadgets
have been changed to use the Goldilocks field (order 2^64 - 2^32 + 1), which is much smaller than bn256. This allows us
to reduce the proof system.</p>
<h2 id="circuits-types"><a class="header" href="#circuits-types">Circuits types</a></h2>
<p>We have next types with we use for circuits:</p>
<p><strong>Num (Number):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Num&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Boolean:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Boolean&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U8:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt8&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U16:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt16&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U32:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt32&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U160:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt160&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 5],
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U256:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt256&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 8],
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U512:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt512&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 16],
}
<span class="boring">}</span></code></pre></pre>
<p>Every type consists of a Variable (the number inside Variable is just the index):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Variable(pub(crate) u64);
<span class="boring">}</span></code></pre></pre>
<p>which is represented in the current Field. Variable is quite diverse, and to have “good” alignment and size we manually
do encoding management to be able to represent it as both copyable variable or witness.</p>
<p>The implementation of this circuit type itself is similar. We can also divide them into classes as main and dependent:
Such type like U8-U512 decoding inside functions to Num<F> for using them in logical operations. As mentioned above, the
property of these types is to perform logical operations and allocate witnesses.</p>
<p>Let’s demonstrate this in a Boolean example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: SmallField&gt; CSAllocatable&lt;F&gt; for Boolean&lt;F&gt; {
    type Witness = bool;
    fn placeholder_witness() -&gt; Self::Witness {
        false
    }

    #[inline(always)]
    fn allocate_without_value&lt;CS: ConstraintSystem&lt;F&gt;&gt;(cs: &amp;mut CS) -&gt; Self {
        let var = cs.alloc_variable_without_value();

        Self::from_variable_checked(cs, var)
    }

    fn allocate&lt;CS: ConstraintSystem&lt;F&gt;&gt;(cs: &amp;mut CS, witness: Self::Witness) -&gt; Self {
        let var = cs.alloc_single_variable_from_witness(F::from_u64_unchecked(witness as u64));

        Self::from_variable_checked(cs, var)
    }
}
<span class="boring">}</span></code></pre></pre>
<p>As you see, you can allocate both with and without witnesses.</p>
<h2 id="hash-function"><a class="header" href="#hash-function">Hash function</a></h2>
<p>In gadgets we have a lot of hash implementation:</p>
<ul>
<li>blake2s</li>
<li>keccak256</li>
<li>poseidon/poseidon2</li>
<li>sha256</li>
</ul>
<p>Each of them perform different functions in our proof system.</p>
<h2 id="queues"><a class="header" href="#queues">Queues</a></h2>
<p>One of the most important gadgets in our system is queue. It helps us to send data between circuits. Here is the quick
explanation how it works:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Struct CircuitQueue{
 head: HashState,
 tail: HashState,
 length: UInt32,
 witness: VecDeque&lt;Witness&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>The structure consists of <code>head</code> and <code>tail</code> commitments that basically are rolling hashes. Also, it has a <code>length</code> of
the queue. These three fields are allocated inside the constraint system. Also, there is a <code>witness</code>, that keeps actual
values that are now stored in the queue.</p>
<p>And here is the main functions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn push(&amp;mut self, value: Element) {
 // increment length
 // head - hash(head, value)
 // witness.push_back(value.witness)
}

fn pop(&amp;mut self) -&gt; Element {
 // check length &gt; 0
 // decrement length
 // value = witness.pop_front()
 // tail = hash(tail, value)
 // return value
}

fn final_check(&amp;self) -&gt; Element {
 // check that length == 0
 // check that head == tail
}
<span class="boring">}</span></code></pre></pre>
<p>So the key point, of how the queue proofs that popped elements are the same as pushed ones, is equality of rolling
hashes that stored in fields <code>head</code> and <code>tail</code>.</p>
<p>Also, we check that we can’t pop an element before it was pushed. This is done by checking that <code>length &gt;= 0</code>.</p>
<p>Very important is making the <code>final_check</code> that basically checks the equality of two hashes. So if the queue is never
empty, and we haven’t checked the equality of <code>head</code> and <code>tail</code> in the end, we also haven’t proven that the elements we
popped are correct.</p>
<p>For now, we use poseidon2 hash. Here is the link to queue implementations:</p>
<ul>
<li><a href="https://github.com/matter-labs/era-boojum/blob/main/src/gadgets/queue/mod.rs#L29">CircuitQueue</a></li>
<li><a href="https://github.com/matter-labs/era-boojum/blob/main/src/gadgets/queue/full_state_queue.rs#L20C12-L20C33">FullStateCircuitQueue</a></li>
</ul>
<p>The difference is that we actually compute and store a hash inside CircuitQueue during <code>push</code> and <code>pop</code> operations. But
in FullStateCircuitQueue our <code>head</code> and <code>tail</code> are just states of sponges. So instead of computing a full hash, we just
absorb a pushed (popped) element.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="circuit-testing"><a class="header" href="#circuit-testing">Circuit testing</a></h1>
<p>This page explains unit tests for circuits. Specifically, it goes through a unit test of
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/main/src/ecrecover/mod.rs#L796">ecrecover</a>. The tests for other
circuits are very similar.</p>
<p>Many of the tests for different circuits are nearly identical, for example:</p>
<ul>
<li>test_signature_for_address_verification (ecrecover)</li>
<li>test_code_unpacker_inner</li>
<li>test_demultiplex_storage_logs_inner</li>
<li>and several others.</li>
</ul>
<p>If you understand one, you will quickly be able to understand them all.</p>
<p>Let’s focus on ecrecover. Ecrecover is a precompile that, given your signature, can compute your address. If our circuit
works correctly, we should be able to recover the proper address, and be able to prove the computation was done
correctly.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(4).png" alt="Contest(4).png" /></p>
<p>The test begins by defining the geometry, max_variables, and max_trace_len. This data will be used to create the
constraint system. Next, we define a helper function:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(5).png" alt="Contest(5).png" /></p>
<p>To help run the test, we have a helper function called configure that returns a builder. The builder knows all of the
gates and gate placement strategy, which will be useful for setting up the constraint system.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(6).png" alt="Contest(6).png" /></p>
<p>The constraint system is almost ready! We still need to add the lookup tables for common boolean functions:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(7).png" alt="Contest(7).png" /></p>
<p>Now the constraint system is ready! We can start the main part of the test!</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(8).png" alt="Contest(8).png" /></p>
<p>Here we have hard coded a secret key with its associated public key, and generate a signature. We will test our circuit
on these inputs! Next we “allocate” these inputs as witnessess:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(9).png" alt="Contest(9).png" /></p>
<p>We have to use special integer types because we are working in a finite field.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(10).png" alt="Contest(10).png" /></p>
<p>The constants here are specific to the curve used, and are described in detail by code comments in the
ecrecover_precompile_inner_routine.</p>
<p>Finally we can call the ecrecover_precompile_inner_routine:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(11).png" alt="Contest(11).png" /></p>
<p>Lastly, we need to check to make sure that 1) we recovered the correct address, and 2) the constraint system can be
satisfied, meaning the proof works.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(12).png" alt="Contest(12).png" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="circuits-2"><a class="header" href="#circuits-2">Circuits</a></h1>
<h2 id="general-description"><a class="header" href="#general-description">General description</a></h2>
<p>The main circuit is called <code>MainVM</code>. It is the one where all the main logic happens.</p>
<p>It consists of multiple cycles, where on each iteration we take a next opcode and try to execute it the following way:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if opcode == Add {
 // do addition
}
if opcode == SRead {
 // do storage read
}
...
<span class="boring">}</span></code></pre></pre>
<p>You may notice that <code>Add</code> instruction is much simpler than the <code>SRead</code> one. When you work with circuits you still need
to execute every opcode.</p>
<p>That’s why we can use the following approach:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if opcode == Add {
 // do addition
}
if opcode == SRead {
 storage_queue.push((address, value));
 // proof storage read in other circuit
}
...
<span class="boring">}</span></code></pre></pre>
<p>So instead of proving <code>SRead</code> we just push a proving request, that will be sent to another circuit, that will prove it.
That’s how we can make our prover structure more optimized and flexible.</p>
<p>For now, we have 13 base layer circuits:</p>
<ul>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Main%20Vm.md">MainVM</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/SortDecommitments.md">CodeDecommitmentsSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/CodeDecommitter.md">CodeDecommitter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/DemuxLogQueue.md">LogDemuxer</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/KeccakRoundFunction.md">KeccakRoundFunction</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sha256RoundFunction.md">Sha256RoundFunction</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Ecrecover.md">ECRecover</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/RAMPermutation.md">RAMPermutation</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageSorter.md">StorageSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageApplication.md">StorageApplication</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">EventsSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">L1MessagesSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/L1MessagesHasher.md">L1MessagesHasher</a></p>
</li>
<li></li>
</ul>
<p>They mostly communicate by queues (the diagram of communication is below).</p>
<h2 id="public-input-structure"><a class="header" href="#public-input-structure">Public Input structure</a></h2>
<p>Public Input (PI) is some piece of data, that is revealed to the verifier. Usually, it consists of some inputs and
outputs.</p>
<p>The main challenge for base layer circuits is the ability to prove unlimited amount of execution. For example, our
<code>MainVm</code> circuit can handle execution of $x$ opcodes. Then, if some transaction causes execution of more than $x$
opcodes, we won’t be able to prove it. That’s why every circuit could be extended to multiple instances. So you can
always use $n$ <code>MainVm</code> instances to handle up to $nx$ opcode executions.</p>
<p>All circuits have the following PI structure:</p>
<p><img src="specs/prover/circuits/./img/diagram.png" alt="diagram.png" /></p>
<div class="table-wrapper"><table><thead><tr><th>start flag</th><th>Boolean that shows if this is the first instance of corresponding circuit type</th></tr></thead><tbody>
<tr><td>finished flag</td><td>Boolean that shows if this is the last instance of corresponding circuit type</td></tr>
<tr><td>Input</td><td>Structure that contains all inputs to this type of circuit (every instance of one circuit type has the same input)</td></tr>
<tr><td>FSM Input and FSM Output</td><td>The field has the same structure. It represents the inner state of circuit execution (the first fsm_input is empty, the second fsm_input equals the first fsm_output and so on…)</td></tr>
<tr><td>Output</td><td>Structure that contains all outputs of this type of circuit (the last instance contains the real output, the output field of the others is empty)</td></tr>
</tbody></table>
</div>
<p>The code implementation can be found
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/main/src/fsm_input_output/mod.rs#L32">here</a>.</p>
<p>In terms of Arithmetization we don’t allocate all these fields like public input variables. A more efficient approach
would be computing commitment of type <code>[Num&lt;F&gt;; 4]</code> with poseidon2 and then allocating these 4 variables as public
inputs.</p>
<p><img src="specs/prover/circuits/./img/image.png" alt="image.png" /></p>
<p>The equality of corresponding parts in different circuits is done during aggregating base layer circuits. Aggregating is
done by recursion level circuits that also verify base layer proofs. For now this is out of our scope, so we will focus
only on base layer.</p>
<h2 id="how-do-all-the-base-layer-circuits-fit-together"><a class="header" href="#how-do-all-the-base-layer-circuits-fit-together">How do all the base layer circuits fit together</a></h2>
<p><img src="specs/prover/circuits/./img/flowchart.png" alt="flowchart.png" /></p>
<h2 id="all-base-layer-circuits-inner-parts"><a class="header" href="#all-base-layer-circuits-inner-parts">All base layer circuits inner parts</a></h2>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Main%20Vm.md">Main Vm</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/SortDecommitments.md">SortDecommitments</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/CodeDecommitter.md">CodeDecommitter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/DemuxLogQueue.md">DemuxLogQueue</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/KeccakRoundFunction.md">KeccakRoundFunction</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sha256RoundFunction.md">Sha256RoundFunction</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Ecrecover.md">Ecrecover</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/RAMPermutation.md">RAMPermutation</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageSorter.md">StorageSorter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageApplication.md">StorageApplication</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">LogSorter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/L1MessagesHasher.md">L1MessagesHasher</a></p>
<p>There are a couple of circuits that do queue sorting. Here is the page that describes the algorithm:
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sorting.md">Sorting</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-4"><a class="header" href="#overview-4">Overview</a></h1>
<p>ZK Stack roll chains will be launched on L1 into a <a href="specs/zk_chains/./shared_bridge.html">shared bridge</a>. The shared bridge will create an
ecosystem of chains, with shared standards, upgrades, and free flow of assets. This free flow of assets will be enabled
by <a href="specs/zk_chains/./hyperbridges.html">hyperbridges</a>. Hyperbridges are trustless and cheap bridges between ZK Chains, allowing
cross-chain function calls.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gateway"><a class="header" href="#gateway">Gateway</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interop"><a class="header" href="#interop">Interop</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>In the Shared bridge document we described how the L1 smart contracts work to support multiple chains, and we emphasized
that the core feature is interop. Interop happens via the same L1-&gt;L2 interface as described in the L1SharedBridge doc.
There is (with the interop upgrade) a Bridgehub, AssetRouter, NativeTokenVault and Nullifier deployed on every L2, and
they serve the same feature as their L1 counterparts. Namely:</p>
<ul>
<li>The Bridgehub is used to start the transaction.</li>
<li>The AssetRouter and NativeTokenVault are the bridge contract that handle the tokens.</li>
<li>The Nullifier is used to prevent reexecution of xL2 txs.</li>
</ul>
<h3 id="interop-process"><a class="header" href="#interop-process">Interop process</a></h3>
<p><img src="specs/zk_chains/./img/hyperbridging.png" alt="Interop" /></p>
<p>The interop process has 7 main steps, each with its substeps:</p>
<ol>
<li>
<p>Starting the transaction on the sending chain</p>
<ul>
<li>The user/calls calls the Bridgehub contract. If they want to use a bridge they call
<code>requestL2TransactionTwoBridges</code>, if they want to make a direct call they call <code>requestL2TransactionDirect</code>
function.</li>
<li>The Bridgehub collects the base token fees necessary for the interop tx to be processed on the destination chain,
and if using the TwoBridges method the calldata and the destination contract ( for more data see Shared bridge
doc).</li>
<li>The Bridgehub emits a <code>NewPriorityRequest</code> event, this is the same as the one in our Mailbox contract. This event
specifies the xL2 txs, which uses the same format as L1-&gt;L2 txs. This event can be picked up and used to receive
the txs.</li>
<li>This new priority request is sent as an L2-&gt;L1 message, it is included in the chains merkle tree of emitted txs.</li>
</ul>
</li>
<li>
<p>The chain settles its proof on L1 or the Gateway, whichever is used as the settlement layer for the chain.</p>
</li>
<li>
<p>On the Settlement Layer (SL), the MessageRoot is updated in the MessageRoot contract. The new data includes all the
L2-&gt;L1 messages that are emitted from the settling chain.</p>
</li>
<li>
<p>The receiving chain picks up the updated MessgeRoot from the Settlement Layer.</p>
</li>
<li>
<p>Now the xL2 txs can be imported on the destination chain. Along with the txs, a merkle proof needs to be sent to link
it to the MessageRoot.</p>
</li>
<li>
<p>Receiving the tx on the destination chain</p>
<ul>
<li>On the destination chain the xL2 txs is verified. This means the merkle proof is checked agains the MessageRoot.
This shows the the xL2 txs was indeed sent.</li>
<li>After this the txs can be executed. The tx hash is stored in the L2Nullifier contract, so that the txs cannot be
replayed.</li>
<li>The specified contract is called, with the calldata, and the message sender =
<code>keccak256(originalMessageSender, originChainId) &gt;&gt; 160</code>. This is to prevent the collision of the msg.sender
addresses.</li>
</ul>
</li>
<li>
<p>The destination chain settles on the SL and the MessageRoot that it imported is checked.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-bridge"><a class="header" href="#shared-bridge">Shared Bridge</a></h1>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<p>Ethereum’s future is rollup-centric. This means breaking with the current paradigm of isolated EVM chains to
infrastructure that is focused on an ecosystem of interconnected zkEVMs, (which we name ZK Chains). This ecosystem will
be grounded on Ethereum, requiring the appropriate L1 smart contracts. Here we outline our ZK Stack approach for these
contracts, their interfaces, the needed changes to the existing architecture, as well as future features to be
implemented.</p>
<p>If you want to know more about ZK Chains, check this
<a href="https://blog.matter-labs.io/introduction-to-hyperchains-fdb33414ead7">blog post</a>, or go through
<a href="https://docs.zksync.io/zk-stack/concepts/zk-chains">our docs</a>.</p>
<h3 id="high-level-design"><a class="header" href="#high-level-design">High-level design</a></h3>
<p>We want to create a system where:</p>
<ul>
<li>ZK Chains should be launched permissionlessly within the ecosystem.</li>
<li>Interop should enable unified liquidity for assets across the ecosystem.</li>
<li>Multi-chain smart contracts need to be easy to develop, which means easy access to traditional bridges, and other
supporting architecture.</li>
</ul>
<p>ZK Chains have specific trust requirements - they need to satisfy certain common standards so that they can trust each
other. This means a single set of L1 smart contracts has to manage the proof verification for all ZK Chains, and if the
proof system is upgraded, all chains have to be upgraded together. New chains will be able to be launched
permissionlessly in the ecosystem according to this shared standard.</p>
<p>To allow unified liquidity each L1 asset (ETH, ERC20, NFTs) will have a single bridge contract on L1 for the whole
ecosystem. These shared bridges will allow users to deposit, withdraw and transfer from any ZK Chain in the ecosystem.
These shared bridges are also responsible for deploying and maintaining their counterparts on the ZK Chains. The
counterparts will be asset contracts extended with bridging functionality.</p>
<p>To enable the bridging functionality:</p>
<ul>
<li>On the L1 we will add a Bridgehub contract which connects asset bridges to all the ZK Chains. This will also be the
contract that holds the ETH for the ecosystem.</li>
<li>On the ZK Chain side we will add special system contracts that enable these features.</li>
</ul>
<p>We want to make the ecosystem as modular as possible, giving developers the ability to modify the architecture as
needed; consensus mechanism, staking, and DA requirements.</p>
<p>We also want the system to be forward-compatible, with future updates like L3s, proof aggregation, alternative State
Transition (ST) contracts, and ZK IP (which would allow unified liquidity between all STs). Those future features have
to be considered in this initial design, so it can evolve to support them (meaning, chains being launched now will still
be able to leverage them when available).</p>
<hr />
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<h3 id="general-architecture"><a class="header" href="#general-architecture">General Architecture</a></h3>
<p><img src="specs/zk_chains/./img/contractsExternal.png" alt="Contracts" /></p>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<h4 id="bridgehub"><a class="header" href="#bridgehub">Bridgehub</a></h4>
<ul>
<li>
<p>Acts as a hub for bridges, so that they have a single point of communication with all ZK Chain contracts. This allows
L1 assets to be locked in the same contract for all ZK Chains. The <code>Bridgehub</code> also implements the following features:</p>
</li>
<li>
<p><code>Registry</code> This is where ZK Chains can register, starting in a permissioned manner, but with the goal to be
permissionless in the future. This is where their <code>chainID</code> is determined. Chains on Gateway will also register here.
This <code>Registry</code> is also where Chain Type Manager contracts should register. Each chain has to specify its desired CTM
when registering (Initially, only one will be available).</p>
<pre><code>function newChain(
        uint256 _chainId,
        address _chainTypeManager
    ) external returns (uint256 chainId);

function newChainTypeManager(address _chainTypeManager) external;
</code></pre>
</li>
<li>
<p><code>BridgehubMailbox</code> routes messages to the Diamond proxy’s Mailbox facet based on chainID</p>
<ul>
<li>Same as the current zkEVM
<a href="https://github.com/matter-labs/era-contracts/blob/main/l1-contracts/contracts/zksync/facets/Mailbox.sol">Mailbox</a>,
just with chainId,</li>
<li>This is where L2 transactions can be requested.</li>
</ul>
<pre><code>  function requestL2TransactionTwoBridges(
      L2TransactionRequestTwoBridgesOuter calldata _request
  )
</code></pre>
<pre><code>struct L2TransactionRequestTwoBridgesOuter {
  uint256 chainId;
  uint256 mintValue;
  uint256 l2Value;
  uint256 l2GasLimit;
  uint256 l2GasPerPubdataByteLimit;
  address refundRecipient;
  address secondBridgeAddress;
  uint256 secondBridgeValue;
  bytes secondBridgeCalldata;
}
</code></pre>
</li>
</ul>
<pre><code>  struct L2TransactionRequestTwoBridgesInner {
    bytes32 magicValue;
    address l2Contract;
    bytes l2Calldata;
    bytes[] factoryDeps;
    bytes32 txDataHash;
}
</code></pre>
<ul>
<li>
<p>The <code>requestL2TransactionTwoBridges</code> function should be used most of the time when bridging to a chain ( the exeption
is when the user bridges directly to a contract on the L2, without using a bridge contract on L1). The logic of it is
the following:</p>
<ul>
<li>The user wants to bridge to chain with the provided <code>L2TransactionRequestTwoBridgesOuter.chainId</code>.</li>
<li>Two bridges are called, the baseTokenBridge (i.e. the L1SharedBridge or L1AssetRouter after the Gateway upgrade) and
an arbitrary second bridge. The Bridgehub will provide the original caller address to both bridges, which can
request that the appropriate amount of tokens are transferred from the caller to the bridge. The caller has to set
the appropriate allowance for both bridges. (Often the bridges coincide, but they don’t have to).</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.mintValue</code> is the amount of baseTokens that will be minted on L2. This is
the amount of tokens that the baseTokenBridge will request from the user. If the baseToken is Eth, it will be
forwarded to the baseTokenBridge.</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.l2Value</code> is the amount of tokens that will be deposited on L2. The second
bridge and the Mailbox receives this as an input (although our second bridge does not use the value).</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.l2GasLimit</code> is the maximum amount of gas that will be spent on L2 to
complete the transaction. The Mailbox receives this as an input.</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.l2GasPerPubdataByteLimit</code> is the maximum amount of gas per pubdata byte
that will be spent on L2 to complete the transaction. The Mailbox receives this as an input.</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.refundRecipient</code> is the address that will be refunded for the gas spent on
L2. The Mailbox receives this as an input.</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.secondBridgeAddress</code> is the address of the second bridge that will be
called. This is the arbitrary address that is called from the Bridgehub.</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.secondBridgeValue</code> is the amount of tokens that will be deposited on L2.
The second bridge receives this value as the baseToken (i.e. Eth on L1).</li>
<li>The <code>L2TransactionRequestTwoBridgesOuter.secondBridgeCalldata</code> is the calldata that will be passed to the second
bridge. This is the arbitrary calldata that is passed from the Bridgehub to the second bridge.</li>
<li>The secondBridge returns the <code>L2TransactionRequestTwoBridgesInner</code> struct to the Bridgehub. This is also passed to
the Mailbox as input. This is where the destination contract, calldata, factoryDeps are determined on the L2.</li>
</ul>
<p>This setup allows the user to bridge the baseToken of the origin chain A to a chain B with some other baseToken, by
specifying the A’s token in the secondBridgeValue, which will be minted on the destination chain as an ERC20 token,
and specifying the amount of B’s token in the mintValue, which will be minted as the baseToken and used to cover the
gas costs.</p>
</li>
</ul>
<h4 id="main-asset-shared-bridges-l2transactionrequesttwobridgesinner"><a class="header" href="#main-asset-shared-bridges-l2transactionrequesttwobridgesinner">Main asset shared bridges L2TransactionRequestTwoBridgesInner</a></h4>
<ul>
<li>
<p>Some assets have to be natively supported (ETH, WETH) and it also makes sense to support some generally accepted token
standards (ERC20 tokens), as this makes it easy to bridge those tokens (and ensures a single version of them exists on
the ZK Chain ecosystem). These canonical asset contracts are deployed from L1 by a bridge shared by all ZK Chains.
This is where assets are locked on L1. These bridges use the BridgeHub to communicate with all ZK Chains. Currently,
these bridges are the <code>WETH</code> and <code>ERC20</code> bridges.</p>
<ul>
<li>The pair on L2 is deployed from L1. The hash of the factory dependencies is stored on L1, and when a ZK Chain wants
to register, it can passes it in for deployment, it is verified, and the contract is deployed on L2. The actual
token contracts on L2 are deployed by the L2 bridge.</li>
</ul>
<pre><code>function initializeChain(
        uint256 _chainId,
        bytes[] calldata _factoryDeps,
        uint256 _deployBridgeImplementationFee,
        uint256 _deployBridgeProxyFee
    ) external payable {
    ....
    // Deploy L2 bridge proxy contract
        l2Bridge[_chainId] = BridgeInitializationHelper.requestDeployTransaction(
            _chainId,
            bridgehead,
            _deployBridgeProxyFee,
            l2SharedBridgeProxyBytecodeHash,
            l2SharedBridgeProxyConstructorData,
            // No factory deps are needed for L2 bridge proxy, because it is already passed in the previous step
            new bytes[](0)
        );
</code></pre>
</li>
</ul>
<h4 id="chain-type-manager"><a class="header" href="#chain-type-manager">Chain Type Manager</a></h4>
<ul>
<li><code>ChainTypeManager</code> A chain type manager manages proof verification and DA for multiple chains. It also implements the
following functionalities:
<ul>
<li><code>ChainTypeRegistry</code> The ST is shared for multiple chains, so initialization and upgrades have to be the same for all
chains. Registration is not permissionless but happens based on the registrations in the bridgehub’s <code>Registry</code>. At
registration a <code>DiamondProxy</code> is deployed and initialized with the appropriate <code>Facets</code> for each ZK Chain.</li>
<li><code>Facets</code> and <code>Verifier</code> are shared across chains that relies on the same ST: <code>Base</code>, <code>Executor</code> , <code>Getters</code>, <code>Admin</code>
, <code>Mailbox.</code>The <code>Verifier</code> is the contract that actually verifies the proof, and is called by the <code>Executor</code>.</li>
<li>Upgrade Mechanism The system requires all chains to be up-to-date with the latest implementation, so whenever an
update is needed, we have to “force” each chain to update, but due to decentralization, we have to give each chain a
time frame. This is done in the update mechanism contract, this is where the bootloader and system contracts are
published, and the <code>ProposedUpgrade</code> is stored. Then each chain can call this upgrade for themselves as needed.
After the deadline is over, the not-updated chains are frozen, that is, cannot post new proofs. Frozen chains can
unfreeze by updating their proof system.</li>
</ul>
</li>
<li>Each chain has a <code>DiamondProxy</code>.
<ul>
<li>The <a href="https://eips.ethereum.org/EIPS/eip-2535">Diamond Proxy</a> is the proxy pattern that is used for the chain
contracts. A diamond proxy points to multiple implementation contracts called facets. Each selector is saved in the
proxy, and the correct facet is selected and called.</li>
<li>In the future the DiamondProxy can be configured by picking alternative facets e.g. Validiums will have their own
<code>Executor</code></li>
</ul>
</li>
</ul>
<h4 id="chain-specific-contracts"><a class="header" href="#chain-specific-contracts">Chain specific contracts</a></h4>
<ul>
<li>A chain might implement its own specific consensus mechanism. This needs its own contracts. Only this contract will be
able to submit proofs to the State Transition contract.</li>
<li>DA contracts.</li>
<li>Currently, the <code>ValidatorTimelock</code> is an example of such a contract.</li>
</ul>
<h3 id="components-interactions"><a class="header" href="#components-interactions">Components interactions</a></h3>
<p>In this section, we will present some diagrams showing the interaction of different components.</p>
<h4 id="new-chain"><a class="header" href="#new-chain">New Chain</a></h4>
<p>A chain registers in the Bridgehub, this is where the chain ID is determined. The chain’s governor specifies the State
Transition that they plan to use. In the first version only a single State Transition contract will be available for
use, our with Boojum proof verification.</p>
<p>At initialization we prepare the <code>ZkSyncHyperchain</code> contract. We store the genesis batch hash in the ST contract, all
chains start out with the same state. A diamond proxy is deployed and initialised with this initial value, along with
predefined facets which are made available by the ST contract. These facets contain the proof verification and other
features required to process proofs. The chain ID is set in the VM in a special system transaction sent from L1.</p>
<!--![newChain.png](./img/newChain.png) Image outdated-->
<hr />
<h3 id="common-standards-and-upgrades"><a class="header" href="#common-standards-and-upgrades">Common Standards and Upgrades</a></h3>
<p>In this initial phase, ZK Chains have to follow some common standards, so that they can trust each other. This means all
chains start out with the same empty state, they have the same VM implementations and proof systems, asset contracts can
trust each on different chains, and the chains are upgraded together. We elaborate on the shared upgrade mechanism here.</p>
<h4 id="upgrade-mechanism"><a class="header" href="#upgrade-mechanism">Upgrade mechanism</a></h4>
<p>Currently, there are three types of upgrades for zkEVM. Normal upgrades (used for new features) are initiated by the
Governor (a multisig) and are public for a certain timeframe before they can be applied. Shadow upgrades are similar to
normal upgrades, but the data is not known at the moment the upgrade is proposed, but only when executed (they can be
executed with the delay, or instantly if approved by the security council). Instant upgrades (used for security issues),
on the other hand happen quickly and need to be approved by the Security Council in addition to the Governor. For ZK
Chains the difference is that upgrades now happen on multiple chains. This is only a problem for shadow upgrades - in
this case, the chains have to tightly coordinate to make all the upgrades happen in a short time frame, as the content
of the upgrade becomes public once the first chain is upgraded. The actual upgrade process is as follows:</p>
<ol>
<li>Prepare Upgrade for all chains:
<ul>
<li>The new facets and upgrade contracts have to be deployed,</li>
<li>The upgrade’ calldata (diamondCut, initCalldata with ProposedUpgrade) is hashed on L1 and the hash is saved.</li>
</ul>
</li>
<li>Upgrade specific chain
<ul>
<li>The upgrade has to be called on the specific chain. The upgrade calldata is passed in as calldata and verified. The
protocol version is updated.</li>
<li>Ideally, the upgrade will be very similar for all chains. If it is not, a smart contract can calculate the
differences. If this is also not possible, we have to set the <code>diamondCut</code> for each chain by hand.</li>
</ul>
</li>
<li>Freeze not upgraded chains
<ul>
<li>After a certain time the chains that are not upgraded are frozen.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-5"><a class="header" href="#overview-5">Overview</a></h1>
<p>The zkEVM is used to execute transactions. It is similar in construction to the EVM, so it executes transactions
similarly, but it plays a fundamentally different role in the zkStack than the EVM does in Ethereum. The EVM is used to
execute smart contracts in Ethereum’s state transition function. This STF needs a client to implement and run it.</p>
<p>Rollups have a different set of requirements, they need to produce a proof that some client executed the STF correctly.
This client is the <a href="specs/zk_evm/./vm_specification/README.html">zkEVM</a>, it is made to run the STF efficiently. The STF is the
<a href="specs/zk_evm/./bootloader.html">Bootloader</a>.</p>
<p>The smart contracts are native zkEVM bytecode, zkEVM can execute these easily. In the future the ZK Stack will also
support EVM bytecode by running an efficient interpreter inside the zkEVM.</p>
<p>The zkEVM has a lot of special features compared to the EVM that are needed for the rollup’s STF, storage, gas metering,
precompiles etc. These functions are either built into the zkEVM, or there are special
<a href="specs/zk_evm/./system_contracts.html">system contracts</a> for them. The system contracts are deployed at predefined addresses, they are
called by the Bootloader, and they have special permissions compared to normal user contracts. These are not to be
confused with the <a href="specs/zk_evm/./precompiles.html">precompiles</a>, which are also pre-deployed contracts with special support from the
zkEVM, but these contract do not have special permissions and are called by the users and not the Bootloader.</p>
<p>The zkEVM also has user-facing features. For the best possible UX the ZK Stack supports native
<a href="specs/zk_evm/./account_abstraction.html">account abstraction</a>. This means users can fully customize how they pay the fees needed to
execute their transactions.</p>
<p>All transactions need to pay fees. The requirements to run a rollup are different than the ones needed to run Ethereum,
so the ZK Stack has a different <a href="specs/zk_evm/./fee_model.html">fee model</a>. The fee model is designed to consider all the components
that are needed to run the rollup: data and proof execution costs on L1, sequencer costs, and prover costs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="account-abstraction"><a class="header" href="#account-abstraction">Account abstraction</a></h1>
<p>One of the other important features of ZKsync is the support of account abstraction. It is highly recommended to read
the documentation on our AA protocol here:
<a href="https://docs.zksync.io/build/developer-reference/account-abstraction">https://docs.zksync.io/build/developer-reference/account-abstraction</a></p>
<h3 id="account-versioning"><a class="header" href="#account-versioning">Account versioning</a></h3>
<p>Each account can also specify which version of the account abstraction protocol do they support. This is needed to allow
breaking changes of the protocol in the future.</p>
<p>Currently, two versions are supported: <code>None</code> (i.e. it is a simple contract and it should never be used as <code>from</code> field
of a transaction), and <code>Version1</code>.</p>
<h3 id="nonce-ordering"><a class="header" href="#nonce-ordering">Nonce ordering</a></h3>
<p>Accounts can also signal to the operator which nonce ordering it should expect from these accounts: <code>Sequential</code> or
<code>Arbitrary</code>.</p>
<p><code>Sequential</code> means that the nonces should be ordered in the same way as in EOAs. This means, that, for instance, the
operator will always wait for a transaction with nonce <code>X</code> before processing a transaction with nonce <code>X+1</code>.</p>
<p><code>Arbitrary</code> means that the nonces can be ordered in arbitrary order. It is supported by the server right now, i.e. if
there is a contract with arbitrary nonce ordering, its transactions will likely either be rejected or get stuck in the
mempool due to nonce mismatch.</p>
<p>Note, that this is not enforced by system contracts in any way. Some sanity checks may be present, but the accounts are
allowed to do however they like. It is more of a suggestion to the operator on how to manage the mempool.</p>
<h3 id="returned-magic-value"><a class="header" href="#returned-magic-value">Returned magic value</a></h3>
<p>Now, both accounts and paymasters are required to return a certain magic value upon validation. This magic value will be
enforced to be correct on the mainnet, but will be ignored during fee estimation. Unlike Ethereum, the signature
verification + fee charging/nonce increment are not included as part of the intrinsic costs of the transaction. These
are paid as part of the execution and so they need to be estimated as part of the estimation for the transaction’s
costs.</p>
<p>Generally, the accounts are recommended to perform as many operations as during normal validation, but only return the
invalid magic in the end of the validation. This will allow to correctly (or at least as correctly as possible) estimate
the price for the validation of the account.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bootloader-2"><a class="header" href="#bootloader-2">Bootloader</a></h1>
<p>On standard Ethereum clients, the workflow for executing blocks is the following:</p>
<ol>
<li>Pick a transaction, validate the transactions &amp; charge the fee, execute it</li>
<li>Gather the state changes (if the transaction has not reverted), apply them to the state.</li>
<li>Go back to step (1) if the block gas limit has not been yet exceeded.</li>
</ol>
<p>However, having such flow on ZKsync (i.e. processing transaction one-by-one) would be too inefficient, since we have to
run the entire proving workflow for each individual transaction. That’s why we need the <em>bootloader</em>: instead of running
N transactions separately, we run the entire batch (set of blocks, more can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20%26%20L2%20blocks%20on%20zkSync.md">here</a>)
as a single program that accepts the array of transactions as well as some other batch metadata and processes them
inside a single big “transaction”. The easiest way to think about bootloader is to think in terms of EntryPoint from
EIP4337: it also accepts the array of transactions and facilitates the Account Abstraction protocol.</p>
<p>The hash of the code of the bootloader is stored on L1 and can only be changed as a part of a system upgrade. Note, that
unlike system contracts, the bootloader’s code is not stored anywhere on L2. That’s why we may sometimes refer to the
bootloader’s address as formal. It only exists for the sake of providing some value to <code>this</code> / <code>msg.sender</code>/etc. When
someone calls the bootloader address (e.g. to pay fees) the EmptyContract’s code is actually invoked.</p>
<p>Bootloader is the program that accepts an array of transactions and executes the entire ZKsync batch. This section will
expand on its invariants and methods.</p>
<h2 id="playground-bootloader-vs-proved-bootloader"><a class="header" href="#playground-bootloader-vs-proved-bootloader">Playground bootloader vs proved bootloader</a></h2>
<p>For convenience, we use the same implementation of the bootloader both in the mainnet batches and for emulating ethCalls
or other testing activities. <em>Only</em> <em>proved</em> bootloader is ever used for batch-building and thus this document only
describes it.</p>
<h2 id="start-of-the-batch"><a class="header" href="#start-of-the-batch">Start of the batch</a></h2>
<p>It is enforced by the ZKPs, that the state of the bootloader is equivalent to the state of a contract transaction with
empty calldata. The only difference is that it starts with all the possible memory pre-allocated (to avoid costs for
memory expansion).</p>
<p>For additional efficiency (and our convenience), the bootloader receives its parameters inside its memory. This is the
only point of non-determinism: the bootloader <em>starts with its memory pre-filled with any data the operator wants</em>.
That’s why it is responsible for validating the correctness of it and it should never rely on the initial contents of
the memory to be correct &amp; valid.</p>
<p>For instance, for each transaction, we check that it is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3058">properly ABI-encoded</a>
and that the transactions
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3736">go exactly one after another</a>.
We also ensure that transactions do not exceed the limits of the memory space allowed for transactions.</p>
<h2 id="transaction-types--their-validation"><a class="header" href="#transaction-types--their-validation">Transaction types &amp; their validation</a></h2>
<p>While the main transaction format is the internal <code>Transaction</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/libraries/TransactionHelper.sol#L25">format</a>,
it is a struct that is used to represent various kinds of transactions types. It contains a lot of <code>reserved</code> fields
that could be used depending in the future types of transactions without need for AA to change the interfaces of their
contracts.</p>
<p>The exact type of the transaction is marked by the <code>txType</code> field of the transaction type. There are 6 types currently
supported:</p>
<ul>
<li><code>txType</code>: 0. It means that this transaction is of legacy transaction type. The following restrictions are enforced:</li>
<li><code>maxFeePerErgs=getMaxPriorityFeePerErg</code> since it is pre-EIP1559 tx type.</li>
<li><code>reserved1..reserved4</code> as well as <code>paymaster</code> are 0. <code>paymasterInput</code> is zero.</li>
<li>Note, that unlike type 1 and type 2 transactions, <code>reserved0</code> field can be set to a non-zero value, denoting that this
legacy transaction is EIP-155-compatible and its RLP encoding (as well as signature) should contain the <code>chainId</code> of
the system.</li>
<li><code>txType</code>: 1. It means that the transaction is of type 1, i.e. transactions access list. ZKsync does not support access
lists in any way, so no benefits of fulfilling this list will be provided. The access list is assumed to be empty. The
same restrictions as for type 0 are enforced, but also <code>reserved0</code> must be 0.</li>
<li><code>txType</code>: 2. It is EIP1559 transactions. The same restrictions as for type 1 apply, but now <code>maxFeePerErgs</code> may not be
equal to <code>getMaxPriorityFeePerErg</code>.</li>
<li><code>txType</code>: 113. It is ZKsync transaction type. This transaction type is intended for AA support. The only restriction
that applies to this transaction type: fields <code>reserved0..reserved4</code> must be equal to 0.</li>
<li><code>txType</code>: 254. It is a transaction type that is used for upgrading the L2 system. This is the only type of transaction
is allowed to start a transaction out of the name of the contracts in kernel space.</li>
<li><code>txType</code>: 255. It is a transaction that comes from L1. There are almost no restrictions explicitly imposed upon this
type of transaction, since the bootloader at the end of its execution sends the rolling hash of the executed priority
transactions. The L1 contract ensures that the hash did indeed match the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L282">hashes of the priority transactions on L1</a>.</li>
</ul>
<p>You can also read more on L1-&gt;L2 transactions and upgrade transactions
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>However, as already stated, the bootloader’s memory is not deterministic and the operator is free to put anything it
wants there. For all of the transaction types above the restrictions are imposed in the following
(<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2828">method</a>),
which is called before starting processing the transaction.</p>
<h2 id="structure-of-the-bootloaders-memory"><a class="header" href="#structure-of-the-bootloaders-memory">Structure of the bootloader’s memory</a></h2>
<p>The bootloader expects the following structure of the memory (here by word we denote 32-bytes, the same machine word as
on EVM):</p>
<h3 id="batch-information"><a class="header" href="#batch-information"><strong>Batch information</strong></a></h3>
<p>The first 8 words are reserved for the batch information provided by the operator.</p>
<ul>
<li><code>0</code> word — the address of the operator (the beneficiary of the transactions).</li>
<li><code>1</code> word — the hash of the previous batch. Its validation will be explained later on.</li>
<li><code>2</code> word — the timestamp of the current batch. Its validation will be explained later on.</li>
<li><code>3</code> word — the number of the new batch.</li>
<li><code>4</code> word — the L1 gas price provided by the operator.</li>
<li><code>5</code> word — the “fair” price for L2 gas, i.e. the price below which the <code>baseFee</code> of the batch should not fall. For
now, it is provided by the operator, but it in the future it may become hardcoded.</li>
<li><code>6</code> word — the base fee for the batch that is expected by the operator. While the base fee is deterministic, it is
still provided to the bootloader just to make sure that the data that the operator has coincides with the data
provided by the bootloader.</li>
<li><code>7</code> word — reserved word. Unused on proved batch.</li>
</ul>
<p>The batch information slots
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3629">are used at the beginning of the batch</a>.
Once read, these slots can be used for temporary data.</p>
<h3 id="temporary-data-for-debug--transaction-processing-purposes"><a class="header" href="#temporary-data-for-debug--transaction-processing-purposes"><strong>Temporary data for debug &amp; transaction processing purposes</strong></a></h3>
<ul>
<li><code>[8..39]</code> – reserved slots for debugging purposes</li>
<li><code>[40..72]</code> – slots for holding the paymaster context data for the current transaction. The role of the paymaster
context is similar to the <a href="https://eips.ethereum.org/EIPS/eip-4337">EIP4337</a>’s one. You can read more about it in the
account abstraction documentation.</li>
<li><code>[73..74]</code> – slots for signed and explorer transaction hash of the currently processed L2 transaction.</li>
<li><code>[75..110]</code> – 36 slots for the calldata for the KnownCodesContract call.</li>
<li><code>[111..1134]</code> – 1024 slots for the refunds for the transactions.</li>
<li><code>[1135..2158]</code> – 1024 slots for the overhead for batch for the transactions. This overhead is suggested by the
operator, i.e. the bootloader will still double-check that the operator does not overcharge the user.</li>
<li><code>[2159..3182]</code> – slots for the “trusted” gas limits by the operator. The user’s transaction will have at its disposal
<code>min(MAX_TX_GAS(), trustedGasLimit)</code>, where <code>MAX_TX_GAS</code> is a constant guaranteed by the system. Currently, it is
equal to 80 million gas. In the future, this feature will be removed.</li>
<li><code>[3183..7282]</code> – slots for storing L2 block info for each transaction. You can read more on the difference L2 blocks
and batches
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20&amp;%20L2%20blocks%20on%20zkSync.md">here</a>.</li>
<li><code>[7283..40050]</code> – slots used for compressed bytecodes each in the following format:
<ul>
<li>32 bytecode hash</li>
<li>32 zeroes (but then it will be modified by the bootloader to contain 28 zeroes and then the 4-byte selector of the
<code>publishCompressedBytecode</code> function of the <code>BytecodeCompressor</code>)</li>
<li>The calldata to the bytecode compressor (without the selector).</li>
</ul>
</li>
<li><code>[40051..40052]</code> – slots where the hash and the number of current priority ops is stored. More on it in the priority
operations
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">section</a>.</li>
</ul>
<h3 id="l1messenger-pubdata"><a class="header" href="#l1messenger-pubdata">L1Messenger Pubdata</a></h3>
<ul>
<li><code>[40053..248052]</code> – slots where the final batch pubdata is supplied to be verified by the L1Messenger. More on how the
L1Messenger system contracts handles the pubdata can be read
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</li>
</ul>
<p>But briefly, this space is used for the calldata to the L1Messenger’s <code>publishPubdataAndClearState</code> function, which
accepts the list of the user L2→L1 logs, published L2→L1 messages as well as bytecodes. It also takes the list of full
state diff entries, which describe how each storage slot has changed as well as compressed state diffs. This method will
then check the correctness of the provided data and publish the hash of the correct pubdata to L1.</p>
<p>Note, that while the realistic number of pubdata that can be published in a batch is 120kb, the size of the calldata to
L1Messenger may be a lot larger due to the fact that this method also accepts the original uncompressed state diff
entries. These will not be published to L1, but will be used to verify the correctness of the compression. The
worst-case number of bytes that may be needed for this scratch space is if all the pubdata consists of repeated writes
(i.e. we’ll need only 4 bytes to include key) that turn into 0 (i.e. they’ll need only 1 byte to describe it). However,
each of these writes in the uncompressed form will be represented as 272 byte state diff entry and so we get the number
of diffs is <code>120k / 5 = 24k</code>. This means that they will have accommodate <code>24k * 272 = 6528000</code> bytes of calldata for the
uncompressed state diffs. Adding 120k on top leaves us with roughly <code>6650000</code> bytes needed for calldata. <code>207813</code> slots
are needed to accommodate this amount of data. We round up to <code>208000</code> slots to give space for constant-size factors for
ABI-encoding, like offsets, lengths, etc.</p>
<p>In theory though much more calldata could be used (if for instance 1 byte is used for enum index). It is the
responsibility of the operator to ensure that it can form the correct calldata for the L1Messenger.</p>
<h3 id="transactions-meta-descriptions"><a class="header" href="#transactions-meta-descriptions"><strong>Transaction’s meta descriptions</strong></a></h3>
<ul>
<li><code>[248053..250100]</code> words — 2048 slots for 1024 transaction’s meta descriptions (their structure is explained below).</li>
</ul>
<p>For internal reasons related to possible future integrations of zero-knowledge proofs about some of the contents of the
bootloader’s memory, the array of the transactions is not passed as the ABI-encoding of the array of transactions, but:</p>
<ul>
<li>We have a constant maximum number of transactions. At the time of this writing, this number is 1024.</li>
<li>Then, we have 1024 transaction descriptions, each ABI encoded as the following struct:</li>
</ul>
<pre><code class="language-solidity">struct BootloaderTxDescription {
  // The offset by which the ABI-encoded transaction's data is stored
  uint256 txDataOffset;
  // Auxiliary data on the transaction's execution. In our internal versions
  // of the bootloader it may have some special meaning, but for the
  // bootloader used on the mainnet it has only one meaning: whether to execute
  // the transaction. If 0, no more transactions should be executed. If 1, then
  // we should execute this transaction and possibly try to execute the next one.
  uint256 txExecutionMeta;
}

</code></pre>
<h3 id="reserved-slots-for-the-calldata-for-the-paymasters-postop-operation"><a class="header" href="#reserved-slots-for-the-calldata-for-the-paymasters-postop-operation"><strong>Reserved slots for the calldata for the paymaster’s postOp operation</strong></a></h3>
<ul>
<li><code>[252149..252188]</code> words — 40 slots which could be used for encoding the calls for postOp methods of the paymaster.</li>
</ul>
<p>To avoid additional copying of transactions for calls for the account abstraction, we reserve some of the slots which
could be then used to form the calldata for the <code>postOp</code> call for the account abstraction without having to copy the
entire transaction’s data.</p>
<h3 id="the-actual-transactions-descriptions"><a class="header" href="#the-actual-transactions-descriptions"><strong>The actual transaction’s descriptions</strong></a></h3>
<ul>
<li><code>[252189..523261]</code></li>
</ul>
<p>Starting from the 487312 word, the actual descriptions of the transactions start. (The struct can be found by this
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/libraries/TransactionHelper.sol#L25">link</a>).
The bootloader enforces that:</p>
<ul>
<li>They are correctly ABI encoded representations of the struct above.</li>
<li>They are located without any gaps in memory (the first transaction starts at word 653 and each transaction goes right
after the next one).</li>
<li>The contents of the currently processed transaction (and the ones that will be processed later on are untouched).
Note, that we do allow overriding data from the already processed transactions as it helps to preserve efficiency by
not having to copy the contents of the <code>Transaction</code> each time we need to encode a call to the account.</li>
</ul>
<h3 id="vm-hook-pointers"><a class="header" href="#vm-hook-pointers"><strong>VM hook pointers</strong></a></h3>
<ul>
<li><code>[523261..523263]</code></li>
</ul>
<p>These are memory slots that are used purely for debugging purposes (when the VM writes to these slots, the server side
can catch these calls and give important insight information for debugging issues).</p>
<h3 id="result-ptr-pointer"><a class="header" href="#result-ptr-pointer"><strong>Result ptr pointer</strong></a></h3>
<ul>
<li>[523264..524287]</li>
</ul>
<p>These are memory slots that are used to track the success status of a transaction. If the transaction with number <code>i</code>
succeeded, the slot <code>2^19 - 1024 + i</code> will be marked as 1 and 0 otherwise.</p>
<h2 id="general-flow-of-the-bootloaders-execution"><a class="header" href="#general-flow-of-the-bootloaders-execution">General flow of the bootloader’s execution</a></h2>
<ol>
<li>At the start of the batch it
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3629">reads the initial batch information</a>
and
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3674">sends the information</a>
about the current batch to the SystemContext system contract.</li>
<li>It goes through each of
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3715">transaction’s descriptions</a>
and checks whether the <code>execute</code> field is set. If not, it ends processing of the transactions and ends execution of
the batch. If the execute field is non-zero, the transaction will be executed and it goes to step 3.</li>
<li>Based on the transaction’s type it decides whether the transaction is an L1 or L2 transaction and processes them
accordingly. More on the processing of the L1 transactions can be read <a href="specs/zk_evm/bootloader.html#l1-l2-transactions">here</a>. More on L2
transactions can be read <a href="specs/zk_evm/bootloader.html#l2-transactions">here</a>.</li>
</ol>
<h2 id="l2-transactions"><a class="header" href="#l2-transactions">L2 transactions</a></h2>
<p>On ZKsync, every address is a contract. Users can start transactions from their EOA accounts, because every address that
does not have any contract deployed on it implicitly contains the code defined in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/DefaultAccount.sol">DefaultAccount.sol</a>
file. Whenever anyone calls a contract that is not in kernel space (i.e. the address is ≥ 2^16) and does not have any
contract code deployed on it, the code for <code>DefaultAccount</code> will be used as the contract’s code.</p>
<p>Note, that if you call an account that is in kernel space and does not have any code deployed there, right now, the
transaction will revert.</p>
<p>We process the L2 transactions according to our account abstraction protocol:
<a href="https://v2-docs.zksync.io/dev/tutorials/custom-aa-tutorial.html#prerequisite">https://v2-docs.zksync.io/dev/tutorials/custom-aa-tutorial.html#prerequisite</a>.</p>
<ol>
<li>We
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1073">deduct</a>
the transaction’s upfront payment for the overhead for the block’s processing. You can read more on how that works in
the fee model
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/zkSync%20fee%20model.md">description</a>.</li>
<li>Then we calculate the gasPrice for these transactions according to the EIP1559 rules.</li>
<li>We
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1180">conduct the validation step</a>
of the AA protocol:</li>
</ol>
<ul>
<li>We calculate the hash of the transaction.</li>
<li>If enough gas has been provided, we near_call the validation function in the bootloader. It sets the tx.origin to the
address of the bootloader, sets the ergsPrice. It also marks the factory dependencies provided by the transaction as
marked and then invokes the validation method of the account and verifies the returned magic.</li>
<li>Calls the accounts and, if needed, the paymaster to receive the payment for the transaction. Note, that accounts may
not use <code>block.baseFee</code> context variable, so they have no way to know what exact sum to pay. That’s why the accounts
typically firstly send <code>tx.maxFeePerErg * tx.ergsLimit</code> and the bootloader
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L730">refunds</a>
for any excess funds sent.</li>
</ul>
<ol>
<li><a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1234">We perform the execution of the transaction</a>.
Note, that if the sender is an EOA, tx.origin is set equal to the <code>from</code> the value of the transaction. During the
execution of the transaction, the publishing of the compressed bytecodes happens: for each factory dependency if it
has not been published yet and its hash is currently pointed to in the compressed bytecodes area of the bootloader, a
call to the bytecode compressor is done. Also, at the end the call to the KnownCodeStorage is done to ensure all the
bytecodes have indeed been published.</li>
<li>We
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1401">refund</a>
the user for any excess funds he spent on the transaction:</li>
</ol>
<ul>
<li>Firstly, the postTransaction operation is called to the paymaster.</li>
<li>The bootloader asks the operator to provide a refund. During the first VM run without proofs the provide directly
inserts the refunds in the memory of the bootloader. During the run for the proved batches, the operator already knows
what which values have to be inserted there. You can read more about it in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/zkSync%20fee%20model.md">documentation</a>
of the fee model.</li>
<li>The bootloader refunds the user.</li>
</ul>
<ol>
<li>We notify the operator about the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1112">refund</a>
that was granted to the user. It will be used for the correct displaying of gasUsed for the transaction in explorer.</li>
</ol>
<h2 id="l1-l2-transactions"><a class="header" href="#l1-l2-transactions">L1-&gt;L2 transactions</a></h2>
<p>L1-&gt;L2 transactions are transactions that were initiated on L1. We assume that <code>from</code> has already authorized the L1→L2
transactions. It also has its L1 pubdata price as well as ergsPrice set on L1.</p>
<p>Most of the steps from the execution of L2 transactions are omitted and we set <code>tx.origin</code> to the <code>from</code>, and
<code>ergsPrice</code> to the one provided by transaction. After that, we use
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/advanced/0_alternative_vm_intro.md#zkevm-specific-opcodes">mimicCall</a>
to provide the operation itself from the name of the sender account.</p>
<p>Note, that for L1→L2 transactions, <code>reserved0</code> field denotes the amount of ETH that should be minted on L2 as a result
of this transaction. <code>reserved1</code> is the refund receiver address, i.e. the address that would receive the refund for the
transaction as well as the msg.value if the transaction fails.</p>
<p>There are two kinds of L1-&gt;L2 transactions:</p>
<ul>
<li>Priority operations, initiated by users (they have type <code>255</code>).</li>
<li>Upgrade transactions, that can be initiated during system upgrade (they have type <code>254</code>).</li>
</ul>
<p>You can read more about differences between those in the corresponding
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">document</a>.</p>
<h2 id="end-of-the-batch"><a class="header" href="#end-of-the-batch">End of the batch</a></h2>
<p>At the end of the batch we set <code>tx.origin</code> and <code>tx.gasprice</code> context variables to zero to save L1 gas on calldata and
send the entire bootloader balance to the operator, effectively sending fees to him.</p>
<p>Also, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3812">set</a>
the fictive L2 block’s data. Then, we call the system context to ensure that it publishes the timestamp of the L2 block
as well as L1 batch. We also reset the <code>txNumberInBlock</code> counter to avoid its state diffs from being published on L1.
You can read more about block processing on ZKsync
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20&amp;%20L2%20blocks%20on%20zkSync.md">here</a>.</p>
<p>After that, we publish the hash as well as the number of priority operations in this batch. More on it
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>Then, we call the L1Messenger system contract for it to compose the pubdata to be published on L1. You can read more
about the pubdata processing
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-fee-model"><a class="header" href="#zksync-fee-model">ZKsync fee model</a></h1>
<p>This document will assume that you already know how gas &amp; fees work on Ethereum.</p>
<p>On Ethereum, all the computational, as well as storage costs, are represented via one unit: gas. Each operation costs a
certain amount of gas, which is generally constant (though it may change during
<a href="https://blog.ethereum.org/2021/03/08/ethereum-berlin-upgrade-announcement">upgrades</a>).</p>
<p>ZKsync as well as other L2s have the issue which does not allow to adopt the same model as the one for Ethereum so
easily: the main reason is the requirement for publishing of the pubdata on Ethereum. This means that prices for L2
transactions will depend on the volatile L1 gas prices and can not be simply hardcoded.</p>
<h2 id="high-level-description"><a class="header" href="#high-level-description">High-level description</a></h2>
<p>ZKsync, being a zkRollup is required to prove every operation with zero knowledge proofs. That comes with a few nuances.</p>
<h3 id="gas_per_pubdata_limit"><a class="header" href="#gas_per_pubdata_limit"><code>gas_per_pubdata_limit</code></a></h3>
<p>As already mentioned, the transactions on ZKsync depend on volatile L1 gas costs to publish the pubdata for batch,
verify proofs, etc. For this reason, ZKsync-specific EIP712 transactions contain the <code>gas_per_pubdata_limit</code> field in
them, denoting the maximum price in <em>gas</em> that the operator **can charge from users for a single byte of pubdata.</p>
<p>For Ethereum transactions (which do not contain this field), it is enforced that the operator will not use a value
larger value than a certain constant.</p>
<h3 id="different-opcode-pricing"><a class="header" href="#different-opcode-pricing">Different opcode pricing</a></h3>
<p>The operations tend to have different “complexity”/”pricing” in zero knowledge proof terms than in standard CPU terms.
For instance, <code>keccak256</code> which was optimized for CPU performance, will cost more to prove.</p>
<p>That’s why you will find the prices for operations on ZKsync a lot different from the ones on Ethereum.</p>
<h3 id="different-intrinsic-costs"><a class="header" href="#different-intrinsic-costs">Different intrinsic costs</a></h3>
<p>Unlike Ethereum, where the intrinsic cost of transactions (<code>21000</code> gas) is used to cover the price of updating the
balances of the users, the nonce and signature verification, on ZKsync these prices are <em>not</em> included in the intrinsic
costs for transactions, due to the native support of account abstraction, meaning that each account type may have their
own transaction cost. In theory, some may even use more zk-friendly signature schemes or other kinds of optimizations to
allow cheaper transactions for their users.</p>
<p>That being said, ZKsync transactions do come with some small intrinsic costs, but they are mostly used to cover costs
related to the processing of the transaction by the bootloader which can not be easily measured in code in real-time.
These are measured via testing and are hard coded.</p>
<h3 id="batch-overhead--limited-resources-of-the-batch"><a class="header" href="#batch-overhead--limited-resources-of-the-batch">Batch overhead &amp; limited resources of the batch</a></h3>
<p>In order to process the batch, the ZKsync team has to pay for proving of the batch, committing to it, etc. Processing a
batch involves some operational costs as well. All of these values we call “Batch overhead”. It consists of two parts:</p>
<ul>
<li>The L2 requirements for proving the circuits (denoted in L2 gas).</li>
<li>The L1 requirements for the proof verification as well as general batch processing (denoted in L1 gas).</li>
</ul>
<p>We generally try to aggregate as many transactions as possible and each transaction pays for the batch overhead
proportionally to how close did the transaction bring the batch to being <em>sealed,</em> i.e. closed and prepared for proof
verification and submission on L1. A transaction gets closer to sealing a batch by using the batch’s <em>limited
resources</em>.</p>
<p>While on Ethereum, the main reason for the existence of batch gas limit is to keep the system decentralized &amp; load low,
i.e. assuming the existence of the correct hardware, only time would be a requirement for a batch to adhere to. In the
case of ZKsync batches, there are some limited resources the batch should manage:</p>
<ul>
<li><strong>Time.</strong> The same as on Ethereum, the batch should generally not take too much time to be closed in order to provide
better UX. To represent the time needed we use a batch gas limit, note that it is higher than the gas limit for a
single transaction.</li>
<li><strong>Slots for transactions.</strong> The bootloader has a limited number of slots for transactions, i.e. it can not take more
than a certain transactions per batch.</li>
<li><strong>The memory of the bootloader.</strong> The bootloader needs to store the transaction’s ABI encoding in its memory &amp; this
fills it up. In practical terms, it serves as a penalty for having transactions with large calldata/signatures in case
of custom accounts.</li>
<li><strong>Pubdata bytes.</strong> In order to fully appreciate the gains from the storage diffs, i.e. the fact that changes in a
single slot happening in the same batch need to be published only once, we need to publish all the batch’s public data
only after the transaction has been processed. Right now, we publish all the data with the storage diffs as well as
L2→L1 messages, etc in a single transaction at the end of the batch. Most nodes have limit of 128kb per transaction
and so this is the limit that each ZKsync batch should adhere to.</li>
</ul>
<p>Each transaction spends the batch overhead proportionally to how close it consumes the resources above.</p>
<p>Note, that before the transaction is executed, the system can not know how many of the limited system resources the
transaction will actually take, so we need to charge for the worst case and provide the refund at the end of the
transaction.</p>
<h3 id="how-basefee-works-on-zksync"><a class="header" href="#how-basefee-works-on-zksync">How <code>baseFee</code> works on ZKsync</a></h3>
<p>In order to protect us from DDoS attacks we need to set a limited <code>MAX_TRANSACTION_GAS_LIMIT</code> per transaction. Since the
computation costs are relatively constant for us, we <em>could</em> use a “fair” <code>baseFee</code> equal to the real costs for us to
compute the proof for the corresponding 1 erg. Note, that <code>gas_per_pubdata_limit</code> should be then set high enough to
cover the fees for the L1 gas needed to send a single pubdata byte on Ethereum. Under large L1 gas,
<code>gas_per_pubdata_limit</code> would also need be large. That means that <code>MAX_TRANSACTION_GAS_LIMIT/gas_per_pubdata_limit</code>
could become too low to allow for enough pubdata for lots of common use cases.</p>
<p>To make common transactions always executable, we must enforce that the users are always able to send at least
<code>GUARANTEED_PUBDATA_PER_TX</code> bytes of pubdata in their transaction. Because of that, the needed <code>gas_per_pubdata_limit</code>
for transactions should never grow beyond <code>MAX_TRANSACTION_GAS_LIMIT/GUARANTEED_PUBDATA_PER_TX</code>. Setting a hard bound on
<code>gas_per_pubdata_limit</code> also means that with the growth of L1 gas prices, the L2 <code>baseFee</code> will have to grow as well (to
ensure that <code>base_fee * gas_per_pubdata_limit = L1_gas_price * l1_gas_per_pubdata)</code>).</p>
<p>This does not actually matter a lot for normal transactions, since most of the costs will still go on pubdata for them.
However, it may matter for computationally intensive tasks, meaning that for them a big upfront payment will be
required, with the refund at the end of the transaction for all the overspent gas.</p>
<h3 id="trusted-gas-limit"><a class="header" href="#trusted-gas-limit">Trusted gas limit</a></h3>
<p>While it was mentioned above that the <code>MAX_TRANSACTION_GAS_LIMIT</code> is needed to protect the operator from users stalling
the state keeper by using too much computation, in case the users may need to use a lot of pubdata (for instance to
publish the bytecode of a new contract), the required gasLimit may go way beyond the <code>MAX_TRANSACTION_GAS_LIMIT</code> (since
the contracts can be 10s of kilobytes in size). All the new contracts to be published are included as part of the
factory dependencies field of the transaction and so the operator already knows how much pubdata will have to published
&amp; how much gas will have to spent on it.</p>
<p>That’s why, to provide the better UX for users, the operator may provide the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L1137">trusted gas limit</a>,
i.e. the limit which exceeds <code>MAX_TRANSACTION_GAS_LIMIT</code> assuming that the operator knows what he is doing (e.g. he is
sure that the excess gas will be spent on the pubdata).</p>
<h3 id="high-level-conclusion"><a class="header" href="#high-level-conclusion">High-level: conclusion</a></h3>
<p>The ZKsync fee model is meant to be the basis of the long-term fee model, which provides both robustness and security.
One of the most distinctive parts of it is the existing of the batch overhead, which is proportional for the resources
consumed by the transaction.</p>
<p>The other distinctive feature of the fee model used on ZKsync is the abundance of refunds, i.e.:</p>
<ul>
<li>For unused limited system resources.</li>
<li>For overpaid computation.</li>
</ul>
<p>This is needed because of the relatively big upfront payments required in ZKsync to provide DDoS security.</p>
<h2 id="formalization"><a class="header" href="#formalization">Formalization</a></h2>
<p>After determining price for each opcode in gas according to the model above, the following formulas are to be used for
calculating <code>baseFee</code> and <code>gasPerPubdata</code> for a batch.</p>
<h4 id="system-wide-constants"><a class="header" href="#system-wide-constants">System-wide constants</a></h4>
<p>These constants are to be hardcoded and can only be changed via either system contracts/bootloader or VM upgrade.</p>
<p><code>BATCH_OVERHEAD_L1_GAS</code> (<em>L</em>1<em>O</em>)— The L1 gas overhead for a batch (proof verification, etc).</p>
<p><code>L1_GAS_PER_PUBDATA_BYTE</code> (<em>L</em>1<em>PUB</em>) — The number of L1 gas needed for a single pubdata byte. It is slightly higher
than 16 gas needed for publishing a non-zero byte of pubdata on-chain (currently the value of 17 is used).</p>
<p><code>BATCH_OVERHEAD_L2_GAS</code> (<em>EO</em>)— The constant overhead denominated in gas. This overhead is created to cover the
amortized costs of proving.</p>
<p><code>BLOCK_GAS_LIMIT</code> (<em>B</em>) — The maximum number of computation gas per batch. This is the maximal number of gas that can be
spent within a batch. This constant is rather arbitrary and is needed to prevent transactions from taking too much time
from the state keeper. It can not be larger than the hard limit of 2^32 of gas for VM.</p>
<p><code>MAX_TRANSACTION_GAS_LIMIT</code> (<em>TM</em>) — The maximal transaction gas limit. For <em>i</em>-th single instance circuit, the price of
each of its units is $SC_i = \lceil \frac{T_M}{CC_i} \rceil$ to ensure that no transaction can run out of these single
instance circuits.</p>
<p><code>MAX_TRANSACTIONS_IN_BATCH</code> (<em>TXM</em>) — The maximum number of transactions per batch. A constant in bootloader. Can
contain almost any arbitrary value depending on the capacity of batch that we want to have.</p>
<p><code>BOOTLOADER_MEMORY_FOR_TXS</code> (<em>BM</em>) — The size of the bootloader memory that is used for transaction encoding
(i.e. excluding the constant space, preallocated for other purposes).</p>
<p><code>GUARANTEED_PUBDATA_PER_TX</code> (<em>PG</em>) — The guaranteed number of pubdata that should be possible to pay for in one ZKsync
batch. This is a number that should be enough for most reasonable cases.</p>
<h4 id="derived-constants"><a class="header" href="#derived-constants">Derived constants</a></h4>
<p>Some of the constants are derived from the system constants above:</p>
<p><code>MAX_GAS_PER_PUBDATA</code> (<em>EPMax</em>) — the <code>gas_price_per_pubdata</code> that should always be enough to cover for publishing a
pubdata byte:</p>
<p>$$
EP_{Max} = \lfloor \frac{T_M}{P_G} \rfloor
$$</p>
<h4 id="externally-provided-batch-parameters"><a class="header" href="#externally-provided-batch-parameters">Externally-provided batch parameters</a></h4>
<p><code>L1_GAS_PRICE</code> (<em>L</em>1<em>P</em>) — The price for L1 gas in ETH.</p>
<p><code>FAIR_GAS_PRICE</code> (<em>Ef</em>) — The “fair” gas price in ETH, that is, the price of proving one circuit (in Ether) divided by
the number we chose as one circuit price in gas.</p>
<p>$$
E_f = \frac{Price_C}{E_C}
$$</p>
<p>where <em>PriceC</em> is the price for proving a circuit in ETH. Even though this price will generally be volatile (due to the
volatility of ETH price), the operator is discouraged to change it often, because it would volatile both volatile gas
price and (most importantly) the required <code>gas_price_per_pubdata</code> for transactions.</p>
<p>Both of the values above are currently provided by the operator. Later on, some decentralized/deterministic way to
provide these prices will be utilized.</p>
<h4 id="determining-base_fee"><a class="header" href="#determining-base_fee">Determining base_fee</a></h4>
<p>When the batch opens, we can calculate the <code>FAIR_GAS_PER_PUBDATA_BYTE</code> (<em>EPf</em>) — “fair” gas per pubdata byte:</p>
<p>$$
EP_f = \lceil \frac{L1_p * L1_{PUB}}{E_f} \rceil
$$</p>
<p>There are now two situations that can be observed:</p>
<p>I.</p>
<p>$$
EP_f &gt; EP_{Max}
$$</p>
<p>This means that the L1 gas price is so high that if we treated all the prices fairly, then the number of gas required to
publish guaranteed pubdata is too high, i.e. allowing at least <em>PG</em> pubdata bytes per transaction would mean that we
would to support <em>tx</em>.<em>gasLimit</em> greater that the maximum gas per transaction <em>TM</em>, allowing to run out of other finite
resources.</p>
<p>If $EP_f &gt; EP_{Max}$, then the user needs to artificially increase the provided <em>Ef</em> to bring the needed
<em>tx</em>.<em>gasPerPubdataByte</em> to <em>EPmax</em></p>
<p>In this case we set the EIP1559 <code>baseFee</code> (<em>Base</em>):</p>
<p>$$
Base = max(E_f, \lceil \frac{L1_P * L1_{PUB}}{EP_{max}} \rceil)
$$</p>
<p>Only transactions that have at least this high gasPrice will be allowed into the batch.</p>
<p>II.</p>
<p>Otherwise, we keep $Base* = E_f$</p>
<p>Note, that both cases are covered with the formula in case (1), i.e.:</p>
<p>$$
Base = max(E_f, \lceil \frac{L1_P * L1_{PUB}}{EP_{max}} \rceil)
$$</p>
<p>This is the base fee that will be always returned from the API via <code>eth_gasGasPrice</code>.</p>
<h4 id="calculating-overhead-for-a-transaction"><a class="header" href="#calculating-overhead-for-a-transaction">Calculating overhead for a transaction</a></h4>
<p>Let’s define by <em>tx</em>.<em>actualGasLimit</em> as the actual gasLimit that is to be used for processing of the transaction
(including the intrinsic costs). In this case, we will use the following formulas for calculating the upfront payment
for the overhead:</p>
<p>$$
S_O = 1/TX_M
$$</p>
<p>$$
M_O(tx) = encLen(tx) / B_M
$$</p>
<p>$$
E_{AO}(tx) = tx.actualGasLimit / T_M
$$</p>
<p>$$
O(tx) = max(S_O, M_O(tx), E_O(tx))
$$</p>
<p>where:</p>
<p><em>SO</em> — is the overhead for taking up 1 slot for a transaction</p>
<p><em>MO</em>(<em>tx</em>) — is the overhead for taking up the memory of the bootloader</p>
<p><em>encLen</em>(<em>tx</em>) — the length of the ABI encoding of the transaction’s struct.</p>
<p><em>EAO</em>(<em>tx</em>) — is the overhead for potentially taking up the gas for single instance circuits.</p>
<p><em>O</em>(<em>tx</em>) — is the total share of the overhead that the transaction should pay for.</p>
<p>Then we can calculate the overhead that the transaction should pay as the following one:</p>
<p>$$
L1_O(tx) = \lceil \frac{L1_O}{L1_{PUB}} \rceil * O(tx) \
E_O(tx) = E_O * O(tx)
$$</p>
<p>Where</p>
<p><em>L</em>1<em>O</em>(<em>tx</em>) — the number of L1 gas overhead (in pubdata equivalent) the transaction should compensate for gas.</p>
<p><em>EO</em>(<em>tx</em>) — the number of L2 gas overhead the transaction should compensate for.</p>
<p>Then:</p>
<p><em>overhead</em>_<em>gas</em>(<em>tx</em>) = <em>EO</em>(<em>tx</em>) + <em>tx</em>.<em>gasPerPubdata</em> ⋅ <em>L</em>1<em>O</em>(<em>tx</em>)</p>
<p>When a transaction is being estimated, the server returns the following gasLimit:</p>
<p><em>tx</em>.<em>gasLimit</em> = <em>tx</em>.<em>actualGasLimit</em> + <em>overhead</em>_<em>gas</em>(<em>tx</em>)</p>
<p>Note, that when the operator receives the transaction, it knows only <em>tx</em>.<em>gasLimit</em>. The operator could derive the
<em>overhead<strong><em>gas</em>(<em>tx</em>) and provide the bootloader with it. The bootloader will then derive
<em>tx</em>.<em>actualGasLimit</em> = <em>tx</em>.<em>gasLimit</em> − <em>overhead</em></strong>gas</em>(<em>tx</em>) and use the formulas above to derive the overhead that
the user should’ve paid under the derived <em>tx</em>.<em>actualGasLimit</em> to ensure that the operator does not overcharge the
user.</p>
<h4 id="note-on-formulas"><a class="header" href="#note-on-formulas">Note on formulas</a></h4>
<p>For the ease of integer calculation, we will use the following formulas to derive the <em>overhead</em>(<em>tx</em>):</p>
<p>$B_O(tx) = E_O + tx.gasPerPubdataByte \cdot \lfloor \frac{L1_O}{L1_{PUB}} \rfloor$</p>
<p>$B_O$ denotes the overhead for batch in gas that the transaction would have to pay if it consumed the resources for
entire batch.</p>
<p>Then, <em>overhead</em>_<em>gas</em>(<em>tx</em>) is the maximum of the following expressions:</p>
<ol>
<li>$S_O = \lceil \frac{B_O}{TX_M} \rceil$</li>
<li>$M_O(tx) = \lceil \frac{B_O \cdot encodingLen(tx)}{B_M} \rceil$</li>
<li>$E_O(tx) = \lceil \frac{B_O \cdot tx.gasBodyLimit}{T_M} \rceil$</li>
</ol>
<h4 id="deriving-overhead_gastx-from-txgaslimit"><a class="header" href="#deriving-overhead_gastx-from-txgaslimit">Deriving <code>overhead_gas(tx)</code> from <code>tx.gasLimit</code></a></h4>
<p>The task that the operator needs to do is the following:</p>
<p>Given the tx.gasLimit, it should find the maximal <code>overhead_gas(tx)</code>, such that the bootloader will accept such
transaction, that is, if we denote by <em>Oop</em> the overhead proposed by the operator, the following equation should hold:</p>
<p>$$
O_{op} ≤ overhead_gas(tx)
$$</p>
<p>for the $tx.bodyGasLimit$ we use the $tx.bodyGasLimit$ = $tx.gasLimit − O_{op}$.</p>
<p>There are a few approaches that could be taken here:</p>
<ul>
<li>Binary search. However, we need to be able to use this formula for the L1 transactions too, which would mean that
binary search is too costly.</li>
<li>The analytical way. This is the way that we will use and it will allow us to find such an overhead in O(1), which is
acceptable for L1-&gt;L2 transactions.</li>
</ul>
<p>Let’s rewrite the formula above the following way:</p>
<p>$$
O_{op} ≤ max(SO, MO(tx), EO(tx))
$$</p>
<p>So we need to find the maximal $O_{op}$, such that $O_{op} ≤ max(S_O, M_O(tx), E_O(tx))$. Note, that it means ensuring
that at least one of the following is true:</p>
<ol>
<li>$O_{op} ≤ S_O$</li>
<li>$O_{op} ≤ M_O(tx)$</li>
<li>$O_{op} ≤ E_O(tx)$</li>
</ol>
<p>So let’s find the largest <em>Oop</em> for each of these and select the maximum one.</p>
<ul>
<li>Solving for (1)</li>
</ul>
<p>$$
O_{op} = \lceil \frac{B_O}{TX_M} \rceil
$$</p>
<ul>
<li>Solving for (2)</li>
</ul>
<p>$$
O_{op} = \lceil \frac{encLen(tx) \cdot B_O}{B_M} \rceil
$$</p>
<ul>
<li>Solving for (3)</li>
</ul>
<p>This one is somewhat harder than the previous ones. We need to find the largest <em>O_{op}</em>, such that:</p>
<p>$$
O_{op} \le \lceil \frac{tx.actualErgsLimit \cdot B_O}{T_M} \rceil   \
$$</p>
<p>$$
O_{op} \le \lceil \frac{(tx.ergsLimit - O_{op}) \cdot B_O}{T_M} \rceil   \
$$</p>
<p>$$
O_{op}  ≤ \lceil \frac{B_O \cdot (tx.ergsLimit - O_{op})}{T_M} \rceil
$$</p>
<p>Note, that all numbers here are integers, so we can use the following substitution:</p>
<p>$$
O_{op} -1 \lt \frac{(tx.ergsLimit - O_{op}) \cdot B_O}{T_M}    \
$$</p>
<p>$$
(O_{op} -1)T_M \lt (tx.ergsLimit - O_{op}) \cdot B_O    \
$$</p>
<p>$$
O_{op} T_M + O_{op} B_O \lt tx.ergsLimit \cdot B_O + T_M    \
$$</p>
<p>$$
O_{op} \lt \frac{tx.ergsLimit \cdot B_O + T_M}{B_O + T_M}    \
$$</p>
<p>Meaning, in other words:</p>
<p>$$
O_{op} = \lfloor \frac{tx.ergsLimit \cdot B_O + T_M - 1}{B_O + T_M} \rfloor
$$</p>
<p>Then, the operator can safely choose the largest one.</p>
<h4 id="discounts-by-the-operator"><a class="header" href="#discounts-by-the-operator">Discounts by the operator</a></h4>
<p>It is important to note that the formulas provided above are to withstand the worst-case scenario and these are the
formulas used for L1-&gt;L2 transactions (since these are forced to be processed by the operator). However, in reality, the
operator typically would want to reduce the overhead for users whenever it is possible. For instance, in the server, we
underestimate the maximal potential <code>MAX_GAS_PER_TRANSACTION</code>, since usually the batches are closed because of either
the pubdata limit or the transactions’ slots limit. For this reason, the operator also provides the operator’s proposed
overhead. The only thing that the bootloader checks is that this overhead is <em>not larger</em> than the maximal required one.
But the operator is allowed to provide a lower overhead.</p>
<h4 id="refunds"><a class="header" href="#refunds">Refunds</a></h4>
<p>As you could see above, this fee model introduces quite some places where users may overpay for transactions:</p>
<ul>
<li>For the pubdata when L1 gas price is too low</li>
<li>For the computation when L1 gas price is too high</li>
<li>The overhead, since the transaction may not use the entire batch resources they could.</li>
</ul>
<p>To compensate users for this, we will provide refunds for users. For all of the refunds to be provable, the counter
counts the number of gas that was spent on pubdata (or the number of pubdata bytes published). We will denote this
number by <em>pubdataused</em>. For now, this value can be provided by the operator.</p>
<p>The fair price for a transaction is</p>
<p>$$
FairFee = E_f \cdot tx.computationalGas + EP_f \cdot pubdataused
$$</p>
<p>We can derive $tx.computationalGas = gasspent − pubdataused \cdot tx.gasPricePerPubdata$, where <em>gasspent</em> is the number
of gas spent for the transaction (can be trivially fetched in Solidity).</p>
<p>Also, the <em>FairFee</em> will include the actual overhead for batch that the users should pay for.</p>
<p>The fee that the user has actually spent is:</p>
<p>$$
ActualFee = gasspent \cdot gasPrice
$$</p>
<p>So we can derive the overpay as</p>
<p>$$
ActualFee − FairFee
$$</p>
<p>In order to keep the invariant of $gasUsed \cdot gasPrice = fee$ , we will formally refund
$\frac{ActualFee - FairFee}{Base}$ gas.</p>
<p>At the moment, this counter is not accessible within the VM and so the operator is free to provide any refund it wishes
(as long as it is greater than or equal to the actual amount of gasLeft after the transaction execution).</p>
<h4 id="refunds-for-repeated-writes"><a class="header" href="#refunds-for-repeated-writes">Refunds for repeated writes</a></h4>
<p>zkEVM is a statediff-based rollup, i.e. the pubdata is published not for transactions, but for storage changes. This
means that whenever a user writes into a storage slot, he incurs certain amount of pubdata. However, not all writes are
equal:</p>
<ul>
<li>If a slot has been already written to in one of the previous batches, the slot has received a short id, which allows
it to require less pubdata in the state diff.</li>
<li>Depending on the <code>value</code> written into a slot, various compression optimizations could be used and so we should reflect
that too.</li>
<li>Maybe the slot has been already written to in this batch and so we don’t to charge anything for it.</li>
</ul>
<p>You can read more about how we treat the pubdata
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</p>
<p>The important part here is that while such refunds are inlined (i.e. unlike the refunds for overhead they happen
in-place during execution and not after the whole transaction has been processed), they are enforced by the operator.
Right now, the operator is the one that decides what refund to provide.</p>
<h2 id="improvements-in-the-upcoming-releases"><a class="header" href="#improvements-in-the-upcoming-releases">Improvements in the upcoming releases</a></h2>
<p>The fee model explained above, while fully functional, has some known issues. These will be tackled with the following
upgrades.</p>
<h3 id="the-quadratic-overhead-for-pubdata"><a class="header" href="#the-quadratic-overhead-for-pubdata">The quadratic overhead for pubdata</a></h3>
<p>Note, that the computational overhead is proportional to the <code>tx.gasLimit</code> and the amount of funds the user will have to
pay is proportional to the L1 gas price (recall the formula of <code>B_O</code>). We can roughly express the transaction overhead
from computation as <code>tx.gasLimit * L1_GAS_PRICE * C</code> where <code>C</code> is just some constant. Note, that since a transaction
typically contains some storage writes, and its
<code>tx.gasLimit = gasSpentOnCompute + pubdataPublished * gasPricePerPubdata</code>, <code>tx.gasLimit</code> is roughly proportional to
<code>gasPricePerPubdata</code> and so it is also proportional to <code>L1_GAS_PRICE</code>.</p>
<p>This means that formula <code>tx.gasLimit * L1_GAS_PRICE * C</code> becomes <em>quadratic</em> to the <code>L1_GAS_PRICE</code>.</p>
<h3 id="gasused-depends-to-gaslimit"><a class="header" href="#gasused-depends-to-gaslimit"><code>gasUsed</code> depends to <code>gasLimit</code></a></h3>
<p>While in general it shouldn’t be the case assuming the correct implementation of <a href="specs/zk_evm/fee_model.html#refunds">refunds</a>, in practice it
turned out that the formulas above, while robust, estimate for the worst case which can be very difference from the
average one. In order to improve the UX and reduce the overhead, the operator charges less for the execution overhead.
However, as a compensation for the risk, it does not fully refund for it.</p>
<h3 id="l1-l2-transactions-do-not-pay-for-their-execution-on-l1"><a class="header" href="#l1-l2-transactions-do-not-pay-for-their-execution-on-l1">L1-&gt;L2 transactions do not pay for their execution on L1</a></h3>
<p>The <code>executeBatches</code> operation on L1 is executed in <code>O(N)</code> where N is the number of priority ops that we have in the
batch. Each executed priority operation will be popped and so it incurs cost for storage modifications. As of now, we do
not charge for it.</p>
<h2 id="zkevm-fee-components-revenue--costs"><a class="header" href="#zkevm-fee-components-revenue--costs">zkEVM Fee Components (Revenue &amp; Costs)</a></h2>
<ul>
<li>On-Chain L1 Costs
<ul>
<li>L1 Commit Batches
<ul>
<li>The commit batches transaction submits pubdata (which is the list of updated storage slots) to L1. The cost of a
commit transaction is calculated as <code>constant overhead + price of pubdata</code>. The <code>constant overhead</code> cost is evenly
distributed among L2 transactions in the L1 commit transaction, but only at higher transaction loads. As for the
<code>price of pubdata</code>, it is known how much pubdata each L2 transaction consumed, therefore, they are charged
directly for that. Multiple L1 batches can be included in a single commit transaction.</li>
</ul>
</li>
<li>L1 Prove Batches
<ul>
<li>Once the off-chain proof is generated, it is submitted to L1 to make the rollup batch final. Currently, each proof
contains only one L1 batch.</li>
</ul>
</li>
<li>L1 Execute Batches
<ul>
<li>The execute batches transaction processes L2 -&gt; L1 messages and marks executed priority operations as such.
Multiple L1 batches can be included in a single execute transaction.</li>
</ul>
</li>
<li>L1 Finalize Withdrawals
<ul>
<li>While not strictly part of the L1 fees, the cost to finalize L2 → L1 withdrawals are covered by Matter Labs. The
finalize withdrawals transaction processes user token withdrawals from zkEVM to Ethereum. Multiple L2 withdrawal
transactions are included in each finalize withdrawal transaction.</li>
</ul>
</li>
</ul>
</li>
<li>On-Chain L2 Revenue
<ul>
<li>L2 Transaction Fee
<ul>
<li>This fee is what the user pays to complete a transaction on zkEVM. It is calculated as
<code>gasLimit x baseFeePerGas - refundedGas x baseFeePerGas</code>, or more simply, <code>gasUsed x baseFeePerGas</code>.</li>
</ul>
</li>
</ul>
</li>
<li>Profit = L2 Revenue - L1 Costs - Off Chain Infrastructure Costs</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="precompiles"><a class="header" href="#precompiles">Precompiles</a></h1>
<p>Precompiled contracts for elliptic curve operations are required in order to perform zkSNARK verification.</p>
<p>The operations that you need to be able to perform are elliptic curve point addition, elliptic curve point scalar
multiplication, and elliptic curve pairing.</p>
<p>This document explains the precompiles responsible for elliptic curve point addition and scalar multiplication and the
design decisions. You can read the specification <a href="https://eips.ethereum.org/EIPS/eip-196">here</a>.</p>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<p>On top of having a set of opcodes to choose from, the EVM also offers a set of more advanced functionalities through
precompiled contracts. These are a special kind of contracts that are bundled with the EVM at fixed addresses and can be
called with a determined gas cost. The addresses start from 1, and increment for each contract. New hard forks may
introduce new precompiled contracts. They are called from the opcodes like regular contracts, with instructions like
CALL. The gas cost mentioned here is purely the cost of the contract and does not consider the cost of the call itself
nor the instructions to put the parameters in memory.</p>
<p>For Go-Ethereum, the code being run is written in Go, and the gas costs are defined in each precompile spec.</p>
<p>In the case of ZKsync Era, ecAdd and ecMul precompiles are written as a smart contract for two reasons:</p>
<ul>
<li>zkEVM needs to be able to prove their execution (and at the moment it cannot do that if the code being run is executed
outside the VM)</li>
<li>Writing custom circuits for Elliptic curve operations is hard, and time-consuming, and after all such code is harder
to maintain and audit.</li>
</ul>
<h2 id="field-arithmetic"><a class="header" href="#field-arithmetic">Field Arithmetic</a></h2>
<p>The BN254 (also known as alt-BN128) is an elliptic curve defined by the equation $y^2 = x^3 + 3$ over the finite field
$\mathbb{F}_p$, being $p = 218882428718392752222464057452572750886963111572978236626890378946452262$08583. The modulus
is less than 256 bits, which is why every element in the field is represented as a <code>uint256</code>.</p>
<p>The arithmetic is carried out with the field elements encoded in the Montgomery form. This is done not only because
operating in the Montgomery form speeds up the computation but also because the native modular multiplication, which is
carried out by Yul’s <code>mulmod</code> opcode, is very inefficient.</p>
<p>Instructions set on ZKsync and EVM are different, so the performance of the same Yul/Solidity code can be efficient on
EVM, but not on zkEVM and opposite.</p>
<p>One such very inefficient command is <code>mulmod</code>. On EVM there is a native opcode that makes modulo multiplication and it
costs only 8 gas, which compared to the other opcodes costs is only 2-3 times more expensive. On zkEVM we don’t have
native <code>mulmod</code> opcode, instead, the compiler does full-with multiplication (e.g. it multiplies two <code>uint256</code>s and gets
as a result an <code>uint512</code>). Then the compiler performs long division for reduction (but only the remainder is kept), in
the generic form it is an expensive operation and costs many opcode executions, which can’t be compared to the cost of
one opcode execution. The worst thing is that <code>mulmod</code> is used a lot for the modulo inversion, so optimizing this one
opcode gives a huge benefit to the precompiles.</p>
<h3 id="multiplication"><a class="header" href="#multiplication">Multiplication</a></h3>
<p>As said before, multiplication was carried out by implementing the Montgomery reduction, which works with general moduli
and provides a significant speedup compared to the naïve approach.</p>
<p>The squaring operation is obtained by multiplying a number by itself. However, this operation can have an additional
speedup by implementing the SOS Montgomery squaring.</p>
<h3 id="inversion"><a class="header" href="#inversion">Inversion</a></h3>
<p>Inversion was performed using the extended binary Euclidean algorithm (also known as extended binary greatest common
divisor). This algorithm is a modification of Algorithm 3 <code>MontInvbEEA</code> from
<a href="https://cetinkayakoc.net/docs/j82.pdf">Montgomery inversion</a>.</p>
<h3 id="exponentiation"><a class="header" href="#exponentiation">Exponentiation</a></h3>
<p>The exponentiation was carried out using the square and multiply algorithm, which is a standard technique for this
operation.</p>
<h2 id="montgomery-form"><a class="header" href="#montgomery-form">Montgomery Form</a></h2>
<p>Let’s take a number <code>R</code>, such that <code>gcd(N, R) == 1</code> and <code>R</code> is a number by which we can efficiently divide and take
module over it (for example power of two or better machine word, aka 2^256). Then transform every number to the form of
<code>x * R mod N</code> / <code>y * R mod N</code> and then we get efficient modulo addition and multiplication. The only thing is that
before working with numbers we need to transform them to the form from <code>x mod N</code> to the <code>x * R mod N</code> and after
performing operations transform the form back.</p>
<p>For the latter, we will assume that <code>N</code> is the module that we use in computations, and <code>R</code> is $2^{256}$, since we can
efficiently divide and take module over this number and it practically satisfies the property of <code>gcd(N, R) == 1</code>.</p>
<h3 id="montgomery-reduction-algorithm-redc"><a class="header" href="#montgomery-reduction-algorithm-redc">Montgomery Reduction Algorithm (REDC)</a></h3>
<blockquote>
<p>Reference: <a href="https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_REDC_algorithm">https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_REDC_algorithm</a></p>
</blockquote>
<pre><code class="language-solidity">/// @notice Implementation of the Montgomery reduction algorithm (a.k.a. REDC).
/// @dev See &lt;https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_REDC_algorithm&gt;
/// @param lowestHalfOfT The lowest half of the value T.
/// @param higherHalfOfT The higher half of the value T.
/// @return S The result of the Montgomery reduction.
function REDC(lowestHalfOfT, higherHalfOfT) -&gt; S {
    let q := mul(lowestHalfOfT, N_PRIME())
    let aHi := add(higherHalfOfT, getHighestHalfOfMultiplication(q, P()))
    let aLo, overflowed := overflowingAdd(lowestHalfOfT, mul(q, P()))
    if overflowed {
        aHi := add(aHi, 1)
    }
    S := aHi
    if iszero(lt(aHi, P())) {
        S := sub(aHi, P())
    }
}

</code></pre>
<p>By choosing $R = 2^{256}$ we avoided 2 modulo operations and one division from the original algorithm. This is because
in Yul, native numbers are uint256 and the modulo operation is native, but for the division, as we work with a 512-bit
number split into two parts (high and low part) dividing by $R$ means shifting 256 bits to the right or what is the
same, discarding the low part.</p>
<h3 id="montgomery-additionsubtraction"><a class="header" href="#montgomery-additionsubtraction">Montgomery Addition/Subtraction</a></h3>
<p>Addition and subtraction in Montgomery form are the same as ordinary modular addition and subtraction because of the
distributive law</p>
<p>$$
\begin{align*}
aR+bR=(a+b)R,\
aR-bR=(a-b)R.
\end{align*}
$$</p>
<pre><code class="language-solidity">/// @notice Computes the Montgomery addition.
/// @dev See &lt;https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_The_REDC_algorithm&gt; for further details on the Montgomery multiplication.
/// @param augend The augend in Montgomery form.
/// @param addend The addend in Montgomery form.
/// @return ret The result of the Montgomery addition.
function montgomeryAdd(augend, addend) -&gt; ret {
    ret := add(augend, addend)
    if iszero(lt(ret, P())) {
        ret := sub(ret, P())
    }
}

/// @notice Computes the Montgomery subtraction.
/// @dev See &lt;https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_The_REDC_algorithm&gt; for further details on the Montgomery multiplication.
/// @param minuend The minuend in Montgomery form.
/// @param subtrahend The subtrahend in Montgomery form.
/// @return ret The result of the Montgomery addition.
function montgomerySub(minuend, subtrahend) -&gt; ret {
    ret := montgomeryAdd(minuend, sub(P(), subtrahend))
}

</code></pre>
<p>We do not use <code>addmod</code>. That’s because in most cases the sum does not exceed the modulus.</p>
<h3 id="montgomery-multiplication"><a class="header" href="#montgomery-multiplication">Montgomery Multiplication</a></h3>
<p>The product of $aR \mod N$ and $bR \mod N$ is $REDC((aR \mod N)(bR \mod N))$.</p>
<pre><code class="language-solidity">/// @notice Computes the Montgomery multiplication using the Montgomery reduction algorithm (REDC).
/// @dev See &lt;https://en.wikipedia.org/wiki/Montgomery_modular_multiplication#The_The_REDC_algorithm&gt; for further details on the Montgomery multiplication.
/// @param multiplicand The multiplicand in Montgomery form.
/// @param multiplier The multiplier in Montgomery form.
/// @return ret The result of the Montgomery multiplication.
function montgomeryMul(multiplicand, multiplier) -&gt; ret {
    let hi := getHighestHalfOfMultiplication(multiplicand, multiplier)
    let lo := mul(multiplicand, multiplier)
    ret := REDC(lo, hi)
}

</code></pre>
<h3 id="montgomery-inversion"><a class="header" href="#montgomery-inversion">Montgomery Inversion</a></h3>
<pre><code class="language-solidity">/// @notice Computes the Montgomery modular inverse skipping the Montgomery reduction step.
/// @dev The Montgomery reduction step is skipped because a modification in the binary extended Euclidean algorithm is used to compute the modular inverse.
/// @dev See the function `binaryExtendedEuclideanAlgorithm` for further details.
/// @param a The field element in Montgomery form to compute the modular inverse of.
/// @return invmod The result of the Montgomery modular inverse (in Montgomery form).
function montgomeryModularInverse(a) -&gt; invmod {
    invmod := binaryExtendedEuclideanAlgorithm(a)
}
</code></pre>
<p>As said before, we use a modified version of the bEE algorithm that lets us “skip” the Montgomery reduction step.</p>
<p>The regular algorithm would be $REDC((aR \mod N)^{−1}(R^3 \mod N))$ which involves a regular inversion plus a
multiplication by a value that can be precomputed.</p>
<h2 id="ecadd"><a class="header" href="#ecadd">ECADD</a></h2>
<p>Precompile for computing elliptic curve point addition. The points are represented in affine form, given by a pair of
coordinates $(x,y)$.</p>
<p>Affine coordinates are the conventional way of expressing elliptic curve points, which use 2 coordinates. The math is
concise and easy to follow.</p>
<p>For a pair of constants $a$ and $b$, an elliptic curve is defined by the set of all points $(x,y)$ that satisfy the
equation $y^2=x^3+ax+b$, plus a special “point at infinity” named $O$.</p>
<h3 id="point-doubling"><a class="header" href="#point-doubling">Point Doubling</a></h3>
<p>To compute $2P$ (or $P+P$), there are three cases:</p>
<ul>
<li>If $P = O$, then $2P = O$.</li>
<li>Else $P = (x, y)$
<ul>
<li>If $y = 0$, then $2P = O$.</li>
<li>Else $y≠0$, then
$$
\begin{gather*} \lambda = \frac{3x_{p}^{2} + a}{2y_{p}} \ x_{r} = \lambda^{2} - 2x_{p} \ y_{r} = \lambda(x_{p} - x_{r}) - y_{p}\end{gather*}
$$</li>
</ul>
</li>
</ul>
<p>The complicated case involves approximately 6 multiplications, 4 additions/subtractions, and 1 division. There could
also be 4 multiplications, 6 additions/subtractions, and 1 division, and if you want you could trade a multiplication
with 2 more additions.</p>
<h3 id="point-addition"><a class="header" href="#point-addition">Point Addition</a></h3>
<p>To compute $P + Q$ where $P \neq Q$, there are four cases:</p>
<ul>
<li>If $P = 0$ and $Q \neq 0$, then $P + Q = Q$.</li>
<li>If $Q = 0$ and $P \neq 0$, then $P + Q = P$.</li>
<li>Else $P = (x_{p},\ y_{p})$ and$Q = (x_{q},\ y_{q})$
<ul>
<li>If $x_{p} = x_{q}$ (and necessarily $y_{p} \neq y_{q}$), then $P + Q = O$.</li>
<li>Else $x_{p} \neq x_{q}$, then
$$
\begin{gather*} \lambda = \frac{y_{2} - y_{1}}{x_{2} - x_{1}} \ x_{r} = \lambda^{2} - x_{p} - x_{q} \ y_{r} = \lambda(x_{p} - x_{r}) - y_{p}\end{gather*}
$$
and $P + Q = R = (x_{r},\ y_{r})$.</li>
</ul>
</li>
</ul>
<p>The complicated case involves approximately 2 multiplications, 6 additions/subtractions, and 1 division.</p>
<h2 id="ecmul"><a class="header" href="#ecmul">ECMUL</a></h2>
<p>Precompile for computing elliptic curve point scalar multiplication. The points are represented in homogeneous
projective coordinates, given by the coordinates $(x , y , z)$. Transformation into affine coordinates can be done by
applying the following transformation: $(x,y) = (X.Z^{-1} , Y.Z^{-1} )$ if the point is not the point at infinity.</p>
<p>The key idea of projective coordinates is that instead of performing every division immediately, we defer the divisions
by multiplying them into a denominator. The denominator is represented by a new coordinate. Only at the very end, do we
perform a single division to convert from projective coordinates back to affine coordinates.</p>
<p>In affine form, each elliptic curve point has 2 coordinates, like $(x,y)$. In the new projective form, each point will
have 3 coordinates, like $(X,Y,Z)$, with the restriction that $Z$ is never zero. The forward mapping is given by
$(x,y)→(xz,yz,z)$, for any non-zero $z$ (usually chosen to be 1 for convenience). The reverse mapping is given by
$(X,Y,Z)→(X/Z,Y/Z)$, as long as $Z$ is non-zero.</p>
<h3 id="point-doubling-1"><a class="header" href="#point-doubling-1">Point Doubling</a></h3>
<p>The affine form case $y=0$ corresponds to the projective form case $Y/Z=0$. This is equivalent to $Y=0$, since $Z≠0$.</p>
<p>For the interesting case where $P=(X,Y,Z)$ and $Y≠0$, let’s convert the affine arithmetic to projective arithmetic.</p>
<p>After expanding and simplifying the equations
(<a href="https://www.nayuki.io/page/elliptic-curve-point-addition-in-projective-coordinates">demonstration here</a>), the
following substitutions come out</p>
<p>$$
\begin{align*} T &amp;= 3X^{2} + aZ^{2},\ U &amp;= 2YZ,\ V &amp;= 2UXY,\ W &amp;= T^{2} - 2V \end{align*}
$$</p>
<p>Using them, we can write</p>
<p>$$
\begin{align*} X_{r}  &amp;= UW \ Y_{r} &amp;= T(V−W)−2(UY)^{2} \ Z_{r} &amp;= U^{3} \end{align*}
$$</p>
<p>As we can see, the complicated case involves approximately 18 multiplications, 4 additions/subtractions, and 0
divisions.</p>
<h3 id="point-addition-1"><a class="header" href="#point-addition-1">Point Addition</a></h3>
<p>The affine form case $x_{p} = x_{q}$ corresponds to the projective form case $X_{p}/Z_{p} = X_{q}/Z_{q}$. This is
equivalent to $X_{p}Z_{q} = X_{q}Z_{p}$, via cross-multiplication.</p>
<p>For the interesting case where $P = (X_{p},\ Y_{p},\ Z_{p})$ , $Q = (X_{q},\ Y_{q},\ Z_{q})$, and
$X_{p}Z_{q} ≠ X_{q}Z_{p}$, let’s convert the affine arithmetic to projective arithmetic.</p>
<p>After expanding and simplifying the equations
(<a href="https://www.nayuki.io/page/elliptic-curve-point-addition-in-projective-coordinates">demonstration here</a>), the
following substitutions come out</p>
<p>$$
\begin{align*}
T_{0} &amp;= Y_{p}Z_{q}\
T_{1} &amp;= Y_{q}Z_{p}\
T &amp;= T_{0} - T_{1}\
U_{0} &amp;= X_{p}Z_{q}\
U_{1} &amp;= X_{q}Z_{p}\
U &amp;= U_{0} - U_{1}\
U_{2} &amp;= U^{2}\
V &amp;= Z_{p}Z_{q}\
W &amp;= T^{2}V−U_{2}(U_{0}+U_{1}) \
\end{align*}
$$</p>
<p>Using them, we can write</p>
<p>$$
\begin{align*} X_{r}  &amp;= UW \ Y_{r} &amp;= T(U_{0}U_{2}−W)−T_{0}U^{3} \ Z_{r} &amp;= U^{3}V \end{align*}
$$</p>
<p>As we can see, the complicated case involves approximately 15 multiplications, 6 additions/subtractions, and 0
divisions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-contracts"><a class="header" href="#system-contracts">System contracts</a></h1>
<p>While most of the primitive EVM opcodes can be supported out of the box (i.e. zero-value calls,
addition/multiplication/memory/storage management, etc), some of the opcodes are not supported by the VM by default and
they are implemented via “system contracts” — these contracts are located in a special <em>kernel space,</em> i.e. in the
address space in range <code>[0..2^16-1]</code>, and they have some special privileges, which users’ contracts don’t have. These
contracts are pre-deployed at the genesis and updating their code can be done only via system upgrade, managed from L1.</p>
<p>The use of each system contract will be explained down below.</p>
<p>Most of the details on the implementation and the requirements for the execution of system contracts can be found in the
doc-comments of their respective code bases. This chapter serves only as a high-level overview of such contracts.</p>
<p>All the codes of system contracts (including <code>DefaultAccount</code>s) are part of the protocol and can only be change via a
system upgrade through L1.</p>
<h2 id="systemcontext"><a class="header" href="#systemcontext">SystemContext</a></h2>
<p>This contract is used to support various system parameters not included in the VM by default, i.e. <code>chainId</code>, <code>origin</code>,
<code>ergsPrice</code>, <code>blockErgsLimit</code>, <code>coinbase</code>, <code>difficulty</code>, <code>baseFee</code>, <code>blockhash</code>, <code>block.number</code>, <code>block.timestamp.</code></p>
<p>It is important to note that the constructor is <strong>not</strong> run for system contracts upon genesis, i.e. the constant context
values are set on genesis explicitly. Notably, if in the future we want to upgrade the contracts, we will do it via
<a href="specs/zk_evm/system_contracts.html#contractdeployer--immutablesimulator">ContractDeployer</a> and so the constructor will be run.</p>
<p>This contract is also responsible for ensuring validity and consistency of batches, L2 blocks and virtual blocks. The
implementation itself is rather straightforward, but to better understand this contract, please take a look at the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20&amp;%20L2%20blocks%20on%20zkSync.md">page</a>
about the block processing on ZKsync.</p>
<h2 id="accountcodestorage"><a class="header" href="#accountcodestorage">AccountCodeStorage</a></h2>
<p>The code hashes of accounts are stored inside the storage of this contract. Whenever a VM calls a contract with address
<code>address</code> it retrieves the value under storage slot <code>address</code> of this system contract, if this value is non-zero, it
uses this as the code hash of the account.</p>
<p>Whenever a contract is called, the VM asks the operator to provide the preimage for the codehash of the account. That is
why data availability of the code hashes is paramount.</p>
<h3 id="constructing-vs-non-constructing-code-hash"><a class="header" href="#constructing-vs-non-constructing-code-hash">Constructing vs Non-constructing code hash</a></h3>
<p>In order to prevent contracts from being able to call a contract during its construction, we set the marker (i.e. second
byte of the bytecode hash of the account) as <code>1</code>. This way, the VM will ensure that whenever a contract is called
without the <code>isConstructor</code> flag, the bytecode of the default account (i.e. EOA) will be substituted instead of the
original bytecode.</p>
<h2 id="bootloaderutilities"><a class="header" href="#bootloaderutilities">BootloaderUtilities</a></h2>
<p>This contract contains some of the methods which are needed purely for the bootloader functionality but were moved out
from the bootloader itself for the convenience of not writing this logic in Yul.</p>
<h2 id="defaultaccount"><a class="header" href="#defaultaccount">DefaultAccount</a></h2>
<p>Whenever a contract that does <strong>not</strong> both:</p>
<ul>
<li>belong to kernel space</li>
<li>have any code deployed on it (the value stored under the corresponding storage slot in <code>AccountCodeStorage</code> is zero)</li>
</ul>
<p>The code of the default account is used. The main purpose of this contract is to provide EOA-like experience for both
wallet users and contracts that call it, i.e. it should not be distinguishable (apart of spent gas) from EOA accounts on
Ethereum.</p>
<h2 id="ecrecover"><a class="header" href="#ecrecover">Ecrecover</a></h2>
<p>The implementation of the ecrecover precompile. It is expected to be used frequently, so written in pure yul with a
custom memory layout.</p>
<p>The contract accepts the calldata in the same format as EVM precompile, i.e. the first 32 bytes are the hash, the next
32 bytes are the v, the next 32 bytes are the r, and the last 32 bytes are the s.</p>
<p>It also validates the input by the same rules as the EVM precompile:</p>
<ul>
<li>The v should be either 27 or 28,</li>
<li>The r and s should be less than the curve order.</li>
</ul>
<p>After that, it makes a precompile call and returns empty bytes if the call failed, and the recovered address otherwise.</p>
<h2 id="empty-contracts"><a class="header" href="#empty-contracts">Empty contracts</a></h2>
<p>Some of the contracts are relied upon to have EOA-like behaviour, i.e. they can be always called and get the success
value in return. An example of such address is 0 address. We also require the bootloader to be callable so that the
users could transfer ETH to it.</p>
<p>For these contracts, we insert the <code>EmptyContract</code> code upon genesis. It is basically a noop code, which does nothing
and returns <code>success=1</code>.</p>
<h2 id="sha256--keccak256"><a class="header" href="#sha256--keccak256">SHA256 &amp; Keccak256</a></h2>
<p>Note that, unlike Ethereum, keccak256 is a precompile (<em>not an opcode</em>) on ZKsync.</p>
<p>These system contracts act as wrappers for their respective crypto precompile implementations. They are expected to be
used frequently, especially keccak256, since Solidity computes storage slots for mapping and dynamic arrays with its
help. That’s why we wrote contracts on pure yul with optimizing the short input case.</p>
<p>The system contracts accept the input and transform it into the format that the zk-circuit expects. This way, some of
the work is shifted from the crypto to smart contracts, which are easier to audit and maintain.</p>
<p>Both contracts should apply padding to the input according to their respective specifications, and then make a
precompile call with the padded data. All other hashing work will be done in the zk-circuit. It’s important to note that
the crypto part of the precompiles expects to work with padded data. This means that a bug in applying padding may lead
to an unprovable transaction.</p>
<h2 id="ecadd--ecmul"><a class="header" href="#ecadd--ecmul">EcAdd &amp; EcMul</a></h2>
<p>These precompiles simulate the behaviour of the EVM’s EcAdd and EcMul precompiles and are fully implemented in Yul
without circuit counterparts. You can read more about them
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Elliptic%20curve%20precompiles.md">here</a>.</p>
<h2 id="l2basetoken--msgvaluesimulator"><a class="header" href="#l2basetoken--msgvaluesimulator">L2BaseToken &amp; MsgValueSimulator</a></h2>
<p>Unlike Ethereum, zkEVM does not have any notion of any special native token. That’s why we have to simulate operations
with Ether via two contracts: <code>L2BaseToken</code> &amp; <code>MsgValueSimulator</code>.</p>
<p><code>L2BaseToken</code> is a contract that holds the balances of ETH for the users. This contract does NOT provide ERC20
interface. The only method for transferring Ether is <code>transferFromTo</code>. It permits only some system contracts to transfer
on behalf of users. This is needed to ensure that the interface is as close to Ethereum as possible, i.e. the only way
to transfer ETH is by doing a call to a contract with some <code>msg.value</code>. This is what <code>MsgValueSimulator</code> system contract
is for.</p>
<p>Whenever anyone wants to do a non-zero value call, they need to call <code>MsgValueSimulator</code> with:</p>
<ul>
<li>The calldata for the call equal to the original one.</li>
<li>Pass <code>value</code> and whether the call should be marked with <code>isSystem</code> in the first extra abi params.</li>
<li>Pass the address of the callee in the second extraAbiParam.</li>
</ul>
<p>More information on the extraAbiParams can be read
<a href="specs/zk_evm/../../guides/advanced/0_alternative_vm_intro.html#flags-for-calls">here</a>.</p>
<h2 id="knowncodestorage"><a class="header" href="#knowncodestorage">KnownCodeStorage</a></h2>
<p>This contract is used to store whether a certain code hash is “known”, i.e. can be used to deploy contracts. On ZKsync,
the L2 stores the contract’s code <em>hashes</em> and not the codes themselves. Therefore, it must be part of the protocol to
ensure that no contract with unknown bytecode (i.e. hash with an unknown preimage) is ever deployed.</p>
<p>The factory dependencies field provided by the user for each transaction contains the list of the contract’s bytecode
hashes to be marked as known. We can not simply trust the operator to “know” these bytecodehashes as the operator might
be malicious and hide the preimage. We ensure the availability of the bytecode in the following way:</p>
<ul>
<li>If the transaction comes from L1, i.e. all its factory dependencies have already been published on L1, we can simply
mark these dependencies as “known”.</li>
<li>If the transaction comes from L2, i.e. (the factory dependencies are yet to publish on L1), we make the user pays by
burning ergs proportional to the bytecode’s length. After that, we send the L2→L1 log with the bytecode hash of the
contract. It is the responsibility of the L1 contracts to verify that the corresponding bytecode hash has been
published on L1.</li>
</ul>
<p>It is the responsibility of the <a href="specs/zk_evm/system_contracts.html#contractdeployer--immutablesimulator">ContractDeployer</a> system contract to deploy only
those code hashes that are known.</p>
<p>The KnownCodesStorage contract is also responsible for ensuring that all the “known” bytecode hashes are also
<a href="specs/zk_evm/../../guides/advanced/0_alternative_vm_intro.html#bytecode-validity">valid</a>.</p>
<h2 id="contractdeployer--immutablesimulator"><a class="header" href="#contractdeployer--immutablesimulator">ContractDeployer &amp; ImmutableSimulator</a></h2>
<p><code>ContractDeployer</code> is a system contract responsible for deploying contracts on ZKsync. It is better to understand how it
works in the context of how the contract deployment works on ZKsync. Unlike Ethereum, where <code>create</code>/<code>create2</code> are
opcodes, on ZKsync these are implemented by the compiler via calls to the ContractDeployer system contract.</p>
<p>For additional security, we also distinguish the deployment of normal contracts and accounts. That’s why the main
methods that will be used by the user are <code>create</code>, <code>create2</code>, <code>createAccount</code>, <code>create2Account</code>, which simulate the
CREATE-like and CREATE2-like behavior for deploying normal and account contracts respectively.</p>
<h3 id="address-derivation"><a class="header" href="#address-derivation"><strong>Address derivation</strong></a></h3>
<p>Each rollup that supports L1→L2 communications needs to make sure that the addresses of contracts on L1 and L2 do not
overlap during such communication (otherwise it would be possible that some evil proxy on L1 could mutate the state of
the L2 contract). Generally, rollups solve this issue in two ways:</p>
<ul>
<li>XOR/ADD some kind of constant to addresses during L1→L2 communication. That’s how rollups closer to full
EVM-equivalence solve it, since it allows them to maintain the same derivation rules on L1 at the expense of contract
accounts on L1 having to redeploy on L2.</li>
<li>Have different derivation rules from Ethereum. That is the path that ZKsync has chosen, mainly because since we have
different bytecode than on EVM, CREATE2 address derivation would be different in practice anyway.</li>
</ul>
<p>You can see the rules for our address derivation in <code>getNewAddressCreate2</code>/ <code>getNewAddressCreate</code> methods in the
ContractDeployer.</p>
<p>Note, that we still add a certain constant to the addresses during L1→L2 communication in order to allow ourselves some
way to support EVM bytecodes in the future.</p>
<h3 id="deployment-nonce"><a class="header" href="#deployment-nonce"><strong>Deployment nonce</strong></a></h3>
<p>On Ethereum, the same nonce is used for CREATE for accounts and EOA wallets. On ZKsync this is not the case, we use a
separate nonce called “deploymentNonce” to track the nonces for accounts. This was done mostly for consistency with
custom accounts and for having multicalls feature in the future.</p>
<h3 id="general-process-of-deployment"><a class="header" href="#general-process-of-deployment"><strong>General process of deployment</strong></a></h3>
<ul>
<li>After incrementing the deployment nonce, the contract deployer must ensure that the bytecode that is being deployed is
available.</li>
<li>After that, it puts the bytecode hash with a
<a href="specs/zk_evm/system_contracts.html#constructing-vs-non-constructing-code-hash">special constructing marker</a> as code for the address of the
to-be-deployed contract.</li>
<li>Then, if there is any value passed with the call, the contract deployer passes it to the deployed account and sets the
<code>msg.value</code> for the next as equal to this value.</li>
<li>Then, it uses <code>mimic_call</code> for calling the constructor of the contract out of the name of the account.</li>
<li>It parses the array of immutables returned by the constructor (we’ll talk about immutables in more details later).</li>
<li>Calls <code>ImmutableSimulator</code> to set the immutables that are to be used for the deployed contract.</li>
</ul>
<p>Note how it is different from the EVM approach: on EVM when the contract is deployed, it executes the initCode and
returns the deployedCode. On ZKsync, contracts only have the deployed code and can set immutables as storage variables
returned by the constructor.</p>
<h3 id="constructor"><a class="header" href="#constructor"><strong>Constructor</strong></a></h3>
<p>On Ethereum, the constructor is only part of the initCode that gets executed during the deployment of the contract and
returns the deployment code of the contract. On ZKsync, there is no separation between deployed code and constructor
code. The constructor is always a part of the deployment code of the contract. In order to protect it from being called,
the compiler-generated contracts invoke constructor only if the <code>isConstructor</code> flag provided (it is only available for
the system contracts). You can read more about flags
<a href="specs/zk_evm/../../guides/advanced/0_alternative_vm_intro.html#flags-for-calls">here</a>.</p>
<p>After execution, the constructor must return an array of:</p>
<pre><code class="language-solidity">struct ImmutableData {
  uint256 index;
  bytes32 value;
}

</code></pre>
<p>basically denoting an array of immutables passed to the contract.</p>
<h3 id="immutables"><a class="header" href="#immutables"><strong>Immutables</strong></a></h3>
<p>Immutables are stored in the <code>ImmutableSimulator</code> system contract. The way how <code>index</code> of each immutable is defined is
part of the compiler specification. This contract treats it simply as mapping from index to value for each particular
address.</p>
<p>Whenever a contract needs to access a value of some immutable, they call the
<code>ImmutableSimulator.getImmutable(getCodeAddress(), index)</code>. Note that on ZKsync it is possible to get the current
execution address you can read more about <code>getCodeAddress()</code>
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/advanced/0_alternative_vm_intro.md#zkevm-specific-opcodes">here</a>.</p>
<h3 id="return-value-of-the-deployment-methods"><a class="header" href="#return-value-of-the-deployment-methods"><strong>Return value of the deployment methods</strong></a></h3>
<p>If the call succeeded, the address of the deployed contract is returned. If the deploy fails, the error bubbles up.</p>
<h2 id="defaultaccount-1"><a class="header" href="#defaultaccount-1">DefaultAccount</a></h2>
<p>The implementation of the default account abstraction. This is the code that is used by default for all addresses that
are not in kernel space and have no contract deployed on them. This address:</p>
<ul>
<li>Contains minimal implementation of our account abstraction protocol. Note that it supports the
<a href="https://v2-docs.zksync.io/dev/developer-guides/aa.html#paymasters">built-in paymaster flows</a>.</li>
<li>When anyone (except bootloader) calls it, it behaves in the same way as a call to an EOA, i.e. it always returns
<code>success = 1, returndatasize = 0</code> for calls from anyone except for the bootloader.</li>
</ul>
<h2 id="l1messenger"><a class="header" href="#l1messenger">L1Messenger</a></h2>
<p>A contract used for sending arbitrary length L2→L1 messages from ZKsync to L1. While ZKsync natively supports a rather
limited number of L1→L2 logs, which can transfer only roughly 64 bytes of data a time, we allowed sending
nearly-arbitrary length L2→L1 messages with the following trick:</p>
<p>The L1 messenger receives a message, hashes it and sends only its hash as well as the original sender via L2→L1 log.
Then, it is the duty of the L1 smart contracts to make sure that the operator has provided full preimage of this hash in
the commitment of the batch.</p>
<p>The <code>L1Messenger</code> is also responsible for validating the total pubdata to be sent on L1. You can read more about it
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</p>
<h2 id="nonceholder"><a class="header" href="#nonceholder">NonceHolder</a></h2>
<p>Serves as storage for nonces for our accounts. Besides making it easier for operator to order transactions (i.e. by
reading the current nonces of account), it also serves a separate purpose: making sure that the pair (address, nonce) is
always unique.</p>
<p>It provides a function <code>validateNonceUsage</code> which the bootloader uses to check whether the nonce has been used for a
certain account or not. Bootloader enforces that the nonce is marked as non-used before validation step of the
transaction and marked as used one afterwards. The contract ensures that once marked as used, the nonce can not be set
back to the “unused” state.</p>
<p>Note that nonces do not necessarily have to be monotonic (this is needed to support more interesting applications of
account abstractions, e.g. protocols that can start transactions on their own, tornado-cash like protocols, etc). That’s
why there are two ways to set a certain nonce as “used”:</p>
<ul>
<li>By incrementing the <code>minNonce</code> for the account (thus making all nonces that are lower than <code>minNonce</code> as used).</li>
<li>By setting some non-zero value under the nonce via <code>setValueUnderNonce</code>. This way, this key will be marked as used and
will no longer be allowed to be used as nonce for accounts. This way it is also rather efficient, since these 32 bytes
could be used to store some valuable information.</li>
</ul>
<p>The accounts upon creation can also provide which type of nonce ordering do they want: Sequential (i.e. it should be
expected that the nonces grow one by one, just like EOA) or Arbitrary, the nonces may have any values. This ordering is
not enforced in any way by system contracts, but it is more of a suggestion to the operator on how it should order the
transactions in the mempool.</p>
<h2 id="eventwriter"><a class="header" href="#eventwriter">EventWriter</a></h2>
<p>A system contract responsible for emitting events.</p>
<p>It accepts in its 0-th extra abi data param the number of topics. In the rest of the extraAbiParams he accepts topics
for the event to emit. Note, that in reality the event the first topic of the event contains the address of the account.
Generally, the users should not interact with this contract directly, but only through Solidity syntax of <code>emit</code>-ing new
events.</p>
<h2 id="compressor"><a class="header" href="#compressor">Compressor</a></h2>
<p>One of the most expensive resource for a rollup is data availability, so in order to reduce costs for the users we
compress the published pubdata in several ways:</p>
<ul>
<li>We compress published bytecodes.</li>
<li>We compress state diffs.</li>
</ul>
<p>This contract contains utility methods that are used to verify the correctness of either bytecode or state diff
compression. You can read more on how we compress state diffs and bytecodes in the corresponding
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">document</a>.</p>
<h2 id="known-issues-to-be-resolved"><a class="header" href="#known-issues-to-be-resolved">Known issues to be resolved</a></h2>
<p>The protocol, while conceptually complete, contains some known issues which will be resolved in the short to middle
term.</p>
<ul>
<li>Fee modeling is yet to be improved. More on it in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/zkSync%20fee%20model.md">document</a>
on the fee model.</li>
<li>We may add some kind of default implementation for the contracts in the kernel space (i.e. if called, they wouldn’t
revert but behave like an EOA).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intro-guide-to-interop"><a class="header" href="#intro-guide-to-interop">Intro Guide to Interop</a></h1>
<h2 id="what-is-interop"><a class="header" href="#what-is-interop">What is Interop</a></h2>
<p>Interop is a way to communicate and transact between two ZK Stack chains. It allows you to:</p>
<p><strong>1. Observe messages:</strong> Track when an interop message (think of it as a special event) is created on the source chain.</p>
<p><strong>2. Send assets:</strong> Transfer ERC20 tokens and other assets between chains.</p>
<p><strong>3. Execute calls:</strong> Call a contract on a remote chain with specific calldata and value.</p>
<p>With interop, you automatically get an account (a.k.a. aliasedAccount) on each chain, which you can control from the
source chain.</p>
<p><strong>4. Execute bundles of calls:</strong> Group multiple remote calls into a single bundle, ensuring all of them execute at once.</p>
<p><strong>5. Execute transactions:</strong> Create transactions on the source chain, which will automatically get executed on the
destination chain, with options to choose from various cross-chain Paymaster solutions to handle gas fees.</p>
<h2 id="how-to-use-interop"><a class="header" href="#how-to-use-interop">How to Use Interop</a></h2>
<p>Here’s a simple example of calling a contract on a destination chain:</p>
<pre><code class="language-solidity">cast send source-chain-rpc.com INTEROP_CENTER_ADDRESS sendInteropWithSingleCall(
 0x1fa72e78 // destination_chain_id,
 0xb4AB2FF34fa... // destination_contract,
 0x29723511000000... // destination_calldata,
 0, // value
 100_000, // gasLimit
 250_000_000, // gasPrice
 )
</code></pre>
<p>While this looks very similar to a ‘regular’ call, there are some nuances, especially around handling failures and
errors.</p>
<p>Let’s explore these key details together.</p>
<h2 id="common-questions-and-considerations"><a class="header" href="#common-questions-and-considerations">Common Questions and Considerations</a></h2>
<h4 id="1-who-pays-for-gas"><a class="header" href="#1-who-pays-for-gas">1. Who pays for gas</a></h4>
<p>When using this method, your account must hold <code>gasLimit * gasPrice</code> worth of destination chain tokens on the source
chain.</p>
<p>For example, if you’re sending the request from Era and the destination chain is Sophon (with SOPH tokens), you’ll need
SOPH tokens available on Era.</p>
<p>Additional payment options are available, which will be covered in later sections.</p>
<h4 id="2-how-does-the-destination-contract-know-its-from-me"><a class="header" href="#2-how-does-the-destination-contract-know-its-from-me">2. How does the destination contract know it’s from me</a></h4>
<p>The destination contract will see <code>msg.sender</code> as <code>keccak(source_account, source_chain)[:20]</code>.</p>
<p>Ideally, we would use something like <code>source_account@source_chain</code> (similar to an email format), but since Ethereum
addresses are limited to 20 bytes, we use a Keccak hash to fit this constraint.</p>
<h4 id="3-who-executes-it-on-the-destination-chain"><a class="header" href="#3-who-executes-it-on-the-destination-chain">3. Who executes it on the destination chain</a></h4>
<p>The call is auto-executed on the destination chain. As a user, you don’t need to take any additional actions.</p>
<h4 id="4-what-if-it-runs-out-of-gas-or-the-gasprice-is-set-too-low"><a class="header" href="#4-what-if-it-runs-out-of-gas-or-the-gasprice-is-set-too-low">4. What if it runs out of gas or the gasPrice is set too low</a></h4>
<p>In either scenario, you can retry the transaction using the <code>retryInteropTransaction</code> method:</p>
<pre><code class="language-solidity">   cast send source-chain.com INTEROP_CENTER_ADDRESS retryInteropTransaction(
     0x2654.. // previous interop transaction hash from above
     200_000, // new gasLimit
     300_000_000 // new gasPrice
    )
</code></pre>
<p><strong>Important</strong> : Depending on your use case, it’s crucial to retry the transaction rather than creating a new one with
<code>sendInteropWithSingleCall</code>.</p>
<p>For example, if your call involves transferring a large amount of assets, initiating a new <code>sendInteropWithSingleCall</code>
could result in freezing or burning those assets again.</p>
<h4 id="5-what-if-my-assets-were-burned-during-the-transaction-but-it-failed-on-the-destination-chain-how-do-i-get-them-back"><a class="header" href="#5-what-if-my-assets-were-burned-during-the-transaction-but-it-failed-on-the-destination-chain-how-do-i-get-them-back">5. What if my assets were burned during the transaction, but it failed on the destination chain? How do I get them back</a></h4>
<p>If your transaction fails on the destination chain, you can either:</p>
<ol>
<li>
<p>Retry the transaction with more gas or a higher gas limit (refer to the retry method above).</p>
</li>
<li>
<p>Cancel the transaction using the following method:</p>
</li>
</ol>
<pre><code class="language-solidity">cast send source-chain INTEROP_CENTER_ADDRESS cancelInteropTransaction(
    0x2654.., // previous interop transaction
    100_000, // gasLimit (cancellation also requires gas, but only to mark it as cancelled)
    300_000_000 // gasPrice
)
</code></pre>
<p>After cancellation, call the claimFailedDeposit method on the source chain contracts to recover the burned assets. Note
that the details for this step may vary depending on the contract specifics.</p>
<h2 id="complex-scenario"><a class="header" href="#complex-scenario">Complex Scenario</a></h2>
<h4 id="6-what-if-i-want-to-transfer-usdc-to-the-sophon-chain-swap-it-for-pepe-coin-and-transfer-the-results-back"><a class="header" href="#6-what-if-i-want-to-transfer-usdc-to-the-sophon-chain-swap-it-for-pepe-coin-and-transfer-the-results-back">6. What if I want to transfer USDC to the Sophon chain, swap it for PEPE coin, and transfer the results back</a></h4>
<p>To accomplish this, you’ll need to:</p>
<ul>
<li>Create multiple <strong>InteropCalls</strong> (e.g., transferring USDC, executing the swap).</li>
<li>Combine these calls into a single <strong>InteropBundle</strong>.</li>
<li>Execute the <strong>InteropTransaction</strong> on the destination chain.</li>
</ul>
<p>The step-by-step process and exact details will be covered in the next section.</p>
<h2 id="technical-details"><a class="header" href="#technical-details">Technical Details</a></h2>
<h3 id="how-does-native-bridging-differ-from-a-third-party-bridging"><a class="header" href="#how-does-native-bridging-differ-from-a-third-party-bridging">How does native bridging differ from a third party bridging</a></h3>
<p>Bridges generally fall into two categories: Native and Third-Party.</p>
<h4 id="1-native-bridges"><a class="header" href="#1-native-bridges">1. Native Bridges</a></h4>
<p>Native bridges enable asset transfers “up and down” (from L2 to L1 and vice versa), but interop (which is also a form of
native bridging) allows you to move them between different L2s.</p>
<p>Instead of doing a “round trip” (L2 → L1 → another L2), interop lets you move assets directly between two L2s, saving
both time and cost.</p>
<h4 id="2-third-party-bridging"><a class="header" href="#2-third-party-bridging">2. Third-Party Bridging</a></h4>
<p>Third-party bridges enable transfers between two L2s, but they rely on their own liquidity. While you, as the user,
receive assets on the destination chain instantly, these assets come from the bridge’s liquidity pool.</p>
<p>Bridge operators then rebalance using native bridging, which requires maintaining token reserves on both sides. Without
interop this adds costs for the bridge operators, often resulting in higher fees for users.</p>
<p>The good news is that third-party bridges can use interop to improve their token transfers by utilizing the
<strong>InteropMessage</strong> layer.</p>
<p>More details on this will follow below.</p>
<h3 id="how-fast-is-it"><a class="header" href="#how-fast-is-it">How Fast is It</a></h3>
<p>Interop speed is determined by its lowest level: <strong>InteropMessage</strong> propagation speed. This essentially depends on how
quickly the destination chain can confirm that the message created by the source chain is valid.</p>
<ul>
<li>
<p><strong>Default Mode:</strong> To prioritize security, the default interop mode waits for a ZK proof to validate the message, which
typically takes around 10 minutes.</p>
</li>
<li>
<p><strong>Fast Mode (Planned):</strong> We are developing an alternative <strong>INTEROP_CENTER</strong> contract (using a different address but
the same interface) that will operate within 1 second. However, this faster mode comes with additional risks, similar
to the approach used by optimistic chains.</p>
</li>
</ul>
<h3 id="4-levels-of-interop"><a class="header" href="#4-levels-of-interop">4 Levels of Interop</a></h3>
<p>When analyzing interop, it can be broken into four levels, allowing you to choose the appropriate level for integration:</p>
<ul>
<li>
<p><strong>InteropMessages:</strong> The lowest level, directly used by third-party bridges and other protocols.</p>
</li>
<li>
<p><strong>InteropCall:</strong> A medium level, designed for use by “library” contracts.</p>
</li>
<li>
<p><strong>InteropCallBundle:</strong> A higher level, intended for use by “user-visible” contracts.</p>
</li>
<li>
<p><strong>InteropTransaction:</strong> The highest level, designed for use in UX and frontends.</p>
</li>
</ul>
<p><img src="specs/interop/../img/levelsofinterop.png" alt="levelsofinterop.png" /></p>
<p>We will be covering the details of each layer in the next section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interop-messages"><a class="header" href="#interop-messages">Interop Messages</a></h1>
<p>In this section, we’re going to cover the lowest level of the interop stack: <strong>Interop Messages</strong> — the interface that
forms the foundation for everything else.</p>
<p>We’ll explore the details of the interface, its use cases, and how it compares to similar interfaces from
Superchain/Optimism.</p>
<p>This is an advanced document. While most users and app developers typically interact with higher levels of interop, it’s
still valuable to understand how the internals work.</p>
<h2 id="basics"><a class="header" href="#basics">Basics</a></h2>
<p><img src="specs/interop/../img/interopmsg.png" alt="interopmsg.png" /></p>
<p>Interop Messages are the lowest level of our stack.</p>
<p>An <strong>InteropMessage</strong> contains data and offers two methods:</p>
<ul>
<li>Send a message</li>
<li>Verify that a given message was sent on some chain</li>
</ul>
<p>Notice that the message itself doesn’t have any ‘destination chain’ or address—it is simply a payload that a user (or
contract) is creating. Think of it as a broadcast.</p>
<p>The <code>InteropCenter</code> is a contract that is pre-deployed on all chains at a fixed address <code>0x00..1234</code>.</p>
<pre><code class="language-solidity">contract InteropCenter {
  // Sends interop message. Can be called by anyone.
  // Returns the unique interopHash.
 function sendInteropMessage(bytes data) returns interopHash;

  // Interop message - uniquely identified by the hash of the payload.
 struct InteropMessage {
   bytes data;
   address sender; // filled by InteropCenter
   uint256 sourceChainId; // filled by InteropCenter
   uint256 messageNum; // a 'nonce' to guarantee different hashes.
 }

 // Verifies if such interop message was ever producted.
 function verifyInteropMessage(bytes32 interopHash, Proof merkleProof) return bool;
}
</code></pre>
<p>When you call <code>sendInteropMessage</code>, the <code>InteropCenter</code> adds additional fields, such as your sender address, source
chain ID, and messageNum (a nonce ensuring the hash of this structure is globally unique). It then returns the
<code>interopHash</code>.</p>
<p>This <code>interopHash</code> serves as a globally unique identifier that can be used on any chain in the network to call
<code>verifyInteropMessage</code>.</p>
<p><img src="specs/interop/../img/verifyinteropmsg.png" alt="A message created on one chain can be verified on any other chain." /></p>
<h4 id="how-do-i-get-the-proof"><a class="header" href="#how-do-i-get-the-proof">How do I get the proof</a></h4>
<p>You’ll notice that <strong>verifyInteropMessage</strong> has a second argument — a proof that you need to provide. This proof is a
Merkle tree proof (more details below). You can obtain it by querying the
<a href="https://docs.zksync.io/build/api-reference/zks-rpc#zks_getl2tol1msgproof">chain</a> , or generate it off-chain - by
looking at the chain’s state on L1</p>
<h4 id="how-does-the-interop-message-differ-from-other-layers-interoptransactions-interopcalls"><a class="header" href="#how-does-the-interop-message-differ-from-other-layers-interoptransactions-interopcalls">How does the interop message differ from other layers (InteropTransactions, InteropCalls)</a></h4>
<p>As the most basic layer, an interop message doesn’t include any advanced features — it lacks support for selecting
destination chains, nullifiers/replay, cancellation, and more.</p>
<p>If you need these capabilities, consider integrating with a higher layer of interop, such as Call or Bundle, which
provide these additional functionalities.</p>
<h2 id="simple-use-case"><a class="header" href="#simple-use-case">Simple Use Case</a></h2>
<p>Before we dive into the details of how the system works, let’s look at a simple use case for a DApp that decides to use
InteropMessage.</p>
<p>For this example, imagine a basic cross-chain contract where the <code>signup()</code> method can be called on chains B, C, and D
only if someone has first called <code>signup_open()</code> on chain A.</p>
<pre><code class="language-solidity">// Contract deployed on chain A.
contract SignupManager {
  public bytes32 sigup_open_msg_hash;
  function signup_open() onlyOwner {
    // We are open for business
    signup_open_msg_hash = InteropCenter(INTEROP_CENTER_ADDRESS).sendInteropMessage("We are open");
  }
}

// Contract deployed on all other chains.
contract SignupContract {
  public bool signupIsOpen;
  // Anyone can call it.
  function openSignup(InteropMessage message, InteropProof proof) {
    InteropCenter(INTEROP_CENTER_ADDRESS).verifyInteropMessage(keccak(message), proof);
    require(message.sourceChainId == CHAIN_A_ID);
    require(message.sender == SIGNUP_MANAGER_ON_CHAIN_A);
    require(message.data == "We are open");
   signupIsOpen = true;
  }

  function signup() {
     require(signupIsOpen);
     signedUpUser[msg.sender] = true;
  }
}
</code></pre>
<p>In the example above, the <code>signupManager</code> on chain A calls the <code>signup_open</code> method. After that, any user on other
chains can retrieve the <code>signup_open_msg_hash</code>, obtain the necessary proof from the Gateway (or another source), and
call the <code>openSignup</code> function on any destination chain.</p>
<h2 id="deeper-technical-dive"><a class="header" href="#deeper-technical-dive">Deeper Technical Dive</a></h2>
<p>Let’s break down what happens inside the InteropCenter when a new interop message is created:</p>
<pre><code class="language-solidity">function sendInteropMessage(bytes data) {
  messageNum += 1;
  msg = InteropMessage({data, msg.sender, block.chain_id, messageNum});
  // Does L2-&gt;L1 Messaging.
  sendToL1(abi.encode(msg));
  return keccak(msg);
}
</code></pre>
<p>As you can see, it populates the necessary data and then calls the <code>sendToL1</code> method.</p>
<p>The <code>sendToL1</code> method is part of a system contract that gathers all messages during a batch, constructs a Merkle tree
from them at the end of the batch, and sends this tree to the SettlementLayer (Gateway) when the batch is committed.</p>
<p><img src="specs/interop/../img/sendtol1.png" alt="sendtol1.png" /></p>
<p>The settlement layer receives the messages and once the proof for the batch is submitted (or more accurately, during the
“execute” step), it will add the root of the Merkle tree to its <code>messageRoot</code> (sometimes called <code>globalRoot</code>).</p>
<p><img src="specs/interop/../img/globalroot.png" alt="globalroot.png" /></p>
<p>The <code>messageRoot</code> is the root of the Merkle tree that includes all messages from all chains. Each chain regularly reads
the messageRoot value from the Gateway to stay synchronized.</p>
<p><img src="specs/interop/../img/gateway.png" alt="gateway.png" /></p>
<p>If a user wants to call <code>verifyInteropMessage</code> on a chain, they first need to query the Gateway for the Merkle path from
the batch they are interested in up to the <code>messageRoot</code>. Once they have this path, they can provide it as an argument
when calling a method on the destination chain (such as the <code>openSignup</code> method in our example).</p>
<p><img src="specs/interop/../img/proofmerklepath.png" alt="proofmerklepath.png" /></p>
<h4 id="what-if-chain-doesnt-provide-the-proof"><a class="header" href="#what-if-chain-doesnt-provide-the-proof">What if Chain doesn’t provide the proof</a></h4>
<p>If the chain doesn’t respond, users can manually re-create the Merkle proof using data available on L1. Every
interopMessage is also sent to L1.</p>
<h4 id="message-roots-change-frequently"><a class="header" href="#message-roots-change-frequently">Message roots change frequently</a></h4>
<p>Yes, message roots update continuously as new chains prove their blocks. However, chains retain historical message roots
for a reasonable period (around 24 hours) to ensure that recently generated Merkle paths remain valid.</p>
<h4 id="is-this-secure-could-a-chain-operator-like-chain-d-use-a-different-message-root"><a class="header" href="#is-this-secure-could-a-chain-operator-like-chain-d-use-a-different-message-root">Is this secure? Could a chain operator, like Chain D, use a different message root</a></h4>
<p>Yes, it’s secure. If a malicious operator on Chain D attempted to use a different message root, they wouldn’t be able to
submit the proof for their new batch to the Gateway. This is because the proof’s public inputs must include the valid
message root.</p>
<h3 id="other-features"><a class="header" href="#other-features">Other Features</a></h3>
<h4 id="dependency-set"><a class="header" href="#dependency-set">Dependency Set</a></h4>
<ul>
<li>In ElasticChain, this is implicitly handled by the Gateway. Any chain that is part of the message root can exchange
messages with any other chain, effectively forming an undirected graph.</li>
</ul>
<h4 id="timestamps-and-expiration"><a class="header" href="#timestamps-and-expiration">Timestamps and Expiration</a></h4>
<ul>
<li>In ElasticChain, older messages become increasingly difficult to validate as it becomes harder to gather the data
required to construct a Merkle proof. Expiration is also being considered for this reason, but the specifics are yet
to be determined.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bundles-and-calls"><a class="header" href="#bundles-and-calls">Bundles and Calls</a></h1>
<h2 id="basics-calls"><a class="header" href="#basics-calls">Basics Calls</a></h2>
<p>Interop Calls are the next level of interfaces, built on top of Interop Messages, enabling you to call contracts on
other chains.</p>
<p><img src="specs/interop/../img/interopcall.png" alt="interopcall.png" /></p>
<p>At this level, the system handles replay protection—once a call is successfully executed, it cannot be executed again
(eliminating the need for your own nullifiers or similar mechanisms).</p>
<p>Additionally, these calls originate from aliased accounts, simplifying permission management (more details on this
below).</p>
<p>Cancellations and retries are managed at the next level (Bundles), which are covered in the following section.</p>
<h3 id="interface"><a class="header" href="#interface">Interface</a></h3>
<p>On the sending side, the interface provides the option to send this “call” to the destination contract.</p>
<pre><code class="language-solidity">struct InteropCall {
 address sourceSender,
 address destinationAddress,
 uint256 destinationChainId,
 calldata data,
 uint256 value
}
contract InteropCenter {
 // On source chain.
  // Sends a 'single' basic internal call to destination chain &amp; address.
  // Internally, it starts a bundle, adds this call and sends it over.
  function sendCall(destinationChain, destinationAddress, calldata, msgValue) returns bytes32 bundleId;
}
</code></pre>
<p>In return, you receive a <code>bundleId</code> (we’ll explain bundles later, but for now, think of it as a unique identifier for
your call).</p>
<p>On the destination chain, you can execute the call using the execute method:</p>
<pre><code class="language-solidity">contract InteropCenter {
  // Executes a given bundle.
  // interopMessage is the message that contains your bundle as payload.
  // If it fails, it can be called again.
  function executeInteropBundle(interopMessage, proof);

  // If the bundle didn't execute succesfully yet, it can be marked as cancelled.
  // See details below.
  function cancelInteropBundle(interopMessage, proof);
}

</code></pre>
<p>You can retrieve the <code>interopMessage</code> (which contains your entire payload) from the Gateway, or you can construct it
yourself using L1 data.</p>
<p>Under the hood, this process calls the <code>destinationAddress</code> with the specified calldata.</p>
<p>This leads to an important question: <strong>Who is the msg.sender for this call?</strong></p>
<h2 id="msgsender-of-the-destination-call"><a class="header" href="#msgsender-of-the-destination-call"><code>msg.sender</code> of the Destination Call</a></h2>
<p>The <code>msg.sender</code> on the destination chain will be the <strong>AliasedAccount</strong> — an address created as a hash of the original
sender and the original source chain.</p>
<p>(Normally, we’d like to use <code>sourceAccount@sourceChain</code>, but since Ethereum limits the size of addresses to 20 bytes, we
compute the Keccak hash of the string above and use this as the address.)</p>
<p>One way to think about it is this: You (as account <code>0x5bFF1...</code> on chain A) can send a call to a contract on a
destination chain, and for that contract, it will appear as if the call came locally from the address
<code>keccak(0x5bFF1 || A)</code>. This means you are effectively “controlling” such an account address on <strong>every ZK Chain</strong> by
sending interop messages from the <code>0x5bFF1...</code> account on chain A.</p>
<p><img src="specs/interop/../img/msgdotsender.png" alt="msgdotsender.png" /></p>
<h2 id="simple-example"><a class="header" href="#simple-example">Simple Example</a></h2>
<p>Imagine you have contracts on chains B, C, and D, and you’d like them to send “reports” to the Headquarters (HQ)
contract on chain A every time a customer makes a purchase.</p>
<pre><code class="language-solidity">// Deployed on chains B, C, D.
contract Shop {
 /// Called by the customers when they buy something.
 function buy(uint256 itemPrice) {
   // handle payment etc.
   ...
   // report to HQ
   InteropCenter(INTEROP_ADDRESS).sendCall(
    324,       // chain id of chain A,
    0xc425..,  // HQ contract on chain A,
    createCalldata("reportSales(uint256)", itemPrice), // calldata
    0,         // no value
  );
 }
}

// Deployed on chain A
contract HQ {
  // List of shops
  mapping (address =&gt; bool) shops;
  mapping (address =&gt; uint256) sales;
  function addShop(address addressOnChain, uint256 chainId) onlyOwner {
    // Adding aliased accounts.
   shops[address(keccak(addressOnChain || chainId))] = true;
  }

  function reportSales(uint256 itemPrice) {
    // only allow calls from our shops (their aliased accounts).
   require(shops[msg.sender]);
   sales[msg.sender] += itemPrice;
  }
}
</code></pre>
<h4 id="who-is-paying-for-gas-how-does-this-call-get-to-the-destination-chain"><a class="header" href="#who-is-paying-for-gas-how-does-this-call-get-to-the-destination-chain">Who is paying for gas? How does this Call get to the destination chain</a></h4>
<p>At this level, the <strong>InteropCall</strong> acts like a hitchhiker — it relies on someone (anyone) to pick it up, execute it, and
pay for the gas!</p>
<p><img src="specs/interop/../img/callride.png" alt="callride.png" /></p>
<p>While any transaction on the destination chain can simply call <code>InteropCenter.executeInteropBundle</code>, if you don’t want
to rely on hitchhiking, you can create one yourself. We’ll discuss this in the section about <strong>Interop Transactions</strong>.</p>
<h2 id="bundles"><a class="header" href="#bundles">Bundles</a></h2>
<p>Before we proceed to discuss <strong>InteropTransactions</strong>, there is one more layer in between: <strong>InteropBundles</strong>.</p>
<p><img src="specs/interop/../img/interopcallbundle.png" alt="interopcallbundle.png" /></p>
<p><strong>Bundles Offer:</strong></p>
<ul>
<li><strong>Shared Fate</strong>: All calls in the bundle either succeed or fail together.</li>
<li><strong>Retries</strong>: If a bundle fails, it can be retried (e.g., with more gas).</li>
<li><strong>Cancellations</strong>: If a bundle has not been successfully executed yet, it can be cancelled.</li>
</ul>
<p>If you look closely at the interface we used earlier, you’ll notice that we were already discussing the execution of
<strong>Bundles</strong> rather than single calls. So, let’s dive into what bundles are and the role they fulfill.</p>
<p>The primary purpose of a bundle is to ensure that a given list of calls is executed in a specific order and has a shared
fate (i.e., either all succeed or all fail).</p>
<p>In this sense, you can think of a bundle as a <strong>“multicall”</strong>, but with two key differences:</p>
<ol>
<li>
<p>You cannot “unbundle” items—an individual <code>InteropCall</code> cannot be run independently; it is tightly tied to the
bundle.</p>
</li>
<li>
<p>Each <code>InteropCall</code> within a bundle can use a different aliased account, enabling separate permissions for each call.</p>
</li>
</ol>
<pre><code class="language-solidity">contract InteropCenter {
 struct InteropBundle {
  // Calls have to be done in this order.
  InteropCall calls[];
  uint256 destinationChain;

  // If not set - anyone can execute it.
  address executionAddresses[];
  // Who can 'cancel' this bundle.
  address cancellationAddress;
 }

 // Starts a new bundle.
 // All the calls that will be added to this bundle (potentially by different contracts)
 // will have a 'shared fate'.
 // The whole bundle must be going to a single destination chain.
 function startBundle(destinationChain) returns bundleId;
 // Adds a new call to the opened bundle.
 // Returns the messageId of this single message in the bundle.
 function addToBundle(bundleId, destinationAddress, calldata, msgValue) return msgHash;
 // Finishes a given bundle, and sends it.
 function finishAndSendBundle(bundleId) return msgHash;
}
</code></pre>
<h3 id="cross-chain-swap-example"><a class="header" href="#cross-chain-swap-example">Cross Chain Swap Example</a></h3>
<p>Imagine you want to perform a swap on chain B, exchanging USDC for PEPE, but all your assets are currently on chain A.</p>
<p>This process would typically involve four steps:</p>
<ol>
<li>Transfer USDC from chain A to chain B.</li>
<li>Set allowance for the swap.</li>
<li>Execute the swap.</li>
<li>Transfer PEPE back to chain A.</li>
</ol>
<p>Each of these steps is a separate “call,” but you need them to execute in exactly this order and, ideally, atomically.
If the swap fails, you wouldn’t want the allowance to remain set on the destination chain.</p>
<p>Below is an example of how this process could look (note that the code is pseudocode; we’ll explain the helper methods
required to make it work in a later section).</p>
<pre><code class="language-solidity">bundleId = InteropCenter(INTEROP_CENTER).startBundle(chainD);
// This will 'burn' the 1k USDC, create the special interopCall
// when this call is executed on chainD, it will mint 1k USDC there.
// BUT - this interopCall is tied to this bundle id.
USDCBridge.transferWithBundle(
  bundleId,
  chainD,
  aliasedAccount(this(account), block.chain_id),
  1000);


// This will create interopCall to set allowance.
InteropCenter.addToBundle(bundleId,
            USDCOnDestinationChain,
            createCalldata("approve", 1000, poolOnDestinationChain),
            0);
// This will create interopCall to do the swap.
InteropCenter.addToBundle(bundleId,
            poolOnDestinationChain,
            createCalldata("swap", "USDC_PEPE", 1000, ...),
            0)
// And this will be the interopcall to transfer all the assets back.
InteropCenter.addToBundle(bundleId,
            pepeBridgeOnDestinationChain,
            createCalldata("transferAll", block.chain_id, this(account)),
            0)


bundleHash = interopCenter.finishAndSendBundle(bundleId);
</code></pre>
<p>In the code above, we created a bundle that anyone can execute on the destination chain. This bundle will handle the
entire process: minting, approving, swapping, and transferring back.</p>
<h3 id="bundle-restrictions"><a class="header" href="#bundle-restrictions">Bundle Restrictions</a></h3>
<p>When starting a bundle, if you specify the <code>executionAddress</code>, only that account will be able to execute the bundle on
the destination chain. If no <code>executionAddress</code> is specified, anyone can trigger the execution.</p>
<h2 id="retries-and-cancellations"><a class="header" href="#retries-and-cancellations">Retries and Cancellations</a></h2>
<p>If bundle execution fails — whether due to a contract error or running out of gas—none of its calls will be applied. The
bundle can be re-run on the <strong>destination chain</strong> without requiring any updates or notifications to the source chain.
More details about retries and gas will be covered in the next level, <strong>Interop Transactions</strong>.</p>
<p>This process can be likened to a “hitchhiker” (or in the case of a bundle, a group of hitchhikers) — if the car they’re
traveling in doesn’t reach the destination, they simply find another ride rather than returning home.</p>
<p>However, there are cases where the bundle should be cancelled. Cancellation can be performed by the
<code>cancellationAddress</code> specified in the bundle itself.</p>
<h4 id="for-our-cross-chain-swap-example"><a class="header" href="#for-our-cross-chain-swap-example">For our cross chain swap example</a></h4>
<ol>
<li>Call <code>cancelInteropBundle(interopMessage, proof)</code> on the destination chain.
<ul>
<li>A helper method for this will be introduced in the later section.</li>
</ul>
</li>
<li>When cancellation occurs, the destination chain will generate an <code>InteropMessage</code> containing cancellation
information.</li>
<li>Using the proof from this method, the user can call the USDC bridge to recover their assets:</li>
</ol>
<pre><code class="language-solidity">USDCBridge.recoverFailedTransfer(bundleId, cancellationMessage, proof);
</code></pre>
<h3 id="some-details-on-our-approach"><a class="header" href="#some-details-on-our-approach">Some details on our approach</a></h3>
<h4 id="destination-contract"><a class="header" href="#destination-contract">Destination Contract</a></h4>
<ul>
<li>On ElasticChain, the destination contract does not need to know it is being called via an interop call. Requests
arrive from `aliased accounts’.</li>
</ul>
<h4 id="batching"><a class="header" href="#batching">Batching</a></h4>
<ul>
<li>ElasticChain supports bundling of messages, ensuring shared fate and strict order.</li>
</ul>
<h4 id="execution-permissions"><a class="header" href="#execution-permissions">Execution Permissions</a></h4>
<ul>
<li>ElasticChain allows restricting who can execute the call or bundle on the destination chain.</li>
</ul>
<h4 id="cancellations"><a class="header" href="#cancellations">Cancellations</a></h4>
<ul>
<li>ElasticChain supports restricting who can cancel. Cancellation can happen at any time.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interop-transactions"><a class="header" href="#interop-transactions">Interop Transactions</a></h1>
<h2 id="basics-1"><a class="header" href="#basics-1">Basics</a></h2>
<p>The <strong>InteropTransaction</strong> sits at the top of our interop stack, acting as the “delivery” mechanism for <strong>Interop
Bundles</strong>.</p>
<p>Think of it like a car that picks up our “hitchhiker” bundles and carries them to their destination.</p>
<p><img src="specs/interop/../img/interoptx.png" alt="interoptx.png" /></p>
<p><strong>Note:</strong> Interop Transactions aren’t the only way to execute a bundle. Once an interop bundle is created on the source
chain, users can simply send a regular transaction on the destination chain to execute it.</p>
<p>However, this approach can be inconvenient as it requires users to have funds on the destination chain to cover gas fees
and to configure the necessary network settings (like the RPC address).</p>
<p><strong>InteropTransactions</strong> simplify this process by handling everything from the source chain. They allow you to select
which <strong>interopBundle</strong> to execute, specify gas details (such as gas amount and gas price), and determine who will cover
the gas costs. This can be achieved using tokens on the source chain or through a paymaster.</p>
<p>Once configured, the transaction will automatically execute, either by the chain operator, the gateway, or off-chain
tools.</p>
<p>An <strong>InteropTransaction</strong> contains two pointers to bundles:</p>
<ul>
<li><strong>feesBundle</strong>: Holds interop calls to cover fees.</li>
<li><strong>bundleHash</strong>: Contains the main execution.</li>
</ul>
<p><img src="specs/interop/../img/ipointers.png" alt="ipointers.png" /></p>
<h2 id="interface-1"><a class="header" href="#interface-1">Interface</a></h2>
<p>The function <code>sendInteropTransaction</code> provides all the options. For simpler use cases, refer to the helper methods
defined later in the article.</p>
<pre><code class="language-solidity">contract InteropCenter {
  /// Creates a transaction that will attempt to execute a given Bundle on the destination chain.
  /// Such transaction can be 'picked up' by the destination chain automatically.
  /// This function covers all the cases - we expect most users to use the helper
  /// functions defined later.
 function sendInteropTransaction(
  destinationChain,
  bundleHash,        // the main bundle that you want to execute on destination chain
  gasLimit,          // gasLimit &amp; price for execution
  gasPrice,
  feesBundleHash,  // this is the bundle that contains the calls to pay for gas
  destinationPaymaster,  // optionally - you can use a paymaster on destination chain
  destinationPaymasterInput); // with specific params


 struct InteropTransaction {
  address sourceChainSender
  uint256 destinationChain
   uint256 gasLimit;
   uint256 gasPrice;
   uint256 value;
   bytes32 bundleHash;
   bytes32 feesBundleHash;
    address destinationPaymaster;
   bytes destinationPaymasterInput;
 }
}
</code></pre>
<p>After creating the <strong>InteropBundle</strong>, you can simply call <code>sendInteropTransaction</code> to create the complete transaction
that will execute the bundle.</p>
<h2 id="retries"><a class="header" href="#retries">Retries</a></h2>
<p>If your transaction fails to execute the bundle (e.g., due to a low gas limit) or isn’t included at all (e.g., due to
too low gasPrice), you can send another transaction to <strong>attempt to execute the same bundle again</strong>.</p>
<p>Simply call <code>sendInteropTransaction</code> again with updated gas settings.</p>
<h3 id="example-of-retrying"><a class="header" href="#example-of-retrying">Example of Retrying</a></h3>
<p>Here’s a concrete example: Suppose you created a bundle to perform a swap that includes transferring 100 ETH, executing
the swap, and transferring some tokens back.</p>
<p>You attempted to send the interop transaction with a low gas limit (e.g., 100). Since you didn’t have any base tokens on
the destination chain, you created a separate bundle to transfer a small fee (e.g., 0.0001) to cover the gas.</p>
<p>You sent your first interop transaction to the destination chain, but it failed due to insufficient gas. However, your
“fee bundle” was successfully executed, as it covered the gas cost for the failed attempt.</p>
<p>Now, you have two options: either cancel the execution bundle (the one with 100 ETH) or retry.</p>
<p>To retry, you decide to set a higher gas limit (e.g., 10,000) and create another fee transfer (e.g., 0.01) but use <strong>the
same execution bundle</strong> as before.</p>
<p>This time, the transaction succeeds — the swap completes on the destination chain, and the resulting tokens are
successfully transferred back to the source chain.</p>
<p><img src="specs/interop/../img/retryexample.png" alt="retryexample.png" /></p>
<h2 id="fees--restrictions"><a class="header" href="#fees--restrictions">Fees &amp; Restrictions</a></h2>
<p>Using an <strong>InteropBundle</strong> for fee payments offers flexibility, allowing users to transfer a small amount to cover the
fees while keeping the main assets in the execution bundle itself.</p>
<h3 id="restrictions"><a class="header" href="#restrictions">Restrictions</a></h3>
<p>This flexibility comes with trade-offs, similar to the validation phases in <strong>Account Abstraction</strong> or <strong>ERC4337</strong>,
primarily designed to prevent DoS attacks. Key restrictions include:</p>
<ul>
<li><strong>Lower gas limits</strong></li>
<li><strong>Limited access to specific slots</strong></li>
</ul>
<p>Additionally, when the <code>INTEROP_CENTER</code> constructs an <strong>InteropTransaction</strong>, it enforces extra restrictions on
<strong>feePaymentBundles</strong>:</p>
<ul>
<li><strong>Restricted Executors</strong>:<br />
Only your <strong>AliasedAccount</strong> on the receiving side can execute the <code>feePaymentBundle</code>.</li>
</ul>
<p>This restriction is crucial for security, preventing others from executing your <strong>fee bundle</strong>, which could cause your
transaction to fail and prevent the <strong>execution bundle</strong> from processing.</p>
<h3 id="types-of-fees"><a class="header" href="#types-of-fees"><strong>Types of Fees</strong></a></h3>
<h4 id="using-the-destination-chains-base-token"><a class="header" href="#using-the-destination-chains-base-token">Using the Destination Chain’s Base Token</a></h4>
<p>The simplest scenario is when you (as the sender) already have the destination chain’s base token available on the
source chain.</p>
<p>For example:</p>
<ul>
<li>If you are sending a transaction from <strong>Era</strong> (base token: ETH) to <strong>Sophon</strong> (base token: SOPH) and already have SOPH
on ERA, you can use it for the fee.</li>
</ul>
<p>To make this easier, we’ll provide a helper function:</p>
<pre><code class="language-solidity">contract InteropCenter {
  // Creates InteropTransaction to the destination chain with payment with base token.
  // Before calling, you have to 'approve' InteropCenter to the ERC20/Bridge that holds the destination chain's base tokens.
  // or if the destination chain's tokens are the same as yours, just attach value to this call.
  function sendInteropTxMinimal(
   destinationChain,
   bundleHash,        // the main bundle that you want to execute on destination chain
   gasLimit,          // gasLimit &amp; price for execution
   gasPrice,
  );
 }
</code></pre>
<h4 id="using-paymaster-on-the-destination-chain"><a class="header" href="#using-paymaster-on-the-destination-chain">Using paymaster on the destination chain</a></h4>
<p>If you don’t have the base token from the destination chain (e.g., SOPH in our example) on your source chain, you’ll
need to use a paymaster on the destination chain instead.</p>
<p>In this case, you’ll send the token you do have (e.g., USDC) to the destination chain as part of the <strong>feeBundleHash</strong>.
Once there, you’ll use it to pay the paymaster on the destination chain to cover your gas fees.</p>
<p>Your <strong>InteropTransaction</strong> would look like this:</p>
<p><img src="specs/interop/../img/paymastertx.png" alt="paymastertx.png" /></p>
<h2 id="automatic-execution"><a class="header" href="#automatic-execution"><strong>Automatic Execution</strong></a></h2>
<p>One of the main advantages of <strong>InteropTransactions</strong> is that they execute automatically. As the sender on the source
chain, you don’t need to worry about technical details like RPC addresses or obtaining proofs — it’s all handled for
you.</p>
<p>After creating an <strong>InteropTransaction</strong>, it can be relayed to the destination chain by anyone. The transaction already
includes a signature (also known as an interop message proof), making it fully self-contained and ready to send without
requiring additional permissions.</p>
<p>Typically, the destination chain’s operator will handle and include incoming <strong>InteropTransactions</strong>. However, if they
don’t, the <strong>Gateway</strong> or other participants can step in to prepare and send them.</p>
<p>You can also use the available tools to create and send the destination transaction yourself. Since the transaction is
self-contained, it doesn’t require additional funds or signatures to execute.</p>
<p><img src="specs/interop/../img/autoexecution.png" alt="Usually destination chain operator will keep querying gateway to see if there are any messages for their chain." /></p>
<p>Once they see the message, they can request the proof from the <strong>Gateway</strong> and also fetch the <strong>InteropBundles</strong>
contained within the message (along with their respective proofs).</p>
<p><img src="specs/interop/../img/chainop.png" alt="Operator getting necessary data from Gateway." /></p>
<p>As the final step, the operator can use the received data to create a regular transaction, which can then be sent to
their chain.</p>
<p><img src="specs/interop/../img/finaltx.png" alt="Creating the final transaction to send to the destination chain" /></p>
<p>The steps above don’t require any special permissions and can be executed by anyone.</p>
<p>While the <strong>Gateway</strong> was used above for tasks like providing proofs, if the Gateway becomes malicious, all this
information can still be constructed off-chain using data available on L1.</p>
<h3 id="how-it-works-under-the-hood"><a class="header" href="#how-it-works-under-the-hood">How it Works Under the hood</a></h3>
<p>We’ll modify the default account to accept interop proofs as signatures, seamlessly integrating with the existing ZKSync
native <strong>Account Abstraction</strong> model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-development-announcement"><a class="header" href="#zksync-development-announcement">ZKsync development announcement</a></h1>
<p>This directory will contain announcements that don’t necessarily serve as documentation, but still provide valuable
information to be stored long-term.</p>
<p>Current announcements:</p>
<ul>
<li>01.08.2024 - <a href="announcements/./attester_commitee.html">Attester committee invitation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attester-committee"><a class="header" href="#attester-committee">Attester Committee</a></h1>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>The Attester committee is a subset of ZKSync nodes. After each l1 batch execution, participating nodes sign its
execution result and send back to the network.</p>
<p>The ultimate goal is to make L1 commit operation contingent on such signatures. This will improve the security and
finality guarantees: having these signatures on L1 shows that additional actors executed the corresponding blocks - and
ended up with the same state root hash.</p>
<h2 id="current-state"><a class="header" href="#current-state">Current State</a></h2>
<p>Before starting the L1 integration, we want to ensure that we can to consistently reach the quorum and collect the
signatures in a timely manner. Currently the main node just stores the signatures in the local DB
(<code>l1_batches_consensus</code> table).</p>
<p>We run a (temporary) PoA network of attesters where the Main Node administrator defines the committee for every L1
batch. There is currently no tangible incentive or any kind of actual or implied reward for the participants - we’ll be
developing these and potential tokenomics later on.</p>
<p>We are looking for participants to this network.</p>
<h2 id="participating-in-the-attester-committee"><a class="header" href="#participating-in-the-attester-committee">Participating in the Attester Committee</a></h2>
<p>Joining the attester committee imposes no additional computational, operational, security, or business overhead for EN
operators.</p>
<p>The only difference in behavior is that the node would asynchronously sign batches and send those signatures to the main
node. Node checks if its public key is part of the committee for the current l1 batch - if it is, this logic kicks in.
We expect the participating nodes to have very high uptime, but there are no penalties for not submitting votes (and we
wouldn’t have any way to impose this).</p>
<p>Participants can leave the committee at any time.</p>
<p>The only action that is required to participate is to share your attester public key with the Main Node operator (by
opening an issue in this repo or using any other communication channel). You can find it in the comment in the
<code>consensus_secrets.yaml</code> file (that was - in most cases - generated by the tool described
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/guides/external-node/10_decentralization.md#generating-secrets">here</a>)</p>
<blockquote>
<p>[!WARNING]</p>
<p>Never share your <strong>private</strong> keys. Make sure you are only sharing the <strong>public</strong> key. It looks like this:
<code># attester:public:secp256k1:02e7b1f24fb58b770cb722bf08e9512c7d8667ec0befa37611eddafd0109656132</code></p>
</blockquote>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="editor.js"></script>
        <script src="mode-rust.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="js/version-box.js"></script>
        <script src="js/mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
