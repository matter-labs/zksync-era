//! Consensus-related functionality.

#![allow(clippy::redundant_locals)]

use zksync_concurrency::{ctx, error::Wrap as _, scope, time};
use zksync_consensus_executor as executor;
use zksync_consensus_roles::validator;
use zksync_consensus_storage::BlockStore;
use zksync_dal::ConnectionPool;

use self::storage::Store;
use crate::sync_layer::{sync_action::ActionQueueSender, MainNodeClient, SyncState};

pub mod config;
pub mod proto;
mod storage;
#[cfg(test)]
pub(crate) mod testonly;
#[cfg(test)]
mod tests;

/// Main node consensus config.
#[derive(Debug, Clone)]
pub struct MainNodeConfig {
    pub executor: executor::Config,
    pub validator: executor::ValidatorConfig,
}

impl MainNodeConfig {
    /// Task generating consensus certificates for the miniblocks generated by `StateKeeper`.
    /// Broadcasts the blocks with certificates to gossip network peers.
    pub async fn run(self, ctx: &ctx::Ctx, pool: ConnectionPool) -> anyhow::Result<()> {
        anyhow::ensure!(
            self.executor.validators
                == validator::ValidatorSet::new(vec![self.validator.key.public()]).unwrap(),
            "currently only consensus with just 1 validator is supported"
        );
        scope::run!(&ctx, |ctx, s| async {
            let store = Store::new(pool);
            let mut block_store = store.clone().into_block_store();
            block_store
                .try_init_genesis(ctx, &self.validator.key)
                .await
                .wrap("block_store.try_init_genesis()")?;
            let (block_store, runner) = BlockStore::new(ctx, Box::new(block_store))
                .await
                .wrap("BlockStore::new()")?;
            s.spawn_bg(runner.run(ctx));
            let executor = executor::Executor {
                config: self.executor,
                block_store,
                validator: Some(executor::Validator {
                    config: self.validator,
                    replica_store: Box::new(store.clone()),
                    payload_manager: Box::new(store.clone()),
                }),
            };
            executor.run(ctx).await
        })
        .await
    }
}

/// Periodically fetches the head of the main node
/// and updates `SyncState` accordingly.
pub async fn run_main_node_state_fetcher(
    ctx: &ctx::Ctx,
    client: &dyn MainNodeClient,
    sync_state: &SyncState,
) -> ctx::OrCanceled<()> {
    const DELAY_INTERVAL: time::Duration = time::Duration::milliseconds(500);
    const RETRY_DELAY_INTERVAL: time::Duration = time::Duration::seconds(5);
    loop {
        match ctx.wait(client.fetch_l2_block_number()).await? {
            Ok(head) => {
                sync_state.set_main_node_block(head);
                ctx.sleep(DELAY_INTERVAL).await?;
            }
            Err(err) => {
                tracing::warn!("main_node_client.fetch_l2_block_number(): {err}");
                ctx.sleep(RETRY_DELAY_INTERVAL).await?;
            }
        }
    }
}

/// External node consensus config.
#[derive(Debug, Clone)]
pub struct FetcherConfig {
    pub executor: executor::Config,
}

impl FetcherConfig {
    /// Task fetching L2 blocks using peer-to-peer gossip network.
    pub async fn run(
        self,
        ctx: &ctx::Ctx,
        pool: ConnectionPool,
        actions: ActionQueueSender,
    ) -> anyhow::Result<()> {
        scope::run!(ctx, |ctx, s| async {
            let store = Store::new(pool);
            let mut block_store = store.clone().into_block_store();
            block_store
                .set_actions_queue(ctx, actions)
                .await
                .wrap("block_store.set_actions_queue()")?;
            let (block_store, runner) = BlockStore::new(ctx, Box::new(block_store))
                .await
                .wrap("BlockStore::new()")?;
            s.spawn_bg(runner.run(ctx));
            let executor = executor::Executor {
                config: self.executor,
                block_store,
                validator: None,
            };
            executor.run(ctx).await
        })
        .await
    }
}
