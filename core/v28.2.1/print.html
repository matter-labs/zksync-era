<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ZKsync Era Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="css/version-box.css">
        <link rel="stylesheet" href="./css/mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ZKsync Era Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/matter-labs/zksync-era/tree/main/docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the documentation! This guide provides comprehensive insights into the architecture, setup, usage, and
advanced features of ZKsync.</p>
<h2 id="documentation-structure"><a class="header" href="#documentation-structure">Documentation Structure</a></h2>
<ul>
<li>
<p><strong>Guides</strong>: The Guides section is designed to help users at every level, from setup and development to advanced
configuration and debugging techniques. It covers essential topics, including Docker setup, repository management, and
architecture.</p>
</li>
<li>
<p><strong>Specs</strong>: This section dives into the technical specifications of our system. Here, you’ll find detailed
documentation on data availability, L1 and L2 communication, smart contract interactions, Zero-Knowledge proofs, and
more. Each topic includes an in-depth explanation to support advanced users and developers.</p>
</li>
<li>
<p><strong>Announcements</strong>: This section highlights important updates, announcements, and committee details, providing
essential information to keep users informed on the latest changes.</p>
</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>Feel free to explore each section according to your needs. This documentation is designed to be modular, so you can jump
to specific topics or follow through step-by-step.</p>
<hr />
<p>Thank you for using our documentation!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-basic-guides"><a class="header" href="#zksync-basic-guides">ZKsync basic guides</a></h1>
<p>This section contains basic guides that aim to explain the ZKsync ecosystem in an easy to grasp way.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="guides/./architecture.html">Architecture</a></li>
<li><a href="guides/./build-docker.html">Build Docker</a></li>
<li><a href="guides/./development.html">Development</a></li>
<li><a href="guides/./launch.html">Launch</a></li>
<li><a href="guides/./repositories.html">Repositories</a></li>
<li><a href="guides/./setup-dev.html">Setup Dev</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-dependencies"><a class="header" href="#installing-dependencies">Installing dependencies</a></h1>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>This is a shorter version of setup guide to make it easier subsequent initializations. If it’s the first time you’re
initializing the workspace, it’s recommended that you read the whole guide below, as it provides more context and tips.</p>
<p>If you run on ‘clean’ Ubuntu on GCP:</p>
<pre><code class="language-bash"># For VMs only! They don't have SSH keys, so we override SSH with HTTPS
git config --global url."https://github.com/".insteadOf git@github.com:
git config --global url."https://".insteadOf git://

# Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# NVM
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash

# All necessary stuff
sudo apt-get update
sudo apt-get install -y build-essential pkg-config cmake clang lldb lld libssl-dev libpq-dev apt-transport-https ca-certificates curl software-properties-common

# Install docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
sudo apt install docker-ce
sudo usermod -aG docker ${USER}

# Start docker.
sudo systemctl start docker

## You might need to re-connect (due to usermod change).

# Node &amp; yarn
nvm install 20
# Important: there will be a note in the output to load
# new paths in your local session, either run it or reload the terminal.
npm install -g yarn
yarn set version 1.22.19

# For running unit tests
cargo install cargo-nextest
# SQL tools
cargo install sqlx-cli --version 0.8.1

# Foundry ZKsync
curl -L https://raw.githubusercontent.com/matter-labs/foundry-zksync/main/install-foundry-zksync | bash
foundryup-zksync

# Non CUDA (GPU) setup, can be skipped if the machine has a CUDA installed for provers
# Don't do that if you intend to run provers on your machine. Check the prover docs for a setup instead.
echo "export ZKSYNC_USE_CUDA_STUBS=true" &gt;&gt; ~/.bashrc
# You will need to reload your `*rc` file here

# Clone the repo to the desired location
git clone git@github.com:matter-labs/zksync-era.git
cd zksync-era
git submodule update --init --recursive
</code></pre>
<p>Don’t forget to look at <a href="guides/setup-dev.html#tips">tips</a>.</p>
<h2 id="supported-operating-systems"><a class="header" href="#supported-operating-systems">Supported operating systems</a></h2>
<p>ZKsync currently can be launched on any *nix operating system (e.g. any linux distribution or macOS).</p>
<p>If you’re using Windows, then make sure to use WSL 2.</p>
<p>Additionally, if you are going to use WSL 2, make sure that your project is located in the <em>linux filesystem</em>, since
accessing NTFS partitions from within WSL is very slow.</p>
<p>If you’re using macOS with an ARM processor (e.g. M1/M2), make sure that you are working in the <em>native</em> environment
(e.g., your terminal and IDE don’t run in Rosetta, and your toolchain is native). Trying to work with ZKsync code via
Rosetta may cause problems that are hard to spot and debug, so make sure to check everything before you start.</p>
<p>If you are a NixOS user or would like to have a reproducible environment, skip to the section about <code>nix</code>.</p>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>Install <code>docker</code>. It is recommended to follow the instructions from the
<a href="https://docs.docker.com/install/">official site</a>.</p>
<p>Note: currently official site proposes using Docker Desktop for Linux, which is a GUI tool with plenty of quirks. If you
want to only have CLI tool, you need the <code>docker-ce</code> package and you can follow
<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04">this guide</a> for Ubuntu.</p>
<p>Installing <code>docker</code> via <code>snap</code> or from the default repository can cause troubles.</p>
<p>You need to install both <code>docker</code> and <code>docker compose</code>.</p>
<p><strong>Note:</strong> <code>docker compose</code> is installed automatically with <code>Docker Desktop</code>.</p>
<p><strong>Note:</strong> On linux you may encounter the following error when you’ll try to work with <code>zksync</code>:</p>
<pre><code>ERROR: Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
</code></pre>
<p>If so, you <strong>do not need</strong> to install <code>docker-machine</code>. Most probably, it means that your user is not added to
the<code>docker</code> group. You can check it as follows:</p>
<pre><code class="language-bash">docker-compose up # Should raise the same error.
sudo docker-compose up # Should start doing things.
</code></pre>
<p>If the first command fails, but the second succeeds, then you need to add your user to the <code>docker</code> group:</p>
<pre><code class="language-bash">sudo usermod -a -G docker your_user_name
</code></pre>
<p>After that, you should logout and login again (user groups are refreshed after the login). The problem should be solved
at this step.</p>
<p>If logging out does not resolve the issue, restarting the computer should.</p>
<h2 id="nodejs--yarn"><a class="header" href="#nodejs--yarn">Node.js &amp; Yarn</a></h2>
<ol>
<li>Install <code>Node</code> (requires version <code>v20</code>). The recommended way is via <a href="https://github.com/nvm-sh/nvm">nvm</a>.</li>
<li>Install <code>yarn</code>. Can be done via <code>npm install -g yarn</code>. Make sure to get version 1.22.19 - you can change the version
by running <code>yarn set version 1.22.19</code>.</li>
</ol>
<h2 id="clang"><a class="header" href="#clang">clang</a></h2>
<p>In order to compile RocksDB, you must have LLVM available. On debian-based linux it can be installed as follows:</p>
<p>On debian-based linux:</p>
<pre><code class="language-bash">sudo apt-get install build-essential pkg-config cmake clang lldb lld
</code></pre>
<p>On macOS:</p>
<p>You need to have an up-to-date <code>Xcode</code>. You can install it directly from <code>App Store</code>. With Xcode command line tools, you
get the Clang compiler installed by default. Thus, having XCode you don’t need to install <code>clang</code>.</p>
<h2 id="openssl"><a class="header" href="#openssl">OpenSSL</a></h2>
<p>Install OpenSSL:</p>
<p>On mac:</p>
<pre><code class="language-bash">brew install openssl
</code></pre>
<p>On debian-based linux:</p>
<pre><code class="language-bash">sudo apt-get install libssl-dev
</code></pre>
<h2 id="rust"><a class="header" href="#rust">Rust</a></h2>
<p>Install <code>Rust</code>’s toolchain version reported in <code>/rust-toolchain.toml</code> (also a later stable version should work).</p>
<p>Instructions can be found on the <a href="https://www.rust-lang.org/tools/install">official site</a>.</p>
<p>Verify the <code>rust</code> installation:</p>
<pre><code class="language-bash">rustc --version
rustc 1.xx.y (xxxxxx 20xx-yy-zz) # Output may vary depending on actual version of rust
</code></pre>
<p>If you are using macOS with ARM processor (e.g. M1/M2), make sure that you use an <code>aarch64</code> toolchain. For example, when
you run <code>rustup show</code>, you should see a similar input:</p>
<pre><code class="language-bash">rustup show
Default host: aarch64-apple-darwin
rustup home:  /Users/user/.rustup

installed toolchains
--------------------

...

active toolchain
----------------

1.67.1-aarch64-apple-darwin (overridden by '/Users/user/workspace/zksync-era/rust-toolchain')
</code></pre>
<p>If you see <code>x86_64</code> mentioned in the output, probably you’re running (or used to run) your IDE/terminal in Rosetta. If
that’s the case, you should probably change the way you run terminal, and/or reinstall your IDE, and then reinstall the
Rust toolchain as well.</p>
<h2 id="postgresql-client-library"><a class="header" href="#postgresql-client-library">PostgreSQL Client Library</a></h2>
<p>For development purposes, you typically only need the PostgreSQL client library, not the full server installation.
Here’s how to install it:</p>
<p>On macOS:</p>
<pre><code class="language-bash">brew install libpq
</code></pre>
<p>On Debian-based Linux:</p>
<pre><code class="language-bash">sudo apt-get install libpq-dev
</code></pre>
<h3 id="cargo-nextest"><a class="header" href="#cargo-nextest">Cargo nextest</a></h3>
<p><a href="https://nexte.st/">cargo-nextest</a> is the next-generation test runner for Rust projects. <code>zkstack dev test rust</code> uses
<code>cargo nextest</code> by default.</p>
<pre><code class="language-bash">cargo install cargo-nextest
</code></pre>
<h3 id="sqlx-cli"><a class="header" href="#sqlx-cli">SQLx CLI</a></h3>
<p>SQLx is a Rust library we use to interact with Postgres, and its CLI is used to manage DB migrations and support several
features of the library.</p>
<pre><code class="language-bash">cargo install --locked sqlx-cli --version 0.8.1
</code></pre>
<h2 id="easier-method-using-nix"><a class="header" href="#easier-method-using-nix">Easier method using <code>nix</code></a></h2>
<p>Nix is a tool that can fetch <em>exactly</em> the right dependencies specified via hashes. The current config is Linux-only but
it is likely that it can be adapted to Mac.</p>
<p>Install <code>nix</code>. Enable the nix command and flakes.</p>
<p>Install docker, rustup and use rust to install SQLx CLI like described above. If you are on NixOS, you also need to
enable nix-ld.</p>
<p>Go to the zksync folder and run <code>nix develop</code>. After it finishes, you are in a shell that has all the dependencies.</p>
<h2 id="foundry-zksync"><a class="header" href="#foundry-zksync">Foundry ZKsync</a></h2>
<p>ZKSync depends on Foundry ZKsync (which is is a specialized fork of Foundry, tailored for ZKsync). Please follow this
<a href="https://foundry-book.zksync.io/getting-started/installation">installation guide</a> to get started with Foundry ZKsync.</p>
<p>Foundry ZKsync can also be used for deploying smart contracts. For commands related to deployment, you can pass flags
for Foundry integration.</p>
<h2 id="non-gpu-setup"><a class="header" href="#non-gpu-setup">Non-GPU setup</a></h2>
<p>Circuit Prover requires a CUDA bindings to run. If you still want to be able to build everything locally on non-CUDA
setup, you’ll need use CUDA stubs.</p>
<p>For a single run, it’s enough to export it on the shell:</p>
<pre><code>export ZKSYNC_USE_CUDA_STUBS=true
</code></pre>
<p>For persistent runs, you can echo it in your ~/.<shell>rc file</p>
<pre><code>echo "export ZKSYNC_USE_CUDA_STUBS=true" &gt;&gt; ~/.&lt;SHELL&gt;rc
</code></pre>
<p>Note that the same can be achieved with RUSTFLAGS (discouraged). The flag is <code>--cfg=no_cuda</code>. You can either set
RUSTFLAGS as env var, or pass it in <code>config.toml</code> (either project level or global). The config would need the following:</p>
<pre><code class="language-toml">[build]
rustflags = ["--cfg=no_cuda"]
</code></pre>
<h2 id="tips"><a class="header" href="#tips">Tips</a></h2>
<h3 id="tip-mold"><a class="header" href="#tip-mold">Tip: <code>mold</code></a></h3>
<p>Optionally, you may want to optimize the build time with the modern linker, <a href="https://github.com/rui314/mold"><code>mold</code></a>.</p>
<p>This linker will speed up the build times, which can be pretty big for Rust binaries.</p>
<p>Follow the instructions in the repo in order to install it and enable for Rust.</p>
<p>If you installed <code>mold</code> to <code>/usr/local/bin/mold</code>, then the quickest way to use it without modifying any files is:</p>
<pre><code class="language-bash">export RUSTFLAGS='-C link-arg=-fuse-ld=/usr/local/bin/mold'
export CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_LINKER="clang"
</code></pre>
<h3 id="tip-speeding-up-building-rocksdb"><a class="header" href="#tip-speeding-up-building-rocksdb">Tip: Speeding up building <code>RocksDB</code></a></h3>
<p>By default, each time you compile <code>rocksdb</code> crate, it will compile required C++ sources from scratch. It can be avoided
by using precompiled versions of library, and it will significantly improve your build times.</p>
<p>In order to do so, you can put compiled libraries to some persistent location, and add the following to your shell
configuration file (e.g. <code>.zshrc</code> or <code>.bashrc</code>):</p>
<pre><code>export ROCKSDB_LIB_DIR=&lt;library location&gt;
export SNAPPY_LIB_DIR=&lt;library location&gt;
</code></pre>
<p>Make sure that compiled libraries match the current version of RocksDB. One way to obtain them, is to compile the
project in the usual way once, and then take built libraries from
<code>target/{debug,release}/build/librocksdb-sys-{some random value}/out</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="development-guide"><a class="header" href="#development-guide">Development guide</a></h1>
<p>This document outlines the steps for setting up and working with ZKsync.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>If you haven’t already, install the prerequisites as described in <a href="guides/./setup-dev.html">Install Dependencies</a>.</p>
<h2 id="installing-the-local-zk-stack-cli"><a class="header" href="#installing-the-local-zk-stack-cli">Installing the local ZK Stack CLI</a></h2>
<p>To set up local development, begin by installing
<a href="https://github.com/matter-labs/zksync-era/blob/main/zkstack_cli/README.md">ZK Stack CLI</a>. From the project’s root
directory, run the following commands:</p>
<pre><code class="language-bash">cd ./zkstack_cli/zkstackup
./install --local
</code></pre>
<p>This installs <code>zkstackup</code> in your user binaries directory (e.g., <code>$HOME/.local/bin/</code>) and adds it to your <code>PATH</code>.</p>
<p>After installation, open a new terminal or reload your shell profile. From the project’s root directory, you can now
run:</p>
<pre><code class="language-bash">zkstackup --local
</code></pre>
<p>This command installs <code>zkstack</code> from the current source directory.</p>
<p>You can proceed to verify the installation and start familiarizing with the CLI by running:</p>
<pre><code class="language-bash">zkstack --help
</code></pre>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/development.html#admonition-note"></a>
</div>
<div>
<p>NOTE: Whenever you want to update you local installation with your changes, just rerun:</p>
<p><code>zkstackup --local</code></p>
<p>You might find convenient to add this alias to your shell profile:</p>
<p><code>alias zkstackup='zkstackup --path /path/to/zksync-era'</code></p>
</div>
</div>
<h2 id="configure-ecosystem"><a class="header" href="#configure-ecosystem">Configure Ecosystem</a></h2>
<p>The project root directory includes configuration files for an ecosystem with a single chain, <code>era</code>. To initialize the
ecosystem, first start the required containers:</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<p>Next, run:</p>
<pre><code class="language-bash">zkstack ecosystem init
</code></pre>
<p>These commands will guide you through the configuration options for setting up the ecosystem.</p>
<div id="admonition-note-1" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/development.html#admonition-note-1"></a>
</div>
<div>
<p>For local development only. You can also use the development defaults by supplying the <code>--dev</code> flag.</p>
</div>
</div>
<p>Initialization may take some time, but key steps (such as downloading and unpacking keys or setting up containers) only
need to be completed once.</p>
<p>To see more detailed output, you can run commands with the <code>--verbose</code> flag.</p>
<h2 id="cleanup"><a class="header" href="#cleanup">Cleanup</a></h2>
<p>To clean up the local ecosystem (e.g., removing containers and clearing the contract cache), run:</p>
<pre><code class="language-bash">zkstack dev clean all
</code></pre>
<p>You can then reinitialize the ecosystem as described in the <a href="guides/development.html#configure-ecosystem">Configure Ecosystem</a> section.</p>
<pre><code class="language-bash">zkstack containers
zkstack ecosystem init
</code></pre>
<h2 id="committing-changes"><a class="header" href="#committing-changes">Committing changes</a></h2>
<p><code>zksync</code> uses pre-commit and pre-push git hooks for basic code integrity checks. Hooks are set up automatically within
the workspace initialization process. These hooks will not allow to commit the code which does not pass several checks.</p>
<p>Currently the following criteria are checked:</p>
<ul>
<li>Code must be formatted via <code>zkstack dev fmt</code>.</li>
<li>Code must be linted via <code>zkstack dev lint</code>.</li>
</ul>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>ZKstack CLI offers multiple subcommands to run specific integration and unit test:</p>
<pre><code class="language-bash">zkstack dev test --help
</code></pre>
<pre><code class="language-bash">Usage: zkstack dev test [OPTIONS] &lt;COMMAND&gt;

Commands:
  integration   Run integration tests
  fees          Run fees test
  revert        Run revert tests
  recovery      Run recovery tests
  upgrade       Run upgrade tests
  build         Build all test dependencies
  rust          Run unit-tests, accepts optional cargo test flags
  l1-contracts  Run L1 contracts tests
  prover        Run prover tests
  wallet        Print test wallets information
  loadtest      Run loadtest
  help          Print this message or the help of the given subcommand(s)
</code></pre>
<h3 id="running-unit-tests"><a class="header" href="#running-unit-tests">Running unit tests</a></h3>
<p>You can run unit tests for the Rust crates in the project by running:</p>
<pre><code class="language-bash">zkstack dev test rust
</code></pre>
<h3 id="running-integration-tests"><a class="header" href="#running-integration-tests">Running integration tests</a></h3>
<p>Running integration tests is more complex. Some tests require a running server, while others need the system to be in a
specific state. Please refer to our CI scripts
<a href="https://github.com/matter-labs/zksync-era/blob/main/.github/workflows/ci-core-reusable.yml">ci-core-reusable.yml</a> to
have a better understanding of the process.</p>
<p>In simple terms, the integration-test workflow consists of three phases:</p>
<ol>
<li>
<p><strong>Initializing the ecosystem</strong></p>
<pre><code class="language-bash">zkstack dev clean all         # remove any previous state
zkstack containers            # start required Docker containers
zkstack ecosystem init        # set up blockchain and contracts
</code></pre>
</li>
<li>
<p><strong>Starting the server</strong></p>
<pre><code class="language-bash">zkstack server                # spin up the server
</code></pre>
<p>This command starts the server and occupies the current terminal window. Open a new terminal window for any
subsequent commands.</p>
</li>
<li>
<p><strong>Running integration tests</strong></p>
<pre><code class="language-bash">zkstack dev test integration  # run the integration tests
</code></pre>
</li>
</ol>
<blockquote>
<p><em>Note: This is a high-level summary and does not reflect every nuance in <code>ci-core-reusable.yml</code>.</em></p>
</blockquote>
<h3 id="running-upgrade-tests"><a class="header" href="#running-upgrade-tests">Running upgrade tests</a></h3>
<p>Similar to integration tests, the whole setup is complicated and it is recommended to refer to our CI scripts
<a href="https://github.com/matter-labs/zksync-era/blob/main/.github/workflows/ci-core-reusable.yml">ci-core-reusable.yml</a> to
have a better understanding of the process.</p>
<p>In simple terms, the upgrade-test workflow consists of three phases:</p>
<ol>
<li>
<p><strong>Initializing the ecosystem</strong><br />
Same as for integration tests workflow.</p>
</li>
<li>
<p><strong>Starting the server</strong><br />
Same as for integration tests workflow.</p>
</li>
<li>
<p><strong>Running integration tests</strong></p>
<pre><code class="language-bash">ZKSYNC_HOME=&lt;path_to_zksync_era&gt; zkstack dev test upgrade # run the upgrade tests
</code></pre>
</li>
</ol>
<blockquote>
<p><em>Note: This is a high-level summary and does not reflect every nuance in <code>ci-core-reusable.yml</code>.</em></p>
</blockquote>
<h3 id="running-load-tests"><a class="header" href="#running-load-tests">Running load tests</a></h3>
<p>The current load test implementation only supports the legacy bridge. To use it, you need to create a new chain with
legacy bridge support:</p>
<pre><code class="language-bash">zkstack chain create --legacy-bridge
zkstack chain init
</code></pre>
<p>After initializing the chain with a legacy bridge, you can run the load test against it.</p>
<pre><code class="language-bash">zkstack dev test loadtest
</code></pre>
<div id="admonition-warning" class="admonition admonish-warning" role="note" aria-labelledby="admonition-warning-title">
<div class="admonition-title">
<div id="admonition-warning-title">
<p>Warning</p>
</div>
<a class="admonition-anchor-link" href="guides/development.html#admonition-warning"></a>
</div>
<div>
<p>Never use legacy bridges in non-testing environments.</p>
</div>
</div>
<h2 id="contracts"><a class="header" href="#contracts">Contracts</a></h2>
<h3 id="build-contracts"><a class="header" href="#build-contracts">Build contracts</a></h3>
<p>Run:</p>
<pre><code class="language-bash">zkstack dev contracts --help
</code></pre>
<p>to see all the options.</p>
<h3 id="publish-source-code-on-etherscan"><a class="header" href="#publish-source-code-on-etherscan">Publish source code on Etherscan</a></h3>
<h4 id="verifier-options"><a class="header" href="#verifier-options">Verifier Options</a></h4>
<p>Most commands interacting with smart contracts support the same verification options as Foundry’s <code>forge</code> command. Just
double check if the following options are available in the subcommand:</p>
<pre><code class="language-bash">--verifier                  -- Verifier to use
--verifier-api-key          -- Verifier API key
--verifier-url              -- Verifier URL, if using a custom provider
</code></pre>
<h4 id="using-foundry"><a class="header" href="#using-foundry">Using Foundry</a></h4>
<p>You can use <code>foundry</code> to verify the source code of the contracts.</p>
<pre><code class="language-bash">forge verify-contract
</code></pre>
<p>Verifies a smart contract on a chosen verification provider.</p>
<p>You must provide:</p>
<ul>
<li>The contract address</li>
<li>The contract name or the path to the contract.</li>
<li>In case of Etherscan verification, you must also provide:
<ul>
<li>Your Etherscan API key, either by passing it as an argument or setting <code>ETHERSCAN_API_KEY</code></li>
</ul>
</li>
</ul>
<p>For more information check <a href="https://book.getfoundry.sh/reference/forge/forge-verify-contract">Foundry’s documentation</a>.</p>
<h2 id="how-to-generate-the-genesisyaml-file"><a class="header" href="#how-to-generate-the-genesisyaml-file">How to generate the <code>genesis.yaml</code> file</a></h2>
<p>To generate the <a href="https://github.com/matter-labs/zksync-era/blob/main//etc/env/file_based/genesis.yaml"><code>genesis.yaml</code></a>
file checkout to the desired <code>zksync-era</code> branch, <a href="guides/development.html#installing-the-local-zk-stack-cli">build <code>zkstack</code></a> from it,
<a href="guides/development.html#configure-ecosystem">configure ecosystem</a> and run the following command:</p>
<pre><code class="language-shell">zkstack dev generate-genesis
</code></pre>
<p>Which runs the <a href="https://github.com/matter-labs/zksync-era/tree/main/core/bin/genesis_generator"><code>genesis_generator</code></a>
package under the hood and generates the genesis file.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-application"><a class="header" href="#running-the-application">Running the application</a></h1>
<p>This document covers common scenarios for launching ZKsync applications set locally.</p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<p>Prepare dev environment prerequisites: see</p>
<p><a href="guides/./setup-dev.html">Installing dependencies</a></p>
<h2 id="setup-local-dev-environment"><a class="header" href="#setup-local-dev-environment">Setup local dev environment</a></h2>
<p>Run the required containers with:</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<p>Setup:</p>
<pre><code class="language-bash">zkstack ecosystem init
</code></pre>
<p>To completely reset the dev environment:</p>
<ul>
<li>
<p>Stop services:</p>
<pre><code class="language-bash">zkstack dev clean all
</code></pre>
</li>
<li>
<p>Repeat the setup procedure above</p>
<pre><code class="language-bash">zkstack containers
zkstack ecosystem init
</code></pre>
</li>
</ul>
<h3 id="run-observability-stack"><a class="header" href="#run-observability-stack">Run observability stack</a></h3>
<p>If you want to run <a href="https://github.com/stefanprodan/dockprom/">Dockprom</a> stack (Prometheus, Grafana) alongside other
containers - add <code>--observability</code> parameter during initialisation.</p>
<pre><code class="language-bash">zkstack containers --observability
</code></pre>
<p>or select <code>yes</code> when prompted during the interactive execution of the command.</p>
<p>That will also provision Grafana with
<a href="https://github.com/matter-labs/era-observability/tree/main/dashboards">era-observability</a> dashboards. You can then
access it at <code>http://127.0.0.1:3000/</code> under credentials <code>admin/admin</code>.</p>
<blockquote>
<p>If you don’t see any data displayed on the Grafana dashboards - try setting the timeframe to “Last 30 minutes”. You
will also have to have <code>jq</code> installed on your system.</p>
</blockquote>
<h2 id="ecosystem-configuration"><a class="header" href="#ecosystem-configuration">Ecosystem Configuration</a></h2>
<p>The ecosystem configuration is spread across multiple files and directories:</p>
<ol>
<li>
<p>Root level:</p>
<ul>
<li><code>ZkStack.yaml</code>: Main configuration file for the entire ecosystem.</li>
</ul>
</li>
<li>
<p><code>configs/</code> directory:</p>
<ul>
<li><code>apps/</code>:
<ul>
<li><code>portal_config.json</code>: Configuration for the portal application.</li>
</ul>
</li>
<li><code>contracts.yaml</code>: Defines smart contract settings and addresses.</li>
<li><code>erc20.yaml</code>: Configuration for ERC20 tokens.</li>
<li><code>initial_deployments.yaml</code>: Specifies initial ERC20 token deployments.</li>
<li><code>wallets.yaml</code>: Contains wallet configurations.</li>
</ul>
</li>
<li>
<p><code>chains/&lt;chain_name&gt;/</code> directory:</p>
<ul>
<li><code>artifacts/</code>: Contains build/execution artifacts.</li>
<li><code>configs/</code>: Chain-specific configuration files.
<ul>
<li><code>contracts.yaml</code>: Chain-specific smart contract settings.</li>
<li><code>external_node.yaml</code>: Configuration for external nodes.</li>
<li><code>general.yaml</code>: General chain configuration.</li>
<li><code>genesis.yaml</code>: Genesis configuration for the chain.</li>
<li><code>secrets.yaml</code>: Secrets and private keys for the chain.</li>
<li><code>wallets.yaml</code>: Wallet configurations for the chain.</li>
</ul>
</li>
<li><code>db/main/</code>: Database files for the chain.</li>
<li><code>ZkStack.yaml</code>: Chain-specific ZkStack configuration.</li>
</ul>
</li>
</ol>
<p>These configuration files are automatically generated during the ecosystem initialization (<code>zkstack ecosystem init</code>) and
chain initialization (<code>zkstack chain init</code>) processes. They control various aspects of the ZKsync ecosystem, including:</p>
<ul>
<li>Network settings</li>
<li>Smart contract deployments</li>
<li>Token configurations</li>
<li>Database settings</li>
<li>Application/Service-specific parameters</li>
</ul>
<p>It’s important to note that while these files can be manually edited, any changes may be overwritten if the ecosystem or
chain is reinitialized. Always back up your modifications and exercise caution when making direct changes to these
files.</p>
<p>For specific configuration needs, it’s recommended to use the appropriate <code>zkstack</code> commands or consult the
documentation for safe ways to customize your setup.</p>
<h2 id="build-and-run-server"><a class="header" href="#build-and-run-server">Build and run server</a></h2>
<p>Run server:</p>
<pre><code class="language-bash">zkstack server
</code></pre>
<p>The server’s configuration files can be found in <code>/chains/&lt;chain_name&gt;/configs</code> directory. These files are created when
running <code>zkstack chain init</code> command.</p>
<h3 id="modifying-configuration-files-manually"><a class="header" href="#modifying-configuration-files-manually">Modifying configuration files manually</a></h3>
<p>To manually modify configuration files:</p>
<ol>
<li>Locate the relevant config file in <code>/chains/&lt;chain_name&gt;/configs</code></li>
<li>Open the file in a text editor</li>
<li>Make necessary changes, following the existing format</li>
<li>Save the file</li>
<li>Restart the relevant services for changes to take effect:</li>
</ol>
<pre><code class="language-bash">zkstack server
</code></pre>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/launch.html#admonition-note"></a>
</div>
<div>
<p>Manual changes to configuration files may be overwritten if the ecosystem is reinitialized or the chain is
reinitialized.</p>
</div>
</div>
<div id="admonition-warning" class="admonition admonish-warning" role="note" aria-labelledby="admonition-warning-title">
<div class="admonition-title">
<div id="admonition-warning-title">
<p>Warning</p>
</div>
<a class="admonition-anchor-link" href="guides/launch.html#admonition-warning"></a>
</div>
<div>
<p>Some properties, such as ports, may require manual modification across different configuration files to
ensure consistency and avoid conflicts.</p>
</div>
</div>
<h2 id="running-server-using-google-cloud-storage-object-store-instead-of-default-in-memory-store"><a class="header" href="#running-server-using-google-cloud-storage-object-store-instead-of-default-in-memory-store">Running server using Google cloud storage object store instead of default In memory store</a></h2>
<p>Get the <code>service_account.json</code> file containing the GCP credentials from kubernetes secret for relevant
environment(stage2/ testnet2) add that file to the default location <code>~/gcloud/service_account.json</code> or update
<code>object_store.toml</code> with the file location</p>
<pre><code class="language-bash">zkstack prover init --bucket-base-url={url} --credentials-file={path/to/service_account.json}
</code></pre>
<h2 id="running-prover-server"><a class="header" href="#running-prover-server">Running prover server</a></h2>
<p>Running on a machine with GPU</p>
<pre><code class="language-bash">zkstack prover run --component=prover
</code></pre>
<div id="admonition-note-1" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/launch.html#admonition-note-1"></a>
</div>
<div>
<p>Running on machine without GPU is currently not supported by <code>zkstack</code>.</p>
</div>
</div>
<h2 id="running-the-verification-key-generator"><a class="header" href="#running-the-verification-key-generator">Running the verification key generator</a></h2>
<pre><code class="language-bash"># ensure that the setup_2^26.key in the current directory, the file can be download from  https://storage.googleapis.com/matterlabs-setup-keys-us/setup-keys/setup_2\^26.key

# To generate all verification keys
cargo run --release --bin zksync_verification_key_generator
</code></pre>
<h2 id="generating-binary-verification-keys-for-existing-json-verification-keys"><a class="header" href="#generating-binary-verification-keys-for-existing-json-verification-keys">Generating binary verification keys for existing json verification keys</a></h2>
<pre><code class="language-bash">cargo run --release --bin zksync_json_to_binary_vk_converter -- -o /path/to/output-binary-vk
</code></pre>
<h2 id="generating-commitment-for-existing-verification-keys"><a class="header" href="#generating-commitment-for-existing-verification-keys">Generating commitment for existing verification keys</a></h2>
<pre><code class="language-bash">cargo run --release --bin zksync_commitment_generator
</code></pre>
<h2 id="running-the-contract-verifier"><a class="header" href="#running-the-contract-verifier">Running the contract verifier</a></h2>
<pre><code class="language-bash">zkstack contract-verifier run
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="connection-refused"><a class="header" href="#connection-refused">Connection Refused</a></h3>
<h4 id="problem"><a class="header" href="#problem">Problem</a></h4>
<pre><code class="language-bash">error sending request for url (http://127.0.0.1:8545/): error trying to connect: tcp connect error: Connection refused (os error 61)
</code></pre>
<h4 id="description"><a class="header" href="#description">Description</a></h4>
<p>It appears that no containers are currently running, which is likely the reason you’re encountering this error.</p>
<h4 id="solution"><a class="header" href="#solution">Solution</a></h4>
<p>Ensure that the necessary containers have been started and are functioning correctly to resolve the issue.</p>
<pre><code class="language-bash">zkstack containers
</code></pre>
<h3 id="zkstack-dev-lint---check--t-contracts-fails"><a class="header" href="#zkstack-dev-lint---check--t-contracts-fails"><code>zkstack dev lint --check -t contracts</code> fails</a></h3>
<h4 id="problem-1"><a class="header" href="#problem-1">Problem</a></h4>
<pre><code>zkstack dev lint --check -t contracts                                                                                                                                                                                                                                 138ms  13:13:01

┌   ZK Stack CLI
│
●  Running linters for targets: [".contracts"]
│
◒  Running contracts linter..                                                                                                                                                                                                                                                                                    │
■  Command failed to run
│
■    Status:
│      exit status: 2
│    Stdout:
│      yarn run v1.22.22
│      $ yarn lint:md &amp;&amp; yarn lint:sol &amp;&amp; yarn lint:ts &amp;&amp; yarn prettier:check
│      $ markdownlint "**/*.md"
│      $ solhint "**/*.sol"
│      A new version of Solhint is available: 5.0.5
│      Please consider updating your Solhint package.
│
│      system-contracts/contracts/ContractDeployer.sol
│      171:27 warning Avoid to use tx.origin avoid-tx-origin
│
│      ✖ 1 problem (0 errors, 1 warning)
│
│      $ eslint .
│      info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
│      info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
│
│
│    Stderr:
│
│      Oops! Something went wrong! :(
│
│      ESLint: 8.57.0
│
│      EslintPluginImportResolveError: typescript with invalid interface loaded as resolver
│      Occurred while linting /home/evl/code/zksync-era/contracts/system-contracts/scripts/utils.ts:19
│      Rule: "import/default"
│      at requireResolver (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:204:17)
│      at fullResolve (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:141:22)
│      at Function.relative (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:158:10)
│      at remotePath (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:811:381)
│      at captureDependency (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:817:463)
│      at captureDependencyWithSpecifiers
│      (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:817:144)
│      at /home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:822:42
│      at Array.forEach (&lt;anonymous&gt;)
│      at ExportMap.parse (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:821:427)
│      at ExportMap.for (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:807:201)
│      error Command failed with exit code 2.
│      error Command failed with exit code 2.
│
│
■  Command failed to run: yarn --cwd contracts lint:check
│
│  Oops! Something went wrong! :(
│
│  ESLint: 8.57.0
│
│  EslintPluginImportResolveError: typescript with invalid interface loaded as resolver
│  Occurred while linting /home/evl/code/zksync-era/contracts/system-contracts/scripts/utils.ts:19
│  Rule: "import/default"
│      at requireResolver (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:204:17)
│      at fullResolve (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:141:22)
│      at Function.relative (/home/evl/code/zksync-era/node_modules/eslint-module-utils/resolve.js:158:10)
│      at remotePath (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:811:381)
│      at captureDependency (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:817:463)
│      at captureDependencyWithSpecifiers (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:817:144)
│      at /home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:822:42
│      at Array.forEach (&lt;anonymous&gt;)
│      at ExportMap.parse (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:821:427)
│      at ExportMap.for (/home/evl/code/zksync-era/contracts/node_modules/eslint-plugin-import/lib/ExportMap.js:807:201)
│  error Command failed with exit code 2.
│  error Command failed with exit code 2.
│
│
▲    0: Command failed to run: yarn --cwd contracts lint:check
│
└  Failed to run command
</code></pre>
<h4 id="description-1"><a class="header" href="#description-1">Description</a></h4>
<p><code>npm</code> setup is nested within our codebase. Contracts has it &amp; so does the core codebase. The 2 can be incompatible,
therefore running <code>yarn install</code> in core ends up with a set of dependencies, with <code>yarn install</code> in contracts having a
different set of dependencies.</p>
<h4 id="solution-1"><a class="header" href="#solution-1">Solution</a></h4>
<pre><code>cd contracts &amp;&amp; yarn install
</code></pre>
<div id="admonition-note-2" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-2-title">
<div class="admonition-title">
<div id="admonition-note-2-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/launch.html#admonition-note-2"></a>
</div>
<div>
<p>This may cause integration tests to fail in core. If so, run <code>yarn install</code> inside core repo.</p>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-v2-project-architecture"><a class="header" href="#zksync-v2-project-architecture">ZKsync v2 Project Architecture</a></h1>
<p>This document will help you answer the question: <em>where can I find the logic for x?</em> by giving a directory-tree style
structure of the physical architecture of the ZKsync Era project.</p>
<h2 id="high-level-overview"><a class="header" href="#high-level-overview">High-Level Overview</a></h2>
<p>The zksync-era repository has the following main units:</p>
<p><ins><strong>Smart Contracts:</strong></ins> All the smart contracts in charge of the protocols on the L1 &amp; L2. Some main contracts:</p>
<ul>
<li>L1 &amp; L2 bridge contracts.</li>
<li>The ZKsync rollup contract on Ethereum.</li>
<li>The L1 proof verifier contract.</li>
</ul>
<p><strong><ins>Core App:</strong></ins> The execution layer. A node running the ZKsync network in charge of the following components:</p>
<ul>
<li>Monitoring the L1 smart contract for deposits or priority operations.</li>
<li>Maintaining a mempool that receives transactions.</li>
<li>Picking up transactions from the mempool, executing them in a VM, and changing the state accordingly.</li>
<li>Generating ZKsync chain blocks.</li>
<li>Preparing circuits for executed blocks to be proved.</li>
<li>Submitting blocks and proofs to the L1 smart contract.</li>
<li>Exposing the Ethereum-compatible web3 API.</li>
</ul>
<p><strong><ins>Prover App:</strong></ins> The prover app takes blocks and metadata generated by the server and constructs a validity zk
proof for them.</p>
<p><strong><ins>Storage Layer:</strong></ins> The different components and subcomponents don’t communicate with each other directly via
APIs, rather via the single source of truth – the db storage layer.</p>
<h2 id="low-level-overview"><a class="header" href="#low-level-overview">Low-Level Overview</a></h2>
<p>This section provides a physical map of folders &amp; files in this repository. It doesn’t aim to be complete, it only shows
the most important parts.</p>
<ul>
<li>
<p><code>/contracts</code>: A submodule with L1, L2, and system contracts. See
<a href="https://github.com/matter-labs/era-contracts/">repository</a>.</p>
</li>
<li>
<p><code>/core</code></p>
<ul>
<li>
<p><code>/bin</code>: Executables for the microservices components comprising ZKsync Core Node.</p>
<ul>
<li><code>/zksync_server</code>: Main sequencer implementation.</li>
<li><code>/external_node</code>: A read replica that can sync from the main node.</li>
<li><code>/tee_prover</code>: Implementation of the TEE prover.</li>
</ul>
</li>
<li>
<p><code>/node</code>: Composable node parts.</p>
<ul>
<li><code>/node_framework</code>: Framework used to compose parts of the node.</li>
<li><code>/api_server</code>: Implementation of Web3 JSON RPC server.</li>
<li><code>/base_token_adjuster</code>: Adaptor to support custom (non-ETH) base tokens.</li>
<li><code>/block_reverter</code>: Component for reverting L2 blocks and L1 batches.</li>
<li><code>/commitment_generator</code>: Component for calculation of commitments required for ZKP generation.</li>
<li><code>/consensus</code>: p2p utilities.</li>
<li><code>/consistency_checker</code>: Security component for the external node.</li>
<li><code>/da_clients</code>: Clients for different data availability solutions.</li>
<li><code>/da_dispatcher</code>: Adaptor for alternative DA solutions.</li>
<li><code>/eth_sender</code>: Component responsible for submitting batches to L1 contract.</li>
<li><code>/eth_watch</code>: Component responsible for retrieving data from the L1 contract.</li>
<li><code>/fee_model</code>: Fee logic implementation.</li>
<li><code>/genesis</code>: Logic for performing chain genesis.</li>
<li><code>/metadata_calculator</code>: Component responsible for Merkle tree maintenance.</li>
<li><code>/node_storage_init</code>: Strategies for the node initialization.</li>
<li><code>/node_sync</code>: Node synchronization for the external node.</li>
<li><code>/proof_data_handler</code>: Gateway API for interaction with the prover subsystem.</li>
<li><code>/reorg_detector</code>: Component responsible for detecting reorgs on the external node.</li>
<li><code>/state_keeper</code>: Main part of the sequencer, responsible for forming blocks and L1 batches.</li>
<li><code>/vm_runner</code>: Set of components generating various data by re-running sealed L1 batches.</li>
</ul>
</li>
<li>
<p><code>/lib</code>: All the library crates used as dependencies of the binary crates above.</p>
<ul>
<li><code>/basic_types</code>: Crate with essential ZKsync primitive types.</li>
<li><code>/config</code>: All the configuration values used by the different ZKsync apps.</li>
<li><code>/contracts</code>: Contains definitions of commonly used smart contracts.</li>
<li><code>/crypto_primitives</code>: Cryptographical primitives used by the different ZKsync crates.</li>
<li><code>/dal</code>: Data availability layer
<ul>
<li><code>/migrations</code>: All the db migrations applied to create the storage layer.</li>
<li><code>/src</code>: Functionality to interact with the different db tables.</li>
</ul>
</li>
<li><code>/db_connection</code>: Generic DB interface.</li>
<li><code>/eth_client</code>: Module providing an interface to interact with an Ethereum node.</li>
<li><code>/eth_signer</code>: Module to sign messages and txs.</li>
<li><code>/mempool</code>: Implementation of the ZKsync transaction pool.</li>
<li><code>/merkle_tree</code>: Implementation of a sparse Merkle tree.</li>
<li><code>/mini_merkle_tree</code>: In-memory implementation of a sparse Merkle tree.</li>
<li><code>/multivm</code>: A wrapper over several versions of VM that have been used by the main node.</li>
<li><code>/object_store</code>: Abstraction for storing blobs outside the main data store.</li>
<li><code>/queued_job_processor</code>: An abstraction for async job processing</li>
<li><code>/state</code>: A state keeper responsible for handling transaction execution and creating miniblocks and L1 batches.</li>
<li><code>/storage</code>: An encapsulated database interface.</li>
<li><code>/test_account</code>: A representation of ZKsync account.</li>
<li><code>/types</code>: ZKsync network operations, transactions, and common types.</li>
<li><code>/utils</code>: Miscellaneous helpers for ZKsync crates.</li>
<li><code>/vlog</code>: ZKsync observability stack.</li>
<li><code>/vm_interface</code>: Generic interface for ZKsync virtual machine.</li>
<li><code>/web3_decl</code>: Declaration of the Web3 API.</li>
</ul>
</li>
<li>
<p><code>/tests</code>: Testing infrastructure for ZKsync network.</p>
<ul>
<li><code>/loadnext</code>: An app for load testing the ZKsync server.</li>
<li><code>/ts-integration</code>: Integration tests set implemented in TypeScript.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>/prover</code>: ZKsync prover orchestrator application.</p>
</li>
<li>
<p><code>/docker</code>: Project docker files.</p>
</li>
<li>
<p><code>/bin</code> &amp; <code>/infrastructure</code>: Infrastructure scripts that help to work with ZKsync applications.</p>
</li>
<li>
<p><code>/etc</code>: Configuration files.</p>
<ul>
<li><code>/env</code>:<code>.env</code> files that contain environment variables for different configurations of ZKsync Server / Prover.</li>
</ul>
</li>
<li>
<p><code>/keys</code>: Verification keys for <code>circuit</code> module.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="build-docker-images"><a class="header" href="#build-docker-images">Build docker images</a></h1>
<p>This document explains how to build Docker images from the source code, instead of using prebuilt ones we distribute</p>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<p>Install prerequisites: see</p>
<p><a href="guides/./setup-dev.html">Installing dependencies</a></p>
<h2 id="build-docker-files"><a class="header" href="#build-docker-files">Build docker files</a></h2>
<p>You may build all images with <a href="https://github.com/matter-labs/zksync-era/blob/main/docker/Makefile">Makefile</a>
located in <a href="https://github.com/matter-labs/zksync-era/blob/main/docker">docker</a> directory in this
repository.</p>
<blockquote>
<p>All commands should be run from the root directory of the repository</p>
</blockquote>
<pre><code class="language-shell">make -C ./docker build-all
</code></pre>
<p>You will get those images:</p>
<pre><code class="language-shell">contract-verifier:2.0
server-v2:2.0
prover:2.0
witness-generator:2.0
external-node:2.0
</code></pre>
<p>Alternatively, you may build only needed components - available targets are</p>
<pre><code class="language-shell">make -C ./docker build-contract-verifier
make -C ./docker build-server-v2
make -C ./docker build-circuit-prover-gpu
make -C ./docker build-witness-generator
make -C ./docker build-external-node
</code></pre>
<h2 id="building-updated-images"><a class="header" href="#building-updated-images">Building updated images</a></h2>
<p>Simply run</p>
<pre><code class="language-shell">make -C ./docker clean-all
make -C ./docker build-all
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repositories"><a class="header" href="#repositories">Repositories</a></h1>
<h2 id="zksync"><a class="header" href="#zksync">ZKsync</a></h2>
<h3 id="core-components"><a class="header" href="#core-components">Core components</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync-era">zksync-era</a></td><td>zk server logic, including the APIs and database accesses</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-wallet-vue">zksync-wallet-vue</a></td><td>Wallet frontend</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-contracts">era-contracts</a></td><td>L1 &amp; L2 contracts, that are used to manage bridges and communication between L1 &amp; L2. Privileged contracts that are running on L2 (like Bootloader or ContractDeployer)</td></tr>
</tbody></table>
</div>
<h3 id="compiler"><a class="header" href="#compiler">Compiler</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-compiler-tester">era-compiler-tester</a></td><td>Integration testing framework for running executable tests on zkEVM</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-tests">era-compiler-tests</a></td><td>Collection of executable tests for zkEVM</td></tr>
<tr><td><a href="https://github.com/matter-labs//era-compiler-llvm">era-compiler-llvm</a></td><td>zkEVM fork of the LLVM framework</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-solidity">era-compiler-solidity</a></td><td>Solidity Yul/EVMLA compiler front end</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-vyper">era-compiler-vyper</a></td><td>Vyper LLL compiler front end</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-llvm-context">era-compiler-llvm-context</a></td><td>LLVM IR generator logic shared by multiple front ends</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-common">era-compiler-common</a></td><td>Common compiler constants</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-compiler-llvm-builder">era-compiler-llvm-builder</a></td><td>Tool for building our fork of the LLVM framework</td></tr>
</tbody></table>
</div>
<h3 id="zkevm--crypto"><a class="header" href="#zkevm--crypto">zkEVM / crypto</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_opcode_defs">era-zkevm_opcode_defs</a></td><td>Opcode definitions for zkEVM - main dependency for many other repos</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zk_evm">era-zk_evm</a></td><td>EVM implementation in pure rust, without circuits</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-sync_vm">era-sync_vm</a></td><td>EVM implementation using circuits</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkEVM-assembly">era-zkEVM-assembly</a></td><td>Code for parsing zkEVM assembly</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_test_harness">era-zkevm_test_harness</a></td><td>Tests that compare the two implementation of the zkEVM - the non-circuit one (zk_evm) and the circuit one (sync_vm)</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_tester">era-zkevm_tester</a></td><td>Assembly runner for zkEVM testing</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-boojum">era-boojum</a></td><td>New proving system library - containing gadgets and gates</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-shivini">era-shivini</a></td><td>Cuda / GPU implementation for the new proving system</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-zkevm_circuits">era-zkevm_circuits</a></td><td>Circuits for the new proving system</td></tr>
<tr><td><a href="https://github.com/matter-labs/franklin-crypto">franklin-crypto</a></td><td>Gadget library for the Plonk / plookup</td></tr>
<tr><td><a href="https://github.com/matter-labs/rescue-poseidon">rescue-poseidon</a></td><td>Library with hash functions used by the crypto repositories</td></tr>
<tr><td><a href="https://github.com/matter-labs/snark-wrapper">snark-wrapper</a></td><td>Circuit to wrap the final FRI proof into snark for improved efficiency</td></tr>
</tbody></table>
</div>
<h4 id="old-proving-system"><a class="header" href="#old-proving-system">Old proving system</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-bellman-cuda">era-bellman-cuda</a></td><td>Cuda implementations for cryptographic functions used by the prover</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-heavy-ops-service">era-heavy-ops-service</a></td><td>Main circuit prover that requires GPU to run</td></tr>
<tr><td><a href="https://github.com/matter-labs/era-circuit_testing">era-circuit_testing</a></td><td>??</td></tr>
</tbody></table>
</div>
<h3 id="tools--contract-developers"><a class="header" href="#tools--contract-developers">Tools &amp; contract developers</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/era-test-node">era-test-node</a></td><td>In memory node for development and smart contract debugging</td></tr>
<tr><td><a href="https://github.com/matter-labs/local-setup">local-setup</a></td><td>Docker-based zk server (together with L1), that can be used for local testing</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-cli">zksync-cli</a></td><td>Command line tool to interact with ZKsync</td></tr>
<tr><td><a href="https://github.com/matter-labs/block-explorer">block-explorer</a></td><td>Online blockchain browser for viewing and analyzing ZKsync chain</td></tr>
<tr><td><a href="https://github.com/matter-labs/dapp-portal">dapp-portal</a></td><td>ZKsync Wallet + Bridge DApp</td></tr>
<tr><td><a href="https://github.com/matter-labs/hardhat-zksync">hardhat-zksync</a></td><td>ZKsync Hardhat plugins</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksolc-bin">zksolc-bin</a></td><td>solc compiler binaries</td></tr>
<tr><td><a href="https://github.com/matter-labs/zkvyper-bin">zkvyper-bin</a></td><td>vyper compiler binaries</td></tr>
</tbody></table>
</div>
<h3 id="examples--documentation"><a class="header" href="#examples--documentation">Examples &amp; documentation</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync-docs">zksync-web-era-docs</a></td><td><a href="https://docs.zksync.io">Public ZKsync documentation</a>, API descriptions etc.</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-contract-templates">zksync-contract-templates</a></td><td>Quick contract deployment and testing with tools like Hardhat on Solidity or Vyper</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-frontend-templates">zksync-frontend-templates</a></td><td>Rapid UI development with templates for Vue, React, Next.js, Nuxt, Vite, etc.</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-scripting-templates">zksync-scripting-templates</a></td><td>Automated interactions and advanced ZKsync operations using Node.js</td></tr>
<tr><td><a href="https://github.com/matter-labs/tutorials">tutorials</a></td><td>Tutorials for developing on ZKsync</td></tr>
</tbody></table>
</div>
<h2 id="zksync-lite"><a class="header" href="#zksync-lite">ZKsync Lite</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Public repository</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/matter-labs/zksync">zksync</a></td><td>ZKsync Lite implementation</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-lite-docs">ZKsync-lite-docs</a></td><td>Public ZKsync Lite documentation</td></tr>
<tr><td><a href="https://github.com/matter-labs/zksync-dapp-checkout">zksync-dapp-checkout</a></td><td>Batch payments DApp</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-advanced-guides"><a class="header" href="#zksync-advanced-guides">ZKsync advanced guides</a></h1>
<p>This section contains more advanced guides that aim to explain complex internals of ZKsync ecosystem in an easy to grasp
way.</p>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="guides/advanced/./01_initialization.html">Local initialization</a></li>
<li><a href="guides/advanced/./02_deposits.html">Deposits</a></li>
<li><a href="guides/advanced/./03_withdrawals.html">Withdrawals</a></li>
<li><a href="guides/advanced/./04_contracts.html">Contracts</a></li>
<li><a href="guides/advanced/./05_how_call_works.html">Calls</a></li>
<li><a href="guides/advanced/./06_how_transaction_works.html">Transactions</a></li>
<li><a href="guides/advanced/./07_fee_model.html">Fee model</a></li>
<li><a href="guides/advanced/./08_how_l2_messaging_works.html">L2 messaging</a></li>
<li><a href="guides/advanced/./09_pubdata.html">Pubdata</a></li>
<li><a href="guides/advanced/./10_pubdata_with_blobs.html">Pubdata with blobs</a></li>
<li><a href="guides/advanced/./11_compression.html">Bytecode compression</a></li>
<li><a href="guides/advanced/./12_alternative_vm_intro.html">EraVM intro</a></li>
<li><a href="guides/advanced/./13_zk_intuition.html">ZK intuition</a></li>
<li><a href="guides/advanced/./14_zk_deeper_overview.html">ZK deeper overview</a></li>
<li><a href="guides/advanced/./15_prover_keys.html">Prover keys</a></li>
<li><a href="guides/advanced/./16_decentralization.html">Decentralization</a></li>
<li><a href="guides/advanced/./17_batch_reverter.html">L1 Batch reversion</a></li>
</ul>
<p>Additionally, there are a few articles that cover specific topics that may be useful for developers actively working on
<code>zksync-era</code> repo:</p>
<ul>
<li><a href="guides/advanced/./90_advanced_debugging.html">Advanced debugging</a></li>
<li><a href="guides/advanced/./91_docker_and_ci.html">Docker and CI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-deeper-dive"><a class="header" href="#zksync-deeper-dive">ZKsync Deeper Dive</a></h1>
<p>The goal of this doc is to show you some more details on how ZKsync works internally.</p>
<p>Please do the dev_setup.md and development.md (these commands do all the heavy lifting on starting the components of the
system).</p>
<p>Now let’s take a look at what’s inside:</p>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<p>Let’s take a deeper look into what <code>zkstack ecosystem init</code> does.</p>
<h4 id="zk-stack-cli"><a class="header" href="#zk-stack-cli">ZK Stack CLI</a></h4>
<p><code>zkstack</code> itself is implemented in Rust (you can see the code in <code>/zkstack_cli</code> directory). If you change anything
there, make sure to run <code>zkstackup --local</code> from the root folder (that compiles and installs this code), before
re-running any <code>zkstack</code> command.</p>
<h4 id="containers"><a class="header" href="#containers">Containers</a></h4>
<p>The first step to initialize a ZK Stack ecosystem is to run the command <code>zkstack containers</code>. This command gets the
docker images for <code>postgres</code> and <code>reth</code>. If the <code>--observability</code> option is passed to the command, or the corresponding
option is selected in the interactive prompt, then Prometheus, Grafana and other observability-related images are
downloaded and run.</p>
<p>Reth (one of the Ethereum clients) will be used to setup our own copy of L1 chain (that our local ZKsync would use).</p>
<p>Postgres is one of the two databases, that is used by ZKsync (the other one is RocksDB). Currently most of the data is
stored in postgres (blocks, transactions etc) - while RocksDB is only storing the state (Tree &amp; Map) - and it used by
VM.</p>
<h4 id="ecosystem"><a class="header" href="#ecosystem">Ecosystem</a></h4>
<p>The next step is to run the command <code>zkstack ecosystem init</code>.</p>
<p>This command:</p>
<ul>
<li>Collects and finalize the ecosystem configuration.</li>
<li>Builds and deploys L1 &amp; L2 contracts.</li>
<li>Initializes each chain defined in the <code>/chains</code> folder. (Currently, a single chain <code>era</code> is defined there, but you can
create your own chains running <code>zkstack chain create</code>).</li>
<li>Sets up observability.</li>
<li>Runs the genesis process.</li>
<li>Initializes the database.</li>
</ul>
<h4 id="postgres"><a class="header" href="#postgres">Postgres</a></h4>
<p>First - postgres database: you’ll be able to see something like</p>
<pre><code>DATABASE_URL = postgres://postgres:notsecurepassword@localhost/zksync_local
</code></pre>
<p>After which we setup the schema (lots of lines with <code>Applied XX</code>).</p>
<p>You can try connecting to postgres now, to see what’s inside:</p>
<pre><code class="language-shell">psql postgres://postgres:notsecurepassword@localhost/zksync_local
</code></pre>
<p>(and then commands like <code>\dt</code> to see the tables, <code>\d TABLE_NAME</code> to see the schema, and <code>select * from XX</code> to see the
contents).</p>
<p>As our network has just started, the database would be quite empty.</p>
<p>You can see the schema for the database in
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/dal/README.md">dal/README.md</a> TODO: add the link to the
document with DB schema.</p>
<h4 id="docker-1"><a class="header" href="#docker-1">Docker</a></h4>
<p>We’re running two things in a docker:</p>
<ul>
<li>a postgres (that we’ve covered above)</li>
<li>a reth (that is the L1 Ethereum chain).</li>
</ul>
<p>Let’s see if they are running:</p>
<pre><code class="language-shell">docker container ls
</code></pre>
<p>and then we can look at the Reth logs:</p>
<pre><code class="language-shell">docker logs zksync-era-reth-1
</code></pre>
<p>Where <code>zksync-era-reth-1</code> is the container name, that we got from the first command.</p>
<p>If everything goes well, you should see that L1 blocks are being produced.</p>
<h4 id="server"><a class="header" href="#server">Server</a></h4>
<p>Now we can start the main server:</p>
<pre><code class="language-bash">zkstack server
</code></pre>
<p>This will actually run a cargo binary (<code>zksync_server</code>).</p>
<p>The server will wait for the new transactions to generate the blocks (these can either be sent via JSON RPC, but it also
listens on the logs from the L1 contract - as things like token bridging etc comes from there).</p>
<p>Currently we don’t send any transactions there (so the logs might be empty).</p>
<p>But you should see some initial blocks in postgres:</p>
<pre><code class="language-sql">select * from miniblocks;
</code></pre>
<h4 id="our-l1-reth"><a class="header" href="#our-l1-reth">Our L1 (reth)</a></h4>
<p>Let’s finish this article, by taking a look at our L1:</p>
<p>We will use the <code>web3</code> tool to communicate with the L1, have a look at <a href="guides/advanced/02_deposits.html">02_deposits.md</a> for installation
instructions. You can check that you’re a (localnet) crypto trillionaire, by running:</p>
<pre><code class="language-bash">./web3 --rpc-url http://localhost:8545 balance 0x36615Cf349d7F6344891B1e7CA7C72883F5dc049
</code></pre>
<p>This is one of the “rich wallets” we predefined for local L1.</p>
<p><strong>Note:</strong> This reth shell is running official Ethereum JSON RPC with Reth-specific extensions documented at
<a href="https://paradigmxyz.github.io/reth/jsonrpc/intro.html">reth docs</a></p>
<p>In order to communicate with L2 (our ZKsync) - we have to deploy multiple contracts onto L1 (our local reth created
Ethereum). You can look on the <code>deployL1.log</code> file - to see the list of contracts that were deployed and their accounts.</p>
<p>First thing in the file, is the deployer/governor wallet - this is the account that can change, freeze and unfreeze the
contracts (basically the owner). You can verify the token balance using the <code>getBalance</code> method above.</p>
<p>Then, there are a bunch of contracts (CRATE2_FACTOR, DIAMOND_PROXY, L1_ALLOW_LIST etc etc) - for each one, the file
contains the address.</p>
<p>You can quickly verify that they were really deployed, by calling:</p>
<pre><code class="language-bash">./web3 --rpc-url http://localhost:8545 address XXX
</code></pre>
<p>Where XXX is the address in the file.</p>
<p>The most important one of them is CONTRACTS_DIAMOND_PROXY_ADDR (which acts as ‘loadbalancer/router’ for others - and
this is the contract that our server is ‘listening’ on).</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Ok - so let’s sum up what we have:</p>
<ul>
<li>a postgres running in docker (main database)</li>
<li>a local instance of ethereum (reth running in docker)
<ul>
<li>which also has a bunch of ‘magic’ contracts deployed</li>
<li>and two accounts with lots of tokens</li>
</ul>
</li>
<li>and a server process</li>
</ul>
<p>In the <a href="guides/advanced/02_deposits.html">next article</a>, we’ll start playing with the system (bridging tokens etc).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zk-sync-deeper-dive---bridging--deposits"><a class="header" href="#zk-sync-deeper-dive---bridging--deposits">ZK-Sync deeper dive - bridging &amp; deposits</a></h1>
<p>In the <a href="guides/advanced/01_initialization.html">first article</a>, we’ve managed to setup our system on local machine and verify that it
works. Now let’s actually start using it.</p>
<h2 id="seeing-the-status-of-the-accounts"><a class="header" href="#seeing-the-status-of-the-accounts">Seeing the status of the accounts</a></h2>
<p>Let’s use a small command line tool (web3 - <a href="https://github.com/mm-zk/web3">https://github.com/mm-zk/web3</a>) to interact with our blockchains.</p>
<pre><code class="language-shell">git clone https://github.com/mm-zk/web3
make build
</code></pre>
<p>Then let’s create the keypair for our temporary account:</p>
<pre><code class="language-shell">./web3 account create
</code></pre>
<p>It will produce a public and private key (for example):</p>
<pre><code>Private key: 0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6
Public address: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p><strong>NOTE:</strong> Keep track of this key and address, as they will be constantly used throughout these articles</p>
<p>Now, let’s see how many tokens we have:</p>
<pre><code class="language-shell">// This checks the tokens on 'L1' (reth)
./web3 --rpc-url http://localhost:8545 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd

// This checks the tokens on 'L2' (ZKsync)
./web3 --rpc-url http://localhost:3050 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>Unsurprisingly we have 0 on both - let’s fix it by first transferring some tokens on L1:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 transfer --pk 0x7726827caac94a7f9e1b160f7ea819f172f7b6f9d2a97f992c38edeab82d4110 7.4 to 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>And now when we check the balance, we should see:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>that we have 7.4 ETH.</p>
<p>and now let’s bridge it over to L2.</p>
<h2 id="bridging-over-to-l2"><a class="header" href="#bridging-over-to-l2">Bridging over to L2</a></h2>
<p>For an easy way to bridge we’ll use <a href="https://github.com/matter-labs/zksync-cli">ZKsync CLI</a></p>
<pre><code class="language-shell">npx zksync-cli bridge deposit --chain=dockerized-node --amount 3 --pk=0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6 --to=0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
# Amount of ETH to deposit: 3
# Private key of the sender: 0x5090c024edb3bdf4ce2ebc2da96bedee925d9d77d729687e5e2d56382cf0a5a6
# Recipient address on L2: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<p>If everything goes well, you should be able to see 3 tokens transferred:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:3050 balance  0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
</code></pre>
<h3 id="diving-deeper---what-exactly-happened"><a class="header" href="#diving-deeper---what-exactly-happened">Diving deeper - what exactly happened</a></h3>
<p>Let’s take a deeper look at what the ‘deposit’ call actually did.</p>
<p>If we look at what ‘deposit’ command has printed, we’ll see something like this:</p>
<pre><code>Transaction submitted 💸💸💸
[...]/tx/0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
</code></pre>
<p>Let’s use the web3 tool and see the details:</p>
<pre><code class="language-shell">./web3 --rpc-url http://localhost:8545 tx --input hex 0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
</code></pre>
<p>returns</p>
<pre><code>Hash: 0xe27dc466c36ad2046766e191017e7acf29e84356465feef76e821708ff18e179
From: 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd
To: 0xa6Bcd8124d42293D3DDFAE6003940A62D8C280F2
Value: 3.000120034768750000 GO
Nonce: 0
Gas Limit: 134871
Gas Price: 1.500000001 gwei
Block Number: 100074
Block Hash: 0x5219e6fef442b4cfd38515ea7119dd6d2e12df82b4d95b1f75fd3650c012f133
Input: 0xeb672419000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd00000000000000000000000000000000000000000000000029a2241af62c000000000000000000000000000000000000000000000000000000000000000000e0000000000000000000000000000000000000000000000000000000000006d0b100000000000000000000000000000000000000000000000000000000000003200000000000000000000000000000000000000000000000000000000000000100000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
</code></pre>
<p>The deposit command has called the contract on address <code>0xa6B</code> (which is exactly the <code>CONTRACTS_DIAMOND_PROXY_ADDR</code> from
<code>deployL1.log</code>), and it has called the method <code>0xeb672419</code> - which is the <code>requestL2Transaction</code> from
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/facets/Mailbox.sol#L220">Mailbox.sol</a></p>
<h4 id="quick-note-on-our-l1-contracts"><a class="header" href="#quick-note-on-our-l1-contracts">Quick note on our L1 contracts</a></h4>
<p>We’re using the DiamondProxy setup, that allows us to have a fixed immutable entry point (DiamondProxy) - that forwards
the requests to different contracts (facets) that can be independently updated and/or frozen.</p>
<p><img src="https://user-images.githubusercontent.com/128217157/229521292-1532a59b-665c-4cc4-8342-d25ad45a8fcd.png" alt="Diamond proxy layout" /></p>
<p>You can find more detailed description in
<a href="https://github.com/matter-labs/era-contracts/blob/main/docs/Overview.md">Contract docs</a></p>
<h4 id="requestl2transaction-function-details"><a class="header" href="#requestl2transaction-function-details">requestL2Transaction Function details</a></h4>
<p>You can use some of the online tools (like <a href="https://calldata-decoder.apoorv.xyz/">https://calldata-decoder.apoorv.xyz/</a>) and pass the input data to it - and
get the nice result:</p>
<pre><code class="language-json">"function": "requestL2Transaction(address,uint256,bytes,uint256,uint256,bytes[],address)",
"params": [
    "0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd",
    "3000000000000000000",
    "0x",
    "641858",
    "800",
    [],
    "0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd"
  ]

</code></pre>
<p>This means that we requested that the 3 ETH (2nd argument) is transferred to 0x6182 (1st argument). The Calldata being
0x0 - means that we’re talking about ETH (this would be a different value for other ERC tokens). Then we also specify a
gas limit (641k) and set the gas per pubdata byte limit to 800. (TODO: explain what these values mean.)</p>
<h4 id="what-happens-under-the-hood"><a class="header" href="#what-happens-under-the-hood">What happens under the hood</a></h4>
<p>The call to requestL2Transaction, is adding the transaction to the priorityQueue and then emits the NewPriorityRequest.</p>
<p>The zk server (that you started with <code>zk server</code> command) is listening on events that are emitted from this contract
(via the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/eth_watch/"><code>eth_watcher</code> component</a>) and adds
them to the postgres database (into <code>transactions</code> table).</p>
<p>You can actually check it - by running the psql and looking at the contents of the table - then you’ll notice that
transaction was successfully inserted, and it was also marked as ‘priority’ (as it came from L1) - as regular
transactions that are received by the server directly are not marked as priority.</p>
<p>You can verify that this is your transaction, by looking at the <code>l1_block_number</code> column (it should match the
<code>block_number</code> from the <code>web3 tx</code> call above).</p>
<p>Notice that the hash of the transaction in the postgres will be different from the one returned by <code>web3 tx</code>. This is
because the postgres keeps the hash of the ‘L2’ transaction (which was ‘inside’ the L1 transaction that <code>web3 tx</code>
returned).</p>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>In this article, we’ve learned how ETH gets bridged from L1 to L2. In the <a href="guides/advanced/03_withdrawals.html">next article</a>, we’ll look
at the other direction - how we transmit messages (and ETH) from L2 to L1.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-deeper-dive-bridging-stuff-back-aka-withdrawals"><a class="header" href="#zksync-deeper-dive-bridging-stuff-back-aka-withdrawals">ZKsync deeper dive bridging stuff back (a.k.a withdrawals)</a></h1>
<p>Assuming that you have completed <a href="guides/advanced/01_initialization.html">part 1</a> and <a href="guides/advanced/02_deposits.html">part 2</a> already, we can bridge the
tokens back by simply calling the zksync-cli:</p>
<pre><code class="language-bash">npx zksync-cli bridge withdraw --chain=dockerized-node
</code></pre>
<p>And providing the account name (public address) and private key.</p>
<p>Afterward, by using <code>web3</code> tools, we can quickly check that funds were transferred back to L1. <strong>And you discover that
they didn’t</strong> - what happened?</p>
<p>Actually we’ll have to run one additional step:</p>
<pre><code class="language-bash">npx zksync-cli bridge withdraw-finalize --chain=dockerized-node
</code></pre>
<p>and pass the transaction that we received from the first call, into the <code>withdraw-finalize</code> call.</p>
<p><strong>Note:</strong> This is not needed on testnet - as we (MatterLabs) - are running an automatic tool that confirms withdrawals.</p>
<h3 id="looking-deeper"><a class="header" href="#looking-deeper">Looking deeper</a></h3>
<p>But let’s take a look what happened under the hood.</p>
<p>Let’s start by looking at the output of our <code>zksync-cli</code>:</p>
<pre><code>Withdrawing 7ETH to 0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd on localnet
Transaction submitted 💸💸💸
L2: tx/0xe2c8a7beaf8879cb197555592c6eb4b6e4c39a772c3b54d1b93da14e419f4683
Your funds will be available in L1 in a couple of minutes.
</code></pre>
<p><strong>important</strong> - your transaction id will be different - make sure that you use it in the methods below.</p>
<p>The tool created the withdraw transaction and it sent it directly to our server (so this is a L2 transaction). The zk
server has received it, and added it into its database. You can check it by querying the <code>transactions</code> table:</p>
<pre><code class="language-shell"># select * from transactions where hash = '\x&lt;YOUR_L2_TRANSACTION_ID_FROM_ABOVE&gt;`
select * from transactions where hash = '\xe2c8a7beaf8879cb197555592c6eb4b6e4c39a772c3b54d1b93da14e419f4683';
</code></pre>
<p>This will print a lot of columns, but let’s start by looking at the <code>data</code> column:</p>
<pre><code class="language-json">{
  "value": "0x6124fee993bc0000",
  "calldata": "0x51cff8d9000000000000000000000000618263ce921f7dd5f4f40c29f6c524aaf97b9bbd",
  "factoryDeps": null,
  "contractAddress": "0x000000000000000000000000000000000000800a"
}
</code></pre>
<p>We can use the ABI decoder tool <a href="https://calldata-decoder.apoorv.xyz/">https://calldata-decoder.apoorv.xyz/</a> to see what this call data means:</p>
<pre><code class="language-json">{
  "function": "withdraw(address)",
  "params": ["0x618263CE921F7dd5F4f40C29f6c524Aaf97b9bbd"]
}
</code></pre>
<p>(and the 0x6124fee993bc0000 in the value is 7000000000000000000 == 7 ETH that we wanted to send).</p>
<p>So the last question is – what is the ‘magic’ contract address: <code>0x800a</code> ?</p>
<pre><code class="language-solidity">/// @dev The address of the eth token system contract
address constant L2_BASE_TOKEN_SYSTEM_CONTRACT_ADDR = address(0x800a);

</code></pre>
<h3 id="system-contracts-on-l2"><a class="header" href="#system-contracts-on-l2">System contracts (on L2)</a></h3>
<p>This is a good opportunity to talk about system contracts that are automatically deployed on L2. You can find the full
list here
<a href="https://github.com/matter-labs/era-system-contracts/blob/436d57da2fb35c40e38bcb6637c3a090ddf60701/scripts/constants.ts#L29">in github</a></p>
<p>This is the place where we specify that <code>bootloader</code> is at address 0x8001, <code>NonceHolder</code> at 0x8003 etc.</p>
<p>This brings us to
<a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/L2EthToken.sol">L2BaseToken.sol</a> that has the
implementation of the L2 Eth.</p>
<p>When we look inside, we can see:</p>
<pre><code class="language-solidity">// Send the L2 log, a user could use it as proof of the withdrawal
bytes memory message = _getL1WithdrawMessage(_l1Receiver, amount);
L1_MESSENGER_CONTRACT.sendToL1(message);
</code></pre>
<p>And <code>L1MessengerContract</code> (that is deployed at 0x8008).</p>
<h3 id="committing-to-l1"><a class="header" href="#committing-to-l1">Committing to L1</a></h3>
<p>And how do these messages get into the L1? The <code>eth_sender</code> class from our server is taking care of this. You can see
the details of the transactions that it posts to L1 in our database in <code>eth_txs</code> table.</p>
<p>If you look at the <code>tx_type</code> column (in psql), you can see that we have 3 different transaction types:</p>
<pre><code class="language-sql">zksync_local=# select contract_address, tx_type from eth_txs;
              contract_address              |          tx_type
--------------------------------------------+---------------------------
 0x54e8159f006750466084913d5bd288d4afb1ee9a | CommitBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | PublishProofBlocksOnchain
 0x54e8159f006750466084913d5bd288d4afb1ee9a | ExecuteBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | CommitBlocks
 0x54e8159f006750466084913d5bd288d4afb1ee9a | PublishProofBlocksOnchain
 0x54e8159f006750466084913d5bd288d4afb1ee9a | ExecuteBlocks
</code></pre>
<p>BTW - all the transactions are sent to the 0x54e address - which is the <code>DiamondProxy</code> deployed on L1 (this address will
be different on your local node - see previous tutorial for more info) .</p>
<p>And inside, all three methods above belong to
<a href="https://github.com/matter-labs/era-contracts/blob/dev/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol">Executor.sol</a>
facet and you can look at
<a href="https://github.com/matter-labs/era-contracts/blob/main/docs/Overview.md#executorfacet">README</a> to see the details of
what each method does.</p>
<p>The short description is:</p>
<ul>
<li>‘CommitBlocks’ - is verifying the block metadata and stores the hash into the L1 contract storage.</li>
<li>‘PublishProof’ - gets the proof, checks that the proof is correct and that it is a proof for the block hash that was
stored in commit blocks. (IMPORTANT: in testnet/localnet we allow empty proofs - so that you don’t have to run the
full prover locally)</li>
<li>‘ExecuteBlocks’ - is the final call, that stores the root hashes in L1 storage. This allows other calls (like
finalizeWithdrawal) to work.</li>
</ul>
<p>So to sum it up - after these 3 calls, the L1 contract has a root hash of a merkle tree, that contains the ‘message’
about the withdrawal.</p>
<h3 id="final-step---finalizing-withdrawal"><a class="header" href="#final-step---finalizing-withdrawal">Final step - finalizing withdrawal</a></h3>
<p>Now we’re ready to actually claim our ETH on L1. We do this by calling a <code>finalizeEthWithdrawal</code> function on the
DiamondProxy contract (Mailbox.sol to be exact).</p>
<p>To prove that we actually can withdraw the money, we have to say in which L2 block the withdrawal happened, and provide
the merkle proof from our withdrawal log, to the root that is stored in the L1 contract.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-contracts"><a class="header" href="#zksync-contracts">ZKsync contracts</a></h1>
<p>Now that we know how to bridge tokens back and forth, let’s talk about running things on ZKsync.</p>
<p>We have a bunch of great tutorials (like this one <a href="https://docs.zksync.io/build/tooling/hardhat/getting-started">https://docs.zksync.io/build/tooling/hardhat/getting-started</a>) that
you can follow to get the exact code &amp; command line calls to create the contracts - so in this article, let’s focus on
how things differ between ZKsync and Ethereum.</p>
<p><strong>Note</strong> Before reading this article, I’d recommend doing the hardhat tutorial above.</p>
<h2 id="ethereum-flow"><a class="header" href="#ethereum-flow">Ethereum flow</a></h2>
<p>In case of Ethereum, you start by writing a contract code in solidity, then you compile it with <code>solc</code>, and you get the
EVM bytecode, deployment bytecode (which is a function that should return the bytecode itself) and ABI (interface).</p>
<p>Afterwards, you send the deployment bytecode to the 0x000 address on Ethereum, which does some magic (executes the
deployment bytecode, that should contain the constructor etc) and puts the contract under the address that is generated
based on your account id and a nonce.</p>
<p>From this moment on, you can send the transactions to this new address (and most of the tools would ask you to provide
the ABI, so that they can set the proper function arguments).</p>
<p>All the bytecode will be run on the EVM (Ethereum Virtual Machine) - that has a stack, access to memory and storage, and
a bunch of opcodes.</p>
<h2 id="zksync-flow"><a class="header" href="#zksync-flow">ZKsync flow</a></h2>
<p>The main part (and the main cost) of the ZKsync is the proving system. In order to make proof as fast as possible, we’re
running a little bit different virtual machine (zkEVM) - that has a slightly different set of opcodes, and also contains
a bunch of registers. More details on this will be written in the future articles.</p>
<p>Having a different VM means that we must have a separate compiler <a href="https://github.com/matter-labs/zksolc-bin">zk-solc</a> -
as the bytecode that is produced by this compiler has to use the zkEVM specific opcodes.</p>
<p>While having a separate compiler introduces a bunch of challenges (for example, we need a custom
<a href="https://github.com/matter-labs/hardhat-zksync">hardhat plugins</a> ), it brings a bunch of benefits too: for example it
allows us to move some of the VM logic (like new contract deployment) into System contracts - which allows faster &amp;
cheaper modifications and increased flexibility.</p>
<h3 id="zksync-system-contracts"><a class="header" href="#zksync-system-contracts">ZKsync system contracts</a></h3>
<p>Small note on system contracts: as mentioned above, we moved some of the VM logic into system contracts, which allows us
to keep VM simpler (and with this - keep the proving system simpler).</p>
<p>You can see the full list (and codes) of the system contracts here:
<a href="https://github.com/matter-labs/era-system-contracts">https://github.com/matter-labs/era-system-contracts</a>.</p>
<p>While some of them are not really visible to the contract developer (like the fact that we’re running a special
<code>Bootleader</code> to package a bunch of transactions together - more info in a future article) - some others are very
visible - like our <code>ContractDeployer</code></p>
<h3 id="contractdeployer"><a class="header" href="#contractdeployer">ContractDeployer</a></h3>
<p>Deploying a new contract differs on Ethereum and ZKsync.</p>
<p>While on Ethereum - you send the transaction to 0x00 address - on ZKsync you have to call the special <code>ContractDeployer</code>
system contract.</p>
<p>If you look on your hardhat example, you’ll notice that your <code>deploy.ts</code> is actually using a <code>Deployer</code> class from the
<code>hardhat-zksync-deploy</code> plugin.</p>
<p>Which inside uses the ZKsync’s web3.js, that calls the contract deployer
<a href="https://github.com/zksync-sdk/zksync2-js/blob/b1d11aa016d93ebba240cdeceb40e675fb948133/src/contract.ts#L76">here</a></p>
<pre><code class="language-typescript">override getDeployTransaction(..) {
    ...
    txRequest.to = CONTRACT_DEPLOYER_ADDRESS;
    ...
}
</code></pre>
<p>Also <code>ContractDeployer</code> adding a special prefix for all the new contract addresses. This means that contract addresses
WILL be different on <code>ZKsync</code> and Ethereum (and also leaves us the possibility of adding Ethereum addresses in the
future if needed).</p>
<p>You can look for <code>CREATE2_PREFIX</code> and <code>CREATE_PREFIX</code> in the code.</p>
<h3 id="gas-costs"><a class="header" href="#gas-costs">Gas costs</a></h3>
<p>Another part, where ZKsync differs from Ethereum is gas cost. The best example for this are storage slots.</p>
<p>If you have two transactions that are updating the same storage slot - and they are in the same ‘batch’ - only the first
one would be charged (as when we write the final storage to ethereum, we just write the final diff of what slots have
changed - so updating the same slot multiple times doesn’t increase the amount of data that we have to write to L1).</p>
<h3 id="account-abstraction-and-some-method-calls"><a class="header" href="#account-abstraction-and-some-method-calls">Account abstraction and some method calls</a></h3>
<p>As <code>ZKsync</code> has a built-in Account Abstraction (more on this in a separate article) - you shouldn’t depend on some of
the solidity functions (like <code>ecrecover</code> - that checks the keys, or <code>tx.origin</code>) - in all the cases, the compiler will
try to warn you.</p>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>In this article, we looked at how contract development &amp; deployment differs on Ethereum and ZKsync (looking at
differences in VMs, compilers and system contracts).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="life-of-a-call"><a class="header" href="#life-of-a-call">Life of a ‘call’</a></h1>
<p>This article will show you how the <code>call</code> method works in our backend. The <code>call</code> method is a ‘read-only’ operation,
which means it doesn’t change anything on the blockchain. This will give you a chance to understand the system,
including the bootloader and VM.</p>
<p>For this example, let’s assume that the contract is already deployed, and we will use the <code>call</code> method to interact with
it.</p>
<p>Since the ‘call’ method is only for reading data, all the calculations will happen in the <code>api_server</code>.</p>
<h3 id="calling-the-call-method"><a class="header" href="#calling-the-call-method">Calling the ‘call’ method</a></h3>
<p>If you need to make calls quickly, you can use the ‘cast’ binary from the
<a href="https://foundry-book.zksync.io/getting-started/installation">Foundry ZKsync</a> suite:</p>
<pre><code class="language-shell=">cast call 0x23DF7589897C2C9cBa1C3282be2ee6a938138f10 "myfunction()()" --rpc-url http://localhost:3050
</code></pre>
<p>The address of your contract is represented by 0x23D…</p>
<p>Alternatively, you can make an RPC call directly, but this can be complicated as you will have to create the correct
payload, which includes computing the ABI for the method, among other things.</p>
<p>An example of an RPC call would be:</p>
<pre><code class="language-shell=">curl --location 'http://localhost:3050' \
--header 'Content-Type: application/json' \
--data '{
    "jsonrpc": "2.0",
    "id": 2,
    "method": "eth_call",
    "params": [
        {
            "from": "0x0000000000000000000000000000000000000000",
            "data": "0x0dfe1681",
            "to": "0x2292539b1232A0022d1Fc86587600d86e26396D2"
        }

    ]
}'
</code></pre>
<p>As you can see, using the RPC call directly is much more complex. That’s why I recommend using the ‘cast’ tool instead.</p>
<h3 id="whats-happening-in-the-server"><a class="header" href="#whats-happening-in-the-server">What’s happening in the server</a></h3>
<p>Under the hood, the ‘cast’ tool calls the <code>eth_call</code> RPC method, which is part of the official Ethereum API set. You can
find the definition of these methods in the <a href="https://github.com/matter-labs/zksync-era/blob/edd48fc37bdd58f9f9d85e27d684c01ef2cac8ae/core/bin/zksync_core/src/api_server/web3/backend_jsonrpc/namespaces/eth.rs" title="namespaces RPC api">namespaces/eth.rs</a> file in our code.</p>
<p>Afterward, it goes to the implementation, which is also in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/api_server/src/web3/namespaces/eth.rs" title="namespaces RPC implementation">namespaces/eth.rs</a> file but in a
different parent directory.</p>
<p>The server then executes the function in a VM sandbox. Since this is a <code>call</code> function, the VM only runs this function
before shutting down. This is handled by the <code>execute_tx_eth_call</code> method, which fetches metadata like block number and
timestamp from the database, and the <code>execute_tx_in_sandbox</code> method, which takes care of the execution itself. Both of
these functions are in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/api_server/src/execution_sandbox/execute.rs" title="execution sandbox">api_server/execution_sandbox.rs</a> file.</p>
<p>Finally, the transaction is pushed into bootloader memory, and the VM executes it until it finishes.</p>
<h3 id="vm"><a class="header" href="#vm">VM</a></h3>
<p>Before we look at the bootloader, let’s briefly examine the VM itself.</p>
<p>The zkEVM is a state machine with a heap, stack, 16 registers, and state. It executes zkEVM assembly, which has many
opcodes similar to EVM, but operates on registers rather than a stack. We have two implementations of the VM: one is in
‘pure rust’ without circuits (in the zk_evm repository), and the other has circuits (in the sync_vm repository). In this
example, the api server uses the ‘zk_evm’ implementation without circuits.</p>
<p>Most of the code that the server uses to interact with the VM is in
<a href="https://github.com/matter-labs/zksync-era/blob/ccd13ce88ff52c3135d794c6f92bec3b16f2210f/core/lib/multivm/src/versions/vm_latest/implementation/execution.rs#L108" title="vm code">core/lib/multivm/src/versions/vm_latest/implementation/execution.rs</a>.</p>
<p>In this line, we’re calling self.state.cycle(), which executes a single VM instruction. You can see that we do a lot of
things around this, such as executing multiple tracers after each instruction. This allows us to debug and provide
additional feedback about the state of the VM.</p>
<h3 id="bootloader--transaction-execution"><a class="header" href="#bootloader--transaction-execution">Bootloader &amp; transaction execution</a></h3>
<p>The Bootloader is a large ‘quasi’ system contract, written in Yul and located in
<a href="https://github.com/matter-labs/era-system-contracts/blob/93a375ef6ccfe0181a248cb712c88a1babe1f119/bootloader/bootloader.yul" title="bootloader code">system_contracts/bootloader/bootloader.yul</a> .</p>
<p>It’s a ‘quasi’ contract because it isn’t actually deployed under any address. Instead, it’s loaded directly into the VM
by the binary in the constructor <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/multivm/src/versions/vm_m6/vm_with_bootloader.rs#L330" title="vm constructor">init_vm_inner</a>.</p>
<p>So why do we still need a bootloader if we have the call data, contract binary, and VM? There are two main reasons:</p>
<ul>
<li>It allows us to ‘glue’ transactions together into one large transaction, making proofs a lot cheaper.</li>
<li>It allows us to handle some system logic (checking gas, managing some L1-L2 data, etc.) in a provable way. From the
circuit/proving perspective, this behaves like contract code.</li>
<li>You’ll notice that the way we run the bootloader in the VM is by first ‘kicking it off’ and cycling step-by-step until
it’s ready to accept the first transaction. Then we ‘inject’ the transaction by putting it in the right place in VM
memory and start iterating the VM again. The bootloader sees the new transaction and simply executes its opcodes.</li>
</ul>
<p>This allows us to ‘insert’ transactions one by one and easily revert the VM state if something goes wrong. Otherwise,
we’d have to start with a fresh VM and re-run all the transactions again.</p>
<h3 id="final-steps"><a class="header" href="#final-steps">Final steps</a></h3>
<p>Since our request was just a ‘call’, after running the VM to the end, we can collect the result and return it to the
caller. Since this isn’t a real transaction, we don’t have to do any proofs, witnesses, or publishing to L1.</p>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>In this article, we covered the ‘life of a call’ from the RPC to the inner workings of the system, and finally to the
‘out-of-circuit’ VM with the bootloader.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="life-of-transaction"><a class="header" href="#life-of-transaction">Life of transaction</a></h1>
<p>In this article, we will explore the lifecycle of a transaction, which is an operation that is stored permanently in the
blockchain and results in a change of its overall state.</p>
<p>To better understand the content discussed here, it is recommended that you first read the
<a href="guides/advanced/./05_how_call_works.html">life of a call</a>.</p>
<h2 id="l1-vs-l2-transactions"><a class="header" href="#l1-vs-l2-transactions">L1 vs L2 transactions</a></h2>
<p>There are two main methods through which transactions can enter the system. The most common approach involves making a
call to the RPC (Remote Procedure Call), where you send what is known as an <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/l2/mod.rs#L140" title="l2 tx"><code>L2Tx</code></a> transaction.</p>
<p>The second method involves interacting with Ethereum directly by sending a ‘wrapped’ transaction to our Ethereum
contract. These transactions are referred to as <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/l1/mod.rs#L183" title="l1 tx"><code>L1Tx</code></a> or Priority transactions, and the process of sending
transactions in this manner is called the ‘priority queue’.</p>
<h3 id="transaction-types"><a class="header" href="#transaction-types">Transaction types</a></h3>
<p>We provide support for five different types of transactions.</p>
<p>Here’s a simplified table of the transaction types:</p>
<div class="table-wrapper"><table><thead><tr><th>Type id</th><th>Transaction type</th><th>Features</th><th>Use cases</th><th>% of transactions (mainnet/testnet)</th></tr></thead><tbody>
<tr><td>0x0</td><td>‘Legacy’</td><td>Only includes <code>gas price</code></td><td>These are traditional Ethereum transactions.</td><td>60% / 82%</td></tr>
<tr><td>0x1</td><td>EIP-2930</td><td>Contains a list of storage keys/addresses the transaction will access</td><td>At present, this type of transaction is not enabled.</td><td></td></tr>
<tr><td>0x2</td><td>EIP-1559</td><td>Includes <code>max_priority_fee_per_gas</code>, <code>max_gas_price</code></td><td>These are Ethereum transactions that provide more control over the gas fee.</td><td>35% / 12%</td></tr>
<tr><td>0x71</td><td>EIP-712 (specific to ZKsync)</td><td>Similar to EIP-1559, but also adds <code>max_gas_per_pubdata</code>, custom signatures, and Paymaster support</td><td>This is used by those who are using ZKsync specific Software Development Kits (SDKs).</td><td>1% / 2%</td></tr>
<tr><td>0xFF</td><td>L1 transactions also known as priority transactions <code>L1Tx</code></td><td>Originating from L1, these have more custom fields like ‘refund’ addresses etc</td><td>Mainly used to transfer funds/data between L1 &amp; L2 layer.</td><td>4% / 3%</td></tr>
</tbody></table>
</div>
<p>Here’s the code that does the parsing: <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/types/src/transaction_request.rs#L196" title="transaction request from bytes">TransactionRequest::from_bytes</a></p>
<h2 id="transactions-lifecycle"><a class="header" href="#transactions-lifecycle">Transactions lifecycle</a></h2>
<h3 id="priority-queue-l1-tx-only"><a class="header" href="#priority-queue-l1-tx-only">Priority queue (L1 Tx only)</a></h3>
<p>L1 transactions are first ‘packaged’ and then sent to our Ethereum contract. After this, the L1 contract records this
transaction in L1 logs. <a href="https://github.com/matter-labs/zksync-era/blob/main/core/node/eth_watch" title="Ethereum watcher component">The <code>eth_watcher</code> component</a> constantly monitors these logs and then adds them to
the database (mempool).</p>
<h3 id="rpc--validation-l2-tx-only"><a class="header" href="#rpc--validation-l2-tx-only">RPC &amp; validation (L2 Tx only)</a></h3>
<p>Transactions are received via the <code>eth_sendRawTransaction</code> method. These are then parsed and validated using the
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/api_server/tx_sender/mod.rs#L288" title="submit tx"><code>submit_tx</code></a> method on the API server.</p>
<p>The validations ensure that the correct amount of gas has been assigned by the user and that the user’s account has
sufficient gas, among other things.</p>
<p>As part of this validation, we also perform a <code>validation_check</code> to ensure that if account abstraction / paymaster is
used, they are prepared to cover the fees. Additionally, we perform a ‘dry_run’ of the transaction for a better
developer experience, providing almost immediate feedback if the transaction fails.</p>
<p>Please note, that transaction can still fail in the later phases, even if it succeeded in the API, as it is going to be
executed in the context of a different block.</p>
<p>Once validated, the transaction is added to the mempool for later execution. Currently, the mempool is stored in the
<code>transactions</code> table in postgres (see the <code>insert_transaction_l2()</code> method).</p>
<h3 id="batch-executor--state-keeper"><a class="header" href="#batch-executor--state-keeper">Batch executor &amp; State keeper</a></h3>
<p>The State Keeper’s job is to take transactions from the mempool and place them into an L1 batch. This is done using the
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/keeper.rs#L257" title="process l1 batch"><code>process_l1_batch()</code></a> method.</p>
<p>This method takes the next transaction from the mempool (which could be either an L1Tx or L2Tx - but L1Tx are always
given the priority and they are taken first), executes it, and checks if the L1 batch is ready to be sealed (for more
details on when we finalize L1 batches, see the ‘Blocks &amp; Batches’ article).</p>
<p>Once the batch is sealed, it’s ready to be sent for proof generation and have this proof committed into L1. More details
on this will be covered in a separate article.</p>
<p>The transaction can have three different results in state keeper:</p>
<ul>
<li>Success</li>
<li>Failure (but still included in the block, and gas was charged)</li>
<li>Rejection - when it fails validation, and cannot be included in the block. This last case should (in theory) never
happen - as we cannot charge the fee in such scenario, and it opens the possibility for the DDoS attack.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fees-aka-gas"><a class="header" href="#fees-aka-gas">Fees (a.k.a gas)</a></h1>
<p>What is the L2 gas price? It’s <strong>0.1 Gwei</strong> (and as we improve our provers/VM we hope it will go down). However, it can
vary at times. Please see further information below.</p>
<h2 id="what-do-you-pay-for"><a class="header" href="#what-do-you-pay-for">What do you pay for</a></h2>
<p>The gas fee covers the following expenses:</p>
<ul>
<li>Calculation and storage (related to most operations)</li>
<li>Publishing data to L1 (a significant cost for many transactions, with the exact amount depending on L1)</li>
<li>Sending ‘bytecode’ to L1 (if not already there) - typically a one-time cost when deploying a new contract</li>
<li>Closing the batch and handling proofs - This aspect also relies on L1 costs (since proof publication must be covered).</li>
</ul>
<h2 id="price-configuration"><a class="header" href="#price-configuration">Price configuration</a></h2>
<p>We have two pricing models (old and new):</p>
<ul>
<li>the <code>L1Pegged</code> - until protocol version 19</li>
<li>the <code>PubdataIndependent</code> - from protocol version 20 (release 1.4.1)</li>
</ul>
<h3 id="l1-pegged-old-fee-model"><a class="header" href="#l1-pegged-old-fee-model">L1 pegged (‘old’ fee model)</a></h3>
<p>Under this fee model, operator was providing <code>FeeParamsV1</code>, which contained:</p>
<ul>
<li>l1_gas_price</li>
<li>minimal_l2_gas_price</li>
</ul>
<p>then, the system was computing <code>L1PeggedBatchFeeModelInput</code>, that contained</p>
<ul>
<li>l1_gas_price</li>
<li>‘fair’ l2 gas price - which in 99% of cases was equal to minimal_gas_price, and was greater from it only if l1 gas
price was huge, to guarantee that we can publish enough data in each transaction (see table below for details).</li>
</ul>
<p>Many other values, were ‘hardcoded’ within the system (for example how to compute the pubdata price based on l1 gas
price, how much committing the proof to L1 costs etc).</p>
<h3 id="pubdataindependent-new-fee-model"><a class="header" href="#pubdataindependent-new-fee-model">PubdataIndependent (‘new’ fee model)</a></h3>
<p>This method is called <code>PubdataIndependent</code> and the change was done to allow more flexibility in pubdata costs (for
example if pubdata is published to another Data Availability layer, or if not published at all - in case of validium).</p>
<p>In this model, there are 8 config options, let’s walk through them:</p>
<p><code>FeeParamsV2</code> contains 2 dynamic prices:</p>
<ul>
<li><code>l1_gas_price</code> - which is used to compute the cost of submitting the proofs on L1</li>
<li><code>l1_pubdata_price</code> - which is the cost of submitting a single byte of pubdata</li>
</ul>
<p>And config options (<code>FeeModelConfigV2</code>) contain:</p>
<ul>
<li><code>minimal_l2_gas_price</code> - similar meaning to the one in the previous model - this should cover the $ costs of running
the machines (node operator costs).</li>
</ul>
<p>2 fields around the maximum capacity of the batch (note - that these are used only for the fee calculation, the actual
sealing criteria are specified in different configuration):</p>
<ul>
<li><code>max_gas_per_batch</code> - expected maximum amount of gas in a batch</li>
<li><code>max_pubdata_per_batch</code> - expected maximum amount of pubdata that we can put in a single batch - due to Ethereum
limitations (usually it is around 128kb when using calldata, and 250 when using 2 blobs)</li>
</ul>
<p>the actual cost of the batch:</p>
<ul>
<li><code>batch_overhead_l1_gas</code> - how much gas operator will have to pay to handle the proof on L1 (this should include
commitBatch, proveBatch and executeBatch costs). This should NOT include any cost that is related to pubdata.</li>
</ul>
<p>And 2 fields about who contributed to closing the batch:</p>
<ul>
<li><code>pubdata_overhead_part</code> - from 0 - 1</li>
<li><code>compute_overhead_part</code> - from 0 - 1</li>
</ul>
<h4 id="cost-distribution-between-transactions"><a class="header" href="#cost-distribution-between-transactions">Cost distribution between transactions</a></h4>
<p>Batches are closed, when we either run out of circuits (a.k.a gas / computation), or run out of pubdata capacity (too
much data to publish to L1 in one transaction), or run out of transactions slots (which should be a rare event with
recent improvements)</p>
<p>Closing each batch, has some cost for the operator - especially the one related to L1 costs - where operator has to
submit a bunch of transactions, including the one to verify the proof (these costs are counted in
<code>batch_overhead_l1_gas</code>).</p>
<p>Now the problem that operator is facing is, who should pay for closing of the batch. In a perfect world, we’d look at
the reason for batch closure (for example pubdata) - and then charge the transactions proportionally to the amount of
pubdata that they used. Unfortunately this is not feasible, as we have to charge the transactions as we go, rather than
at the end of the batch (that can have 1000s of transactions).</p>
<p>That’s why we have the logic of <code>pubdata_overhead_part</code> and <code>compute_overhead_part</code>. These represent the ‘odds’ whether
pubdata or compute were the reason for the batch closure - and based on this information, we distribute the costs to
transactions:</p>
<pre><code>cost_of_closing_the_batch = (compute_overhead_part * TX_GAS_USED / MAX_GAS_IN_BLOCK + pubdata_overhead_part * PUBDATA_USED / MAX_PUBDATA_IN_BLOCK)
</code></pre>
<h4 id="custom-base-token-configurations"><a class="header" href="#custom-base-token-configurations">Custom base token configurations</a></h4>
<p>When running a system based on a custom token, all the gas values above, should refer to YOUR custom token.</p>
<p>Example, if you’re running USDC as base token, and ETH currently costs 2000$, and current L1 gas price is 30 Gwei.</p>
<ul>
<li><code>l1_gas_price</code> param should be set to 30 * 2000 == 60’000 Gwei</li>
<li><code>l1_pubdata_price</code> should be also updated accordingly (also multiplied by 2’000). (note: currently bootloader has a
cap of 1M gwei per pubdata price - and we’re working on removing this limitation)</li>
<li><code>minimal_l2_gas_price</code> should be set in such way, that <code>minimal_l2_gas_price * max_gas_per_batch / 10**18 $</code> is enough
to pay for your CPUs and GPUs</li>
</ul>
<h4 id="validium--data-availability-configurations"><a class="header" href="#validium--data-availability-configurations">Validium / Data-availability configurations</a></h4>
<p>If you’re running a system with validium without any DA, you can just set the <code>l1_pubdata_price</code> to 0,
<code>max_pubdata_per_batch</code> to some large value, and set <code>pubdata_overhead_part</code> to 0, and <code>compute_overhead_part</code> to 1.</p>
<p>If you’re running alternative DA, you should adjust the <code>l1_pubdata_price</code> to roughly cover the cost of writing one byte
to the DA, and set <code>max_pubdata_per_batch</code> to the DA limits.</p>
<p>Note: currently system still requires operator to keep the data in memory and compress it, which means that setting huge
values of <code>max_pubdata_per_batch</code> might not work. This will be fixed in the future.</p>
<p>Assumption: ETH costs 2’000$, L1 gas cost is 30 Gwei, blob costs 2Gwei (per byte), and DA allows 1MB payload that cost 1
cent.</p>
<div class="table-wrapper"><table><thead><tr><th>flag</th><th>rollup with calldata</th><th>rollup with 4844 (blobs)</th><th>value for validium</th><th>value for DA</th></tr></thead><tbody>
<tr><td><code>l1_pubdata_price</code></td><td>510’000’000’000</td><td>2’000’000’000</td><td>0</td><td>5’000</td></tr>
<tr><td><code>max_pubdata_per_batch</code></td><td>120’000</td><td>250’000</td><td>1’000’000’000’000</td><td>1’000’000</td></tr>
<tr><td><code>pubdata_overhead_part</code></td><td>0.7</td><td>0.4</td><td>0</td><td>0.1</td></tr>
<tr><td><code>compute_overhead_part</code></td><td>0.5</td><td>0.7</td><td>1</td><td>1</td></tr>
<tr><td><code>batch_overhead_l1_gas</code></td><td>1’000’000</td><td>1’000’000</td><td>1’000’000</td><td>1’400’000</td></tr>
</tbody></table>
</div>
<p>The cost of l1 batch overhead is higher for DA, as it has to cover the additional costs of checking on L1 if DA actually
got the data.</p>
<h2 id="l1-vs-l2-pricing"><a class="header" href="#l1-vs-l2-pricing">L1 vs L2 pricing</a></h2>
<p>Here is a simplified table displaying various scenarios that illustrate the relationship between L1 and L2 fees:</p>
<div class="table-wrapper"><table><thead><tr><th>L1 gas price</th><th>L2 ‘minimal price’</th><th>L2 ‘gas price’</th><th>L2 gas per pubdata</th><th>Note</th></tr></thead><tbody>
<tr><td>0.25 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>17</td><td>Gas prices are equal, so the charge is 17 gas, just like on L1.</td></tr>
<tr><td>10 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>680</td><td>L1 is 40 times more expensive, so we need to charge more L2 gas per pubdata byte to cover L1 publishing costs.</td></tr>
<tr><td>250 Gwei</td><td>0.25 Gwei</td><td>0.25 Gwei</td><td>17000</td><td>L1 is now very expensive (1000 times more than L2), so each pubdata costs a lot of gas.</td></tr>
<tr><td>10000 Gwei</td><td>0.25 Gwei</td><td>8.5 Gwei</td><td>20000</td><td>L1 is so expensive that we have to raise the L2 gas price, so the gas needed for publishing doesn’t exceed the 20k limit, ensuring L2 remains usable.</td></tr>
</tbody></table>
</div>
<p><strong>Why is there a 20k gas per pubdata limit?</strong> - We want to make sure every transaction can publish at least 4kb of data
to L1. The maximum gas for a transaction is 80 million (80M/4k = 20k).</p>
<h3 id="l2-fair-price"><a class="header" href="#l2-fair-price">L2 Fair price</a></h3>
<p>The L2 fair gas price is currently determined by the StateKeeper/Sequencer configuration and is set at 0.10 Gwei (see
<code>fair_l2_gas_price</code> in the config). This price is meant to cover the compute costs (CPU + GPU) for the sequencer and
prover. It can be changed as needed, with a safety limit of 10k Gwei in the bootloader. Once the system is
decentralized, more deterministic rules will be established for this price.</p>
<h3 id="l1-gas-price"><a class="header" href="#l1-gas-price">L1 Gas price</a></h3>
<p>The L1 gas price is fetched by querying L1 every 20 seconds. This is managed by the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/l1_gas_price/gas_adjuster/mod.rs#L30" title="gas_adjuster"><code>GasAdjuster</code></a>, which
calculates the median price from recent blocks and enables more precise price control via the config (for example,
adjusting the price with <code>internal_l1_pricing_multiplier</code> or setting a specific value using
<code>internal_enforced_l1_gas_price</code>).</p>
<h3 id="overhead-gas"><a class="header" href="#overhead-gas">Overhead gas</a></h3>
<p>As mentioned earlier, fees must also cover the overhead of generating proofs and submitting them to L1. While the
detailed calculation is complex, the short version is that a full proof of an L1 batch costs around <strong>1 million L2 gas,
plus 1M L1 gas (roughly equivalent of 60k published bytes)</strong>. In every transaction, you pay a portion of this fee
proportional to the part of the batch you are using.</p>
<h2 id="transactions"><a class="header" href="#transactions">Transactions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Transaction Field</th><th>Conditions</th><th>Note</th></tr></thead><tbody>
<tr><td>gas_limit</td><td><code>&lt;= max_allowed_l2_tx_gas_limit</code></td><td>The limit (4G gas) is set in the <code>StateKeeper</code> config; it’s the limit for the entire L1 batch.</td></tr>
<tr><td>gas_limit</td><td><code>&lt;= MAX_GAS_PER_TRANSACTION</code></td><td>This limit (80M) is set in bootloader.</td></tr>
<tr><td>gas_limit</td><td><code>&gt; l2_tx_intrinsic_gas</code></td><td>This limit (around 14k gas) is hardcoded to ensure that the transaction has enough gas to start.</td></tr>
<tr><td>max_fee_per_gas</td><td><code>&gt;= fair_l2_gas_price</code></td><td>Fair L2 gas price (0.1 Gwei on Era) is set in the <code>StateKeeper</code> config</td></tr>
<tr><td></td><td><code>&lt;=validation_computational_gas_limit</code></td><td>There is an additional, stricter limit (300k gas) on the amount of gas that a transaction can use during validation.</td></tr>
</tbody></table>
</div>
<h3 id="why-do-we-have-two-limits-80m-and-4g"><a class="header" href="#why-do-we-have-two-limits-80m-and-4g">Why do we have two limits: 80M and 4G</a></h3>
<p>The operator can set a custom transaction limit in the bootloader. However, this limit must be within a specific range,
meaning it cannot be less than 80M or more than 4G.</p>
<h3 id="why-validation-is-special"><a class="header" href="#why-validation-is-special">Why validation is special</a></h3>
<p>In Ethereum, there is a fixed cost for verifying a transaction’s correctness by checking its signature. However, in
ZKsync, due to Account Abstraction, we may need to execute some contract code to determine whether it’s ready to accept
the transaction. If the contract rejects the transaction, it must be dropped, and there’s no one to charge for that
process.</p>
<p>Therefore, a stricter limit on validation is necessary. This prevents potential DDoS attacks on the servers, where
people could send invalid transactions to contracts that require expensive and time-consuming verifications. By imposing
a stricter limit, the system maintains stability and security.</p>
<h2 id="actual-gas-calculation"><a class="header" href="#actual-gas-calculation">Actual gas calculation</a></h2>
<p>From the Virtual Machine (VM) point of view, there is only a bootloader. When executing transactions, we insert the
transaction into the bootloader memory and let it run until it reaches the end of the instructions related to that
transaction (for more details, refer to the ‘Life of a Call’ article).</p>
<p>To calculate the gas used by a transaction, we record the amount of gas used by the VM before the transaction execution
and subtract it from the remaining gas after the execution. This difference gives us the actual gas used by the
transaction.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let gas_remaining_before = vm.gas_remaining();
execute_tx();
let gas_used = gas_remaining_before - vm.gas_remaining();
<span class="boring">}</span></code></pre></pre>
<h2 id="gas-estimation"><a class="header" href="#gas-estimation">Gas estimation</a></h2>
<p>Before sending a transaction to the system, most users will attempt to estimate the cost of the request using the
<code>eth_estimateGas</code> call.</p>
<p>To estimate the gas limit for a transaction, we perform a binary search (between 0 and the <code>MAX_L2_TX_GAS_LIMIT</code> of 80M)
to find the smallest amount of gas under which the transaction still succeeds.</p>
<p>For added safety, we include some ‘padding’ by using two additional config options: <code>gas_price_scale_factor</code> (currently
1.5) and <code>estimate_gas_scale_factor</code> (currently 1.3). These options are used to increase the final estimation.</p>
<p>The first option simulates the volatility of L1 gas (as mentioned earlier, high L1 gas can affect the actual gas cost of
data publishing), and the second one serves as a ‘safety margin’.</p>
<p>You can find this code in <a href="https://github.com/matter-labs/zksync-era/blob/714a8905d407de36a906a4b6d464ec2cab6eb3e8/core/lib/zksync_core/src/api_server/tx_sender/mod.rs#L656" title="get_txs_fee_in_wei">get_txs_fee_in_wei</a> function.</p>
<h2 id="qa"><a class="header" href="#qa">Q&amp;A</a></h2>
<h3 id="is-zksync-really-cheaper"><a class="header" href="#is-zksync-really-cheaper">Is ZKsync really cheaper</a></h3>
<p>In short, yes. As seen in the table at the beginning, the regular L2 gas price is set to 0.25 Gwei, while the standard
Ethereum price is around 60-100 Gwei. However, the cost of publishing to L1 depends on L1 prices, meaning that the
actual transaction costs will increase if the L1 gas price rises.</p>
<h3 id="why-do-i-hear-about-large-refunds"><a class="header" href="#why-do-i-hear-about-large-refunds">Why do I hear about large refunds</a></h3>
<p>There are a few reasons why refunds might be ‘larger’ on ZKsync (i.e., why we might be overestimating the fees):</p>
<ul>
<li>We must assume (pessimistically) that you’ll have to pay for all the slot/storage writes. In practice, if multiple
transactions touch the same slot, we only charge one of them.</li>
<li>We have to account for larger fluctuations in the L1 gas price (using gas_price_scale_factor mentioned earlier) - this
might cause the estimation to be significantly higher, especially when the L1 gas price is already high, as it then
impacts the amount of gas used by pubdata.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-l2-to-l1-messaging-works"><a class="header" href="#how-l2-to-l1-messaging-works">How L2 to L1 messaging works</a></h1>
<p>In this article, we will explore the workings of Layer 2 (L2) to Layer 1 (L1) messaging in ZKsync Era.</p>
<p>If you’re uncertain about why messaging is necessary in the first place, please refer to our <a href="https://code.zksync.io/tutorials/how-to-send-l2-l1-message">user
documentation</a>.</p>
<p>For ease of understanding, here’s a quick visual guide. We will unpack each part in detail as we progress.</p>
<p><img src="https://user-images.githubusercontent.com/128217157/257739371-f971c10b-87c7-4ee9-bd0e-731670c616ac.png" alt="overview image" /></p>
<h2 id="part-1---user-generates-a-message"><a class="header" href="#part-1---user-generates-a-message">Part 1 - User Generates a Message</a></h2>
<p>Consider the following contract. Its main function is to forward any received string to L1:</p>
<pre><code class="language-solidity">contract Messenger {
  function sendMessage(string memory message) public returns (bytes32 messageHash) {
    messageHash = L1_MESSENGER_CONTRACT.sendToL1(bytes(message));
  }
}

</code></pre>
<p>From a developer’s standpoint, you only need to invoke the <code>sendToL1</code> method, and your task is complete.</p>
<p>It’s worth noting, however, that transferring data to L1 typically incurs high costs. These costs are associated with
the ‘pubdata cost’ that is charged for each byte in the message. As a workaround, many individuals choose to send the
hash of the message instead of the full message, as this helps to conserve resources.</p>
<h2 id="part-2---system-contract-execution"><a class="header" href="#part-2---system-contract-execution">Part 2 - System Contract Execution</a></h2>
<p>The previously mentioned <code>sendToL1</code> method executes a call to the <code>L1Messenger.sol</code> system contract
<a href="https://github.com/matter-labs/era-system-contracts/blob/f01df555c03860b6093dd669d119eed4d9f8ec99/contracts/L1Messenger.sol#L22">here</a>. This system contract performs tasks such as computing the appropriate gas cost and hashes, and
then it broadcasts an Event carrying the complete message.</p>
<pre><code class="language-solidity">function sendToL1(bytes calldata _message) external override returns (bytes32 hash) {
  // ...
  SystemContractHelper.toL1(true, bytes32(uint256(uint160(msg.sender))), hash);
  emit L1MessageSent(msg.sender, hash, _message);
}

</code></pre>
<p>As depicted in the leading image, this stage is where the message data splits. The full body of the message is emitted
for retrieval in Part 5 by the StateKeeper, while the hash of the message proceeds to be added to the Virtual Machine
(VM) - as it has to be included in the proof.</p>
<p>The method then sends the message’s hash to the <code>SystemContractHelper</code>, which makes an internal call:</p>
<pre><code class="language-solidity">function toL1(
  bool _isService,
  bytes32 _key,
  bytes32 _value
) internal {
  // ...
  address callAddr = TO_L1_CALL_ADDRESS;
  assembly {
    call(_isService, callAddr, _key, _value, 0xFFFF, 0, 0)
  }
}

</code></pre>
<p>Following the <code>TO_L1_CALL_ADDRESS</code>, we discover that it’s set to a placeholder value. So what exactly is occurring here?</p>
<h2 id="part-3---compiler-tricks-and-the-eravm"><a class="header" href="#part-3---compiler-tricks-and-the-eravm">Part 3 - Compiler Tricks and the EraVM</a></h2>
<p>Our VM features special opcodes designed to manage operations that aren’t possible in the Ethereum Virtual Machine
(EVM), such as publishing data to L1. But how can we make these features accessible to Solidity?</p>
<p>We could expand the language by introducing new Solidity opcodes, but that would require modifying the solc compiler,
among other things. Hence, we’ve adopted a different strategy.</p>
<p>To access these unique eraVM opcodes, the Solidity code simply executes a call to a specific address (the full list can
be seen <a href="https://github.com/matter-labs/era-system-contracts/blob/e96dfe0b5093fa95c2fb340c0411c646327db921/contracts/libraries/SystemContractsCaller.sol#L12">here</a>). This call is compiled by the solc frontend, and then on the compiler backend, we
intercept it and replace it with the correct eraVM opcode call <a href="https://github.com/matter-labs/era-compiler-llvm-context/blob/main/src/eravm/evm/call.rs">here</a>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match simulation_address {
  Some(compiler_common::ADDRESS_TO_L1) =&gt; {
    return crate::zkevm::general::to_l1(context, is_first, in_0, in_1);
  }
}
<span class="boring">}</span></code></pre></pre>
<p>This method allows your message to reach the VM.</p>
<h2 id="part-4---inside-the-virtual-machine"><a class="header" href="#part-4---inside-the-virtual-machine">Part 4 - Inside the Virtual Machine</a></h2>
<p>The zkEVM assembly translates these <a href="https://github.com/matter-labs/era-zkEVM-assembly/blob/v1.3.2/src/assembly/instruction/log.rs#L32">opcodes</a> into LogOpcodes.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const ALL_CANONICAL_MODIFIERS: [&amp;'static str; 5] =
    ["sread", "swrite", "event", "to_l1", "precompile"];
let variant = match idx {
  0 =&gt; LogOpcode::StorageRead,
  1 =&gt; LogOpcode::StorageWrite,
  2 =&gt; LogOpcode::Event,
  3 =&gt; LogOpcode::ToL1Message,
  4 =&gt; LogOpcode::PrecompileCall,
}
<span class="boring">}</span></code></pre></pre>
<p>Each opcode is then converted into the corresponding <a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/v1.3.2/src/definitions/log.rs#L16">LogOpcode</a> and written into the Log
<a href="https://github.com/matter-labs/era-zk_evm/blob/v1.3.2/src/opcodes/execution/log.rs">here</a>, which is handled by the EventSink oracle.</p>
<h2 id="part-5---the-role-of-the-state-keeper"><a class="header" href="#part-5---the-role-of-the-state-keeper">Part 5 - The Role of the State Keeper</a></h2>
<p>At this stage, the state keeper needs to collect all the messages generated by the VM execution and append them to the
calldata it transmits to Ethereum.</p>
<p>This process is divided into two steps:</p>
<ul>
<li>Retrieval of the ‘full’ messages</li>
<li>Extraction of all the message hashes.</li>
</ul>
<p>Why are these steps kept separate?</p>
<p>To avoid overwhelming our circuits with the content of entire messages, we relay them through Events, sending only their
hash to the VM. In this manner, the VM only adds to the proof the information that a message with a specific hash was
sent.</p>
<h3 id="retrieving-full-message-contents"><a class="header" href="#retrieving-full-message-contents">Retrieving Full Message Contents</a></h3>
<p>We go through all the Events generated during the run <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/types/src/event.rs#L147">here</a> and identify those coming from the
<code>L1_MESSENGER_ADDRESS</code> that corresponds to the <code>L1MessageSent</code> topic. These Events represent the ‘emit’ calls executed
in Part 2.</p>
<h3 id="retrieving-message-hashes"><a class="header" href="#retrieving-message-hashes">Retrieving Message Hashes</a></h3>
<p>Message hashes are transmitted alongside the other <code>l2_to_l1_logs</code> within the <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/vm/src/vm.rs#L81">VmExecutionResult</a>.</p>
<p>The StateKeeper collects them from the <a href="https://github.com/matter-labs/era-zk_evm_abstractions/blob/15a2af404902d5f10352e3d1fac693cc395fcff9/src/queries.rs#L30C2-L30C2">LogQueries</a> that the VM creates (these log queries also contain
information about storage writes, so we use the AUX_BYTE filter to determine which ones contain L1 messages. The entire
list can be found <a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/v1.3.2/src/system_params.rs#L37C39-L37C39">here</a>). The StateKeeper employs the VM’s EventSink to filter them out <a href="https://github.com/matter-labs/zksync-era/blob/43d7bd587a84b1b4489f4c6a4169ccb90e0df467/core/lib/vm/src/event_sink.rs#L116">here</a>.</p>
<h2 id="part-6---interaction-with-ethereum-l1"><a class="header" href="#part-6---interaction-with-ethereum-l1">Part 6 - Interaction with Ethereum (L1)</a></h2>
<p>After the StateKeeper has collected all the required data, it invokes the <code>CommitBlocks</code> method from the
<a href="https://github.com/matter-labs/era-contracts/blob/b04dcaf2256a9b2626eeaefbf1b281f0119d30ab/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol#L21">Executor.sol</a> contract.</p>
<p>Inside the <code>processL2Blocks</code> method, we iterate through the list of L2 message hashes, ensuring that the appropriate
full text is present for each:</p>
<pre><code class="language-solidity">// show preimage for hashed message stored in log
if (logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR) {
    (bytes32 hashedMessage, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);
    // check that the full message body matches the hash.
    require(keccak256(l2Messages[currentMessage]) == hashedMessage, "k2");
</code></pre>
<p>Currently, the executor is deployed on Ethereum mainnet at
<a href="https://etherscan.io/address/0x389a081BCf20e5803288183b929F08458F1d863D">0x389a081BCf20e5803288183b929F08458F1d863D</a>.</p>
<p>You can view an example of our contract execution from Part 1, carrying the message “My sample message”, in this Sepolia
transaction: <a href="https://sepolia.etherscan.io/tx/0x18c2a113d18c53237a4056403047ff9fafbf772cb83ccd44bb5b607f8108a64c">0x18c2a113d18c53237a4056403047ff9fafbf772cb83ccd44bb5b607f8108a64c</a>.</p>
<h2 id="part-7---verifying-message-inclusion"><a class="header" href="#part-7---verifying-message-inclusion">Part 7 - Verifying Message Inclusion</a></h2>
<p>We’ve now arrived at the final stage — how L1 users and contracts can confirm a message’s presence in L1.</p>
<p>This is accomplished through the <code>ProveL2MessageInclusion</code> function call in <a href="https://github.com/matter-labs/era-contracts/blob/b04dcaf2256a9b2626eeaefbf1b281f0119d30ab/l1-contracts/contracts/state-transition/chain-deps/facets/Mailbox.sol#L70">Mailbox.sol</a>.</p>
<p>Users supply the proof (merkle path) and the message, and the contract verifies that the merkle path is accurate and
matches the root hash.</p>
<pre><code class="language-solidity">bytes32 calculatedRootHash = Merkle.calculateRoot(_proof, _index, hashedLog);
bytes32 actualRootHash = s.l2LogsRootHashes[_blockNumber];

return actualRootHash == calculatedRootHash;
</code></pre>
<h2 id="summary-4"><a class="header" href="#summary-4">Summary</a></h2>
<p>In this article, we’ve traveled through a vast array of topics: from a user contract dispatching a message to L1 by
invoking a system contract, to this message’s hash making its way all the way to the VM via special opcodes. We’ve also
explored how it’s ultimately included in the execution results (as part of QueryLogs), gathered by the State Keeper, and
transmitted to L1 for final verification.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>Pubdata in ZKsync can be divided up into 4 different categories:</p>
<ol>
<li>L2 to L1 Logs</li>
<li>L2 to L1 Messages</li>
<li>Smart Contract Bytecodes</li>
<li>Storage writes</li>
</ol>
<p>Using data corresponding to these 4 facets, across all executed batches, we’re able to reconstruct the full state of L2.
One thing to note is that the way that the data is represented changes in a pre-boojum and post-boojum zkEVM. At a high
level, in a pre-boojum era these are represented as separate fields while in boojum they are packed into a single bytes
array.</p>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/advanced/09_pubdata.html#admonition-note"></a>
</div>
<div>
<p>When the 4844 was integrated this bytes array was moved from being part of the calldata to blob data.</p>
</div>
</div>
<p>While the structure of the pubdata changes, we can use the same strategy to pull the relevant information. First, we
need to filter all of the transactions to the L1 ZKsync contract for only the <code>commitBlocks/commitBatches</code> transactions
where the proposed block has been referenced by a corresponding <code>executeBlocks/executeBatches</code> call (the reason for this
is that a committed or even proven block can be reverted but an executed one cannot). Once we have all the committed
blocks that have been executed, we then will pull the transaction input and the relevant fields, applying them in order
to reconstruct the current state of L2.</p>
<p>One thing to note is that in both systems some of the contract bytecode is compressed into an array of indices where
each 2 byte index corresponds to an 8 byte word in a dictionary. More on how that is done <a href="guides/advanced/./11_compression.html">here</a>.
Once the bytecode has been expanded, the hash can be taken and checked against the storage writes within the
<code>AccountCodeStorage</code> contract which connects an address on L2 with the 32 byte code hash:</p>
<pre><code class="language-solidity">function _storeCodeHash(address _address, bytes32 _hash) internal {
  uint256 addressAsKey = uint256(uint160(_address));
  assembly {
    sstore(addressAsKey, _hash)
  }
}

</code></pre>
<h3 id="pre-boojum-era"><a class="header" href="#pre-boojum-era">Pre-Boojum Era</a></h3>
<p>In pre-boojum era the superset of pubdata fields and input to the <code>commitBlocks</code> function follows the following format:</p>
<pre><code class="language-solidity">/// @notice Data needed to commit new block
/// @param blockNumber Number of the committed block
/// @param timestamp Unix timestamp denoting the start of the block execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param l2LogsTreeRoot The root hash of the tree that contains all L2 -&gt; L1 logs in the block
/// @param priorityOperationsHash Hash of all priority operations from this block
/// @param initialStorageChanges Storage write access as a concatenation key-value
/// @param repeatedStorageChanges Storage write access as a concatenation index-value
/// @param l2Logs concatenation of all L2 -&gt; L1 logs in the block
/// @param l2ArbitraryLengthMessages array of hash preimages that were sent as value of L2 logs by special system L2 contract
/// @param factoryDeps (contract bytecodes) array of L2 bytecodes that were deployed
struct CommitBlockInfo {
  uint64 blockNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 l2LogsTreeRoot;
  bytes32 priorityOperationsHash;
  bytes initialStorageChanges;
  bytes repeatedStorageChanges;
  bytes l2Logs;
  bytes[] l2ArbitraryLengthMessages;
  bytes[] factoryDeps;
}

</code></pre>
<p>The 4 main fields to look at here are:</p>
<ol>
<li><code>initialStorageChanges</code>: Storage slots being written to for the first time and the corresponding value
<ol>
<li>Structure: <code>num entries as u32 || for each entry: (32 bytes key, 32 bytes final value)</code></li>
</ol>
</li>
<li><code>repeatedStorageChanges</code>: ids of the slots being written to and the corresponding value
<ol>
<li>Structure: <code>num entries as u32 || for each entry: (8 byte id, 32 bytes final value)</code></li>
</ol>
</li>
<li><code>factoryDeps</code>: An array of uncompressed bytecodes</li>
<li><code>l2ArbitraryLengthMessages</code> : L2 → L1 Messages
<ol>
<li>We don’t need them all, we are just concerned with messages sent from the <code>Compressor/BytecodeCompressor</code> contract</li>
<li>These messages will follow the compression algorithm outline <a href="guides/advanced/./11_compression.html">here</a></li>
</ol>
</li>
</ol>
<p>For the ids on the repeated writes, they are generated as we process the first time keys. For example: if we see
<code>[&lt;key1, val1&gt;, &lt;key2, val2&gt;]</code> (starting from an empty state) then we can assume that the next time a write happens to
<code>key1</code> it will be encoded as <code>&lt;1, new_val&gt;</code> and so on and so forth. There is a little shortcut here where the last new
id generated as part of a batch will be in the <code>indexRepeatedStorageChanges</code> field.</p>
<h3 id="post-boojum-era"><a class="header" href="#post-boojum-era">Post-Boojum Era</a></h3>
<pre><code class="language-solidity">/// @notice Data needed to commit new batch
/// @param batchNumber Number of the committed batch
/// @param timestamp Unix timestamp denoting the start of the batch execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param priorityOperationsHash Hash of all priority operations from this batch
/// @param bootloaderHeapInitialContentsHash Hash of the initial contents of the bootloader heap. In practice it serves as the commitment to the transactions in the batch.
/// @param eventsQueueStateHash Hash of the events queue state. In practice it serves as the commitment to the events in the batch.
/// @param systemLogs concatenation of all L2 -&gt; L1 system logs in the batch
/// @param pubdataCommitments Packed pubdata commitments/data.
/// @dev pubdataCommitments format: This will always start with a 1 byte pubdataSource flag. Current allowed values are 0 (calldata) or 1 (blobs)
///                             kzg: list of: opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes) = 144 bytes
///                             calldata: pubdataCommitments.length - 1 - 32 bytes of pubdata
///                                       and 32 bytes appended to serve as the blob commitment part for the aux output part of the batch commitment
/// @dev For 2 blobs we will be sending 288 bytes of calldata instead of the full amount for pubdata.
/// @dev When using calldata, we only need to send one blob commitment since the max number of bytes in calldata fits in a single blob and we can pull the
///     linear hash from the system logs
struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes pubdataCommitments;
}

</code></pre>
<p>The main difference between the two <code>CommitBatchInfo</code> and <code>CommitBlockInfo</code> structs is that we have taken a few of the
fields and merged them into a single bytes array called <code>pubdataCommitments</code>. In the <code>calldata</code> mode, the pubdata is
being passed using that field. In the <code>blobs</code> mode, that field is used to store the KZG commitments and proofs. More on
EIP-4844 blobs <a href="guides/advanced/./10_pubdata_with_blobs.html">here</a>. In the Validium mode, the field will either be empty or store the
inclusion proof for the DA blob.</p>
<p>The 2 main fields needed for state reconstruction are the bytecodes and the state diffs. The bytecodes follow the same
structure and reasoning in the old system (as explained above). The state diffs will follow the compression illustrated
below.</p>
<h3 id="compression-of-state-diffs-in-post-boojum-era"><a class="header" href="#compression-of-state-diffs-in-post-boojum-era">Compression of State Diffs in Post-Boojum Era</a></h3>
<h4 id="keys"><a class="header" href="#keys">Keys</a></h4>
<p>Keys will be packed in the same way as they were before boojum. The only change is that we’ll avoid using the 8-byte
enumeration index and will pack it to the minimal necessary number of bytes. This number will be part of the pubdata.
Once a key has been used, it can already use the 4 or 5 byte enumeration index and it is very hard to have something
cheaper for keys that has been used already. The opportunity comes when remembering the ids for accounts to spare some
bytes on nonce/balance key, but ultimately the complexity may not be worth it.</p>
<p>There is some room for the keys that are being written for the first time, however, these are rather more complex and
achieve only a one-time effect (when the key is published for the first time).</p>
<h4 id="values"><a class="header" href="#values">Values</a></h4>
<p>Values are much easier to compress, since they usually contain only zeroes. Also, we can leverage the nature of how
those values are changed. For instance if nonce has been increased only by 1, we do not need to write the entire 32-byte
new value, we can just tell that the slot has been <em>increased</em> and then supply only 1-byte value of <em>the size by which</em>
it was increased. This way instead of 32 bytes we need to publish only 2 bytes: first byte to denote which operation has
been applied and the second by to denote the size by which the addition has been made.</p>
<p>If we decide to have just the following 4 types of changes: <code>Add</code>, <code>Sub,</code> <code>Transform</code>, <code>NoCompression</code> where:</p>
<ul>
<li><code>Add</code> denotes that the value has been increased. (modulo 2^256)</li>
<li><code>Sub</code> denotes that the value has been decreased. (modulo 2^256)</li>
<li><code>Transform</code> denotes the value just has been changed (i.e. we disregard any potential relation between the previous and
the new value, though the new value might be small enough to save up on the number of bytes).</li>
<li><code>NoCompression</code> denotes that the whole 32 byte value will be used.</li>
</ul>
<p>Where the byte size of the output can be anywhere from 0 to 31 (also 0 makes sense for <code>Transform</code>, since it denotes
that it has been zeroed out). For <code>NoCompression</code> the whole 32 byte value is used.</p>
<p>So the format of the pubdata will be the following:</p>
<h5 id="part-1-header"><a class="header" href="#part-1-header">Part 1. Header</a></h5>
<ul>
<li><code>&lt;version = 1 byte&gt;</code> — this will enable easier automated unpacking in the future. Currently, it will be only equal to
<code>1</code>.</li>
<li><code>&lt;total_logs_len = 3 bytes&gt;</code> — we need only 3 bytes to describe the total length of the L2→L1 logs.</li>
<li><code>&lt;the number of bytes used for derived keys = 1 byte&gt;</code>. At the beginning it will be equal to <code>4</code>, but then it will
automatically switch to <code>5</code> when needed.</li>
</ul>
<h5 id="part-2-initial-writes"><a class="header" href="#part-2-initial-writes">Part 2. Initial writes</a></h5>
<ul>
<li><code>&lt;num_of_initial_writes = 2 bytes&gt;</code> (since each initial write publishes at least 32 bytes for key, then
<code>2^16 * 32 = 2097152</code> will be enough for a lot of time (right now with the limit of 120kb it will take more than 15 L1
txs to use up all the space there).</li>
<li>Then for each <code>&lt;key, value&gt;</code> pair for each initial write:
<ul>
<li>print key as 32-byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>). More on it
<a href="https://www.notion.so/Pubdata-compression-v1-4b0dd8c151014c8ab96dbd7e66e17599?pvs=21">below</a>.</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<h4 id="part-3-repeated-writes"><a class="header" href="#part-3-repeated-writes">Part 3. Repeated writes</a></h4>
<p>Note, that there is no need to write the number of repeated writes, since we know that until the end of the pubdata, all
the writes will be repeated ones.</p>
<ul>
<li>For each <code>&lt;key, value&gt;</code> pair for each repeated write:
<ul>
<li>print key as either 4 or 5 byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pubdata-post-4844"><a class="header" href="#pubdata-post-4844">Pubdata Post 4844</a></h1>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>EIP-4844, commonly known as Proto-Danksharding, is an upgrade to the ethereum protocol that introduces a new data
availability solution embedded in layer 1. More information about it can be found
<a href="https://ethereum.org/en/roadmap/danksharding/">here</a>. With proto-danksharding we can utilize the new blob data
availability for cheaper storage of pubdata when we commit batches resulting in more transactions per batch and cheaper
batches/transactions. We want to ensure we have the flexibility at the contract level to process both pubdata via
calldata, as well as pubdata via blobs. A quick callout here, while 4844 has introduced blobs as new DA layer, it is the
first step in full Danksharding. With full Danksharding ethereum will be able to handle a total of 64 blobs per block
unlike 4844 which supports just 6 per block.</p>
<blockquote>
<p>💡 Given the nature of 4844 development from a solidity viewpoint, we’ve had to create a temporary contract
<code>BlobVersionedHash.yul</code> which acts in place of the eventual <code>BLOBHASH</code> opcode.</p>
</blockquote>
<h2 id="technical-approach"><a class="header" href="#technical-approach">Technical Approach</a></h2>
<p>The approach spans both L2 system contracts and L1 ZKsync contracts (namely <code>Executor.sol</code>). When a batch is sealed on
L2 we will chunk it into blob-sized pieces (4096 elements * 31 bytes per what is required by our circuits), take the
hash of each chunk, and send them to L1 via system logs. Within <code>Executor.sol</code> , when we are dealing with blob-based
commitments, we verify that the blob contains the correct data with the point evaluation precompile. If the batch
utilizes calldata instead, the processing should remain the same as in a pre-4844 ZKsync. Regardless of if pubdata is in
calldata or blobs are used, the batch’s commitment changes as we include new data within the auxiliary output.</p>
<p>Given that this is the first step to a longer-term solution, and the restrictions of proto-danksharding that get lifted
for full danksharding, we impose the following constraints:</p>
<ol>
<li>we will support a maximum of 2 blobs per batch</li>
<li>only 1 batch will be committed in a given transaction</li>
<li>we will always send 2 system logs (one for each potential blob commitment) even if the batch only uses 1 blob.</li>
</ol>
<p>This simplifies the processing logic on L1 and stops us from increasing the blob base fee (increases when there 3 or
more blobs in a given block).</p>
<h2 id="backward-compatibility"><a class="header" href="#backward-compatibility">Backward-compatibility</a></h2>
<p>While some of the parameter formatting changes, we maintain the same function signature for <code>commitBatches</code> and still
allow for pubdata to be submitted via calldata:</p>
<pre><code class="language-solidity">struct StoredBatchInfo {
  uint64 batchNumber;
  bytes32 batchHash;
  uint64 indexRepeatedStorageChanges;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 l2LogsTreeRoot;
  uint256 timestamp;
  bytes32 commitment;
}

struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes pubdataCommitments;
}

function commitBatches(StoredBatchInfo calldata _lastCommittedBatchData, CommitBatchInfo[] calldata _newBatchesData)
  external;

</code></pre>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<h3 id="bootloader-memory"><a class="header" href="#bootloader-memory">Bootloader Memory</a></h3>
<p>With the increase in the amount of pubdata due to blobs, changes can be made to the bootloader memory to facilitate more
l2 to l1 logs, compressed bytecodes, and pubdata. We take the naive approach for l2 to l1 logs and the compressed
bytecode, doubling their previous constraints from <code>2048</code> logs and <code>32768 slots</code> to <code>4096 logs</code> and <code>65536 slots</code>
respectively. We then increase the number of slots for pubdata from <code>208000</code> to <code>411900</code>. Copying the comment around
pubdata slot calculation from our code:</p>
<pre><code class="language-solidity">One of "worst case" scenarios for the number of state diffs in a batch is when 240kb of pubdata is spent
on repeated writes, that are all zeroed out. In this case, the number of diffs is 240k / 5 = 48k. This means that they will have
accommodate 13056000 bytes of calldata for the uncompressed state diffs. Adding 120k on top leaves us with
roughly 13176000 bytes needed for calldata. 411750 slots are needed to accommodate this amount of data.
We round up to 411900 slots just in case.
</code></pre>
<p>The overall bootloader max memory is increased from <code>24000000</code> to <code>30000000</code> bytes to accommodate the increases.</p>
<h3 id="l2-system-contracts"><a class="header" href="#l2-system-contracts">L2 System Contracts</a></h3>
<p>We introduce a new system contract PubdataChunkPublisher that takes the full pubdata, creates chunks that are each
126,976 bytes in length (this is calculated as 4096 elements per blob each of which has 31 bytes), and commits them in
the form of 2 system logs. We have the following keys for system logs:</p>
<pre><code class="language-solidity">enum SystemLogKey {
  L2_TO_L1_LOGS_TREE_ROOT_KEY,
  TOTAL_L2_TO_L1_PUBDATA_KEY,
  STATE_DIFF_HASH_KEY,
  PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY,
  PREV_BATCH_HASH_KEY,
  CHAINED_PRIORITY_TXN_HASH_KEY,
  NUMBER_OF_LAYER_1_TXS_KEY,
  BLOB_ONE_HASH_KEY,
  BLOB_TWO_HASH_KEY,
  EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY
}

</code></pre>
<p>In addition to the blob commitments, the hash of the total pubdata is still sent and is used if a batch is committed
with pubdata as calldata vs as blob data. As stated earlier, even when we only have enough pubdata for a single blob, 2
system logs are sent. The hash value in the second log in this case will <code>bytes32(0)</code> .</p>
<p>One important thing is that we don’t try to reason about the data here, that is done in the L1Messenger and Compressor
contracts. The main purpose of this is to commit to blobs and have those commitments travel to L1 via system logs.</p>
<h3 id="l1-executor-facet"><a class="header" href="#l1-executor-facet">L1 Executor Facet</a></h3>
<p>While the function signature for <code>commitBatches</code> and the structure of <code>CommitBatchInfo</code> stays the same, the format of
<code>CommitBatchInfo.pubdataCommitments</code> changes. Before 4844, this field held a byte array of pubdata, now it can hold
either the total pubdata as before or it can hold a list of concatenated info for kzg blob commitments. To differentiate
between the two, a header byte is prepended to the byte array. At the moment we only support 2 values:</p>
<pre><code class="language-solidity">/// @dev Enum used to determine the source of pubdata. At first we will support calldata and blobs but this can be extended.
enum PubdataSource {
    Calldata = 0,
    Blob = 1
}
</code></pre>
<p>We reject all other values in the first byte.</p>
<h3 id="calldata-based-pubdata-processing"><a class="header" href="#calldata-based-pubdata-processing">Calldata Based Pubdata Processing</a></h3>
<p>When using calldata, we want to operate on <code>pubdataCommitments[1:pubdataCommitments.length - 32]</code> as this is the full
pubdata that was committed to via system logs. The reason we don’t operate on the last 32 bytes is that we also include
what the blob commitment for this data would be as a way to make our witness generation more generic. Only a single blob
commitment is needed for this as the max size of calldata is the same size as a single blob. When processing the system
logs in this context, we will check the hash of the supplied pubdata without the 1 byte header for pubdata source
against the value in the corresponding system log with key <code>TOTAL_L2_TO_L1_PUBDATA_KEY</code>. We still require logs for the 2
blob commitments, even if these logs contain values we will substitute them for <code>bytes32(0)</code> when constructing the batch
commitment.</p>
<h3 id="blob-based-pubdata-processing"><a class="header" href="#blob-based-pubdata-processing">Blob Based Pubdata Processing</a></h3>
<p>The format for <code>pubdataCommitments</code> changes when we send pubdata as blobs, containing data we need to verify the blob
contents via the newly introduced point evaluation precompile. The data is <code>pubdataCommitments[1:]</code> is the concatenation
of <code>opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)</code> for each blob
attached to the transaction, lowering our calldata from N → 144 bytes per blob. More on how this is used later on.</p>
<p>Utilizing blobs causes us to process logs in a slightly different way. Similar to how it’s done when pubdata is sent via
calldata, we require a system log with a key of the <code>TOTAL_L2_TO_L1_PUBDATA_KEY</code> , although the value is ignored and
extract the 2 blob hashes from the <code>BLOB_ONE_HASH_KEY</code> and <code>BLOB_TWO_HASH_KEY</code> system logs to be used in the batch
commitment.</p>
<p>While calldata verification is simple, comparing the hash of the supplied calldata versus the value in the system log,
we need to take a few extra steps when verifying the blobs attached to the transaction contain the correct data. After
processing the logs and getting the 2 blob linear hashes, we will have all the data we need to call the
<a href="https://eips.ethereum.org/EIPS/eip-4844#point-evaluation-precompile">point evaluation precompile</a>. Recall that the
contents of <code>pubdataCommitments</code> have the opening point (in its 16 byte form), claimed value, the commitment, and the
proof of this claimed value. The last piece of information we need is the blob’s versioned hash (obtained via <code>BLOBHASH</code>
opcode).</p>
<p>There are checks within <code>_verifyBlobInformation</code> that ensure that we have the correct blob linear hashes and that if we
aren’t expecting a second blob, the linear hash should be equal to <code>bytes32(0)</code>. This is how we signal to our circuits
that we didn’t publish any information in the second blob.</p>
<p>Verifying the commitment via the point evaluation precompile goes as follows (note that we assume the header byte for
pubdataSource has already been removed by this point):</p>
<pre><code class="language-solidity">// The opening point is passed as 16 bytes as that is what our circuits expect and use when verifying the new batch commitment
// PUBDATA_COMMITMENT_SIZE = 144 bytes
pubdata_commitments &lt;- [opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)] from calldata
opening_point = bytes32(pubdata_commitments[:16])
versioned_hash &lt;- from BLOBHASH opcode

// Given that we needed to pad the opening point for the precompile, append the data after.
point_eval_input = versioned_hash || opening_point || pubdataCommitments[16: PUBDATA_COMMITMENT_SIZE]

// this part handles the following:
// verify versioned_hash == hash(commitment)
// verify P(z) = y
res &lt;- point_valuation_precompile(point_eval_input)

assert uint256(res[32:]) == BLS_MODULUS
</code></pre>
<p>Where correctness is validated by checking the latter 32 bytes of output from the point evaluation call is equal to
<code>BLS_MODULUS</code>.</p>
<h3 id="batch-commitment-and-proof-of-equivalence"><a class="header" href="#batch-commitment-and-proof-of-equivalence">Batch Commitment and Proof of Equivalence</a></h3>
<p>With the contents of the blob being verified, we need to add this information to the batch commitment so that it can
further be part of the verification of the overall batch by our proof system. Our batch commitment is the hashing of a
few different values: passthrough data (holding our new state root, and next enumeration index to be used), meta
parameters (flag for if zk porter is available, bootloader bytecode hash, and default account bytecode hash), and
auxiliary output. The auxiliary output changes with 4844, adding in 4 new fields and the new corresponding encoding:</p>
<ul>
<li>2 <code>bytes32</code> fields for linear hashes
<ul>
<li>These are the hashes of the blob’s preimages</li>
</ul>
</li>
<li>2 <code>bytes32</code> for 4844 output commitment hashes
<ul>
<li>These are <code>(versioned hash || opening point || evaluation value)</code></li>
<li>The format of the opening point here is expected to be the 16 byte value passed by calldata</li>
</ul>
</li>
<li>We encode an additional 28 <code>bytes32(0)</code> at the end because with the inclusion of vm 1.5.0, our circuits support a
total of 16 blobs that will be used once the total number of blobs supported by ethereum increase.</li>
</ul>
<pre><code class="language-solidity">abi.encode(
    l2ToL1LogsHash,
    _stateDiffHash,
    _batch.bootloaderHeapInitialContentsHash,
    _batch.eventsQueueStateHash,
    _blob1LinearHash,
    _blob1OutputCommitment,
    _blob2LinearHash,
    _blob2OutputCommitment,
    _encode28Bytes32Zeroes()
);
</code></pre>
<p>There are 3 different scenarios that change the values posted here:</p>
<ol>
<li>We submit pubdata via calldata</li>
<li>We only utilize a single blob</li>
<li>We use both blobs</li>
</ol>
<p>When we use calldata, the values <code>_blob1LinearHash</code>, <code>_blob1OutputCommitment</code>, <code>_blob2LinearHash</code>, and
<code>_blob2OutputCommitment</code> should all be <code>bytes32(0)</code>. If we are using blobs but only have a single blob,
<code>_blob1LinearHash</code> and <code>_blob1OutputCommitment</code> should correspond to that blob, while <code>_blob2LinearHash</code> and
<code>_blob2OutputCommitment</code> will be <code>bytes32(0)</code>. Following this, when we use both blobs, the data for these should be
present in all of the values.</p>
<p>Our circuits will then handle the proof of equivalence, following a method similar to the moderate approach mentioned
<a href="https://notes.ethereum.org/@vbuterin/proto_danksharding_faq#Moderate-approach-works-with-any-ZK-SNARK">here</a>, verifying
that the total pubdata can be repackaged as the blobs we submitted and that the commitments in fact evaluate to the
given value at the computed opening point.</p>
<h2 id="pubdata-contents-and-blobs"><a class="header" href="#pubdata-contents-and-blobs">Pubdata Contents and Blobs</a></h2>
<p>Given how data representation changes on the consensus layer (where blobs live) versus on the execution layer (where
calldata is found), there is some preprocessing that takes place to make it compatible. When calldata is used for
pubdata, we keep it as is and no additional processing is required to transform it. Recalling the above section when
pubdata is sent via calldata it has the format: source byte (1 bytes) || pubdata || blob commitment (32 bytes) and so we
must first trim it of the source byte and blob commitment before decoding it. A more detailed guide on the format can be
found in our documentation. Using blobs requires a few more steps:</p>
<pre><code class="language-python">ZKSYNC_BLOB_SIZE = 31 * 4096

# First we pad the pubdata with the required amount of zeroes to fill
# the nearest blobs
padding_amount = ZKSYNC_BLOB_SIZE - len(pubdata) % ZKSYNC_BLOB_SIZE)
padded_pubdata = pad_right_with_zeroes(pubdata, padding_amount)

# We then chunk them into `ZKSYNC_BLOB_SIZE` sized arrays
blobs = chunk(padded_pubdata, ZKSYNC_BLOB_SIZE)

# Each blob is then encoded to be compatible with the CL
for blob in blobs:
    encoded_blob = zksync_pubdata_into_ethereum_4844_data(blob)
</code></pre>
<p>Now we can apply the encoding formula, with some of the data from the blob commit transaction to move from encoded blobs
back into decodable zksync pubdata:</p>
<pre><code class="language-python"># opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)
BLOB_PUBDATA_COMMITMENT_SIZE = 144

# Parse the kzg commitment from the commit calldata
commit_calldata_without_source = commit_calldata[1:]
for i in range(0, len(commit_calldata_without_source), BLOB_PUBDATA_COMMITMENT_SIZE):
    # We can skip the opening point and claimed value, ignoring the proof
    kzg_commitment = commit_calldata_without_source[48:96]

# We then need to pull the blobs in the correct order, this can be found by matching
# each blob with their kzg_commitment keeping the order from the calldata
encoded_blobs = pull_blob_for_each_kzg_commitment(kzg_commitments)

# Decode each blob into the zksync specific format
for encoded_blob in encoded_blobs:
    decoded_blob = ethereum_4844_data_into_zksync_pubdata(encoded_blob)

reconstructed_pubdata = concat(decoded_blobs)
</code></pre>
<p>The last thing to do depends on the strategy taken, the two approaches are:</p>
<ul>
<li>Remove all trailing zeroes after concatenation</li>
<li>Parse the data and ignore the extra zeroes at the end</li>
</ul>
<p>The second option is a bit messier so going with the first, we can then decode the pubdata and when we get to the last
state diff, if the number of bytes is less than specified we know that the remaining data are zeroes. The needed
functions can be found within the
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/3a973afb3cf2b50b7138c1af61cc6ac3d7d0189f/src/eip_4844/mod.rs#L358">zkevm_circuits code</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bytecode-compression"><a class="header" href="#bytecode-compression">Bytecode Compression</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>As we are a rollup - all the bytecodes that contracts used in our chain must be copied into L1 (so that the chain can be
reconstructed from L1 if needed).</p>
<p>Given the want/need to cutdown on space used, bytecode is compressed prior to being posted to L1. At a high level
bytecode is chunked into opcodes (which have a size of 8 bytes), assigned a 2 byte index, and the newly formed byte
sequence (indexes) are verified and sent to L1. This process is split into 2 different parts: (1)
<a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/utils/src/bytecode.rs#L31">the server side operator</a>
handling the compression and (2)
<a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/Compressor.sol">the system contract</a> verifying
that the compression is correct before sending to L1.</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p>Original bytecode:</p>
<pre><code>0x000000000000000A000000000000000D000000000000000A000000000000000C000000000000000B000000000000000A000000000000000D000000000000000A000000000000000D000000000000000A000000000000000B000000000000000B
</code></pre>
<p>Split to 8-byte chunks:</p>
<pre><code>000000000000000A 000000000000000D 000000000000000A 000000000000000C
000000000000000B 000000000000000A 000000000000000D 000000000000000A
000000000000000D 000000000000000A 000000000000000B 000000000000000B
</code></pre>
<p>Dictionary would be:</p>
<pre><code>0 -&gt; 0xA (count: 5)
1 -&gt; 0xD (count: 3, first seen: 1)
2 -&gt; 0xB (count: 3, first seen: 4)
3 -&gt; 0xC (count: 1)
</code></pre>
<p>Note that <code>1</code> maps to <code>0xD</code>, as it occurs three times, and first occurrence is earlier than first occurrence of <code>0xB</code>,
that also occurs three times.</p>
<p>Compressed bytecode:</p>
<pre><code>0x0004000000000000000A000000000000000D000000000000000B000000000000000C000000010000000300020000000100000001000000020002
</code></pre>
<p>Split into three parts:</p>
<ol>
<li><code>length_of_dict</code> is stored in the first 2 bytes</li>
<li>dictionary entries are stored in the next <code>8 * length_of_dict</code> bytes</li>
<li>2-byte indices are stored in the rest of the bytes</li>
</ol>
<pre><code>0004

000000000000000A 000000000000000D 000000000000000B 000000000000000C

0000 0001 0000 0003
0002 0000 0001 0000
0001 0000 0002 0002
</code></pre>
<h2 id="server-side-operator"><a class="header" href="#server-side-operator">Server Side Operator</a></h2>
<p>This is the part that is responsible for taking bytecode, that has already been chunked into 8 byte words, performing
validation, and compressing it.</p>
<h3 id="validation-rules"><a class="header" href="#validation-rules">Validation Rules</a></h3>
<p>For bytecode to be considered valid it must satisfy the following:</p>
<ol>
<li>Bytecode length must be less than 2097120 ((2^16 - 1) * 32) bytes.</li>
<li>Bytecode length must be a multiple of 32.</li>
<li>Number of words cannot be even.</li>
</ol>
<p><a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/utils/src/bytecode.rs#L133">Source</a></p>
<h3 id="compression-algorithm"><a class="header" href="#compression-algorithm">Compression Algorithm</a></h3>
<p>At a high level, each 8 byte word from the chunked bytecode is assigned a 2 byte index (constraint on size of dictionary
of chunk → index is 2^16 + 1 elements). The length of the dictionary, dictionary entries (index assumed through order),
and indexes are all concatenated together to yield the final compressed version.</p>
<p>The following is a simplified version of the algorithm:</p>
<pre><code class="language-python">
statistic: Map[chunk, (count, first_pos)]
dictionary: Map[chunk, index]
encoded_data: List[index]

for position, chunk in chunked_bytecode:
    if chunk is in statistic:
        statistic[chunk].count += 1
    else:
        statistic[chunk] = (count=1, first_pos=pos)

statistic.sort(primary=count, secondary=first_pos, order=desc)

for chunk in sorted_statistic:
    dictionary[chunk] = len(dictionary) # length of dictionary used to keep track of index

for chunk in chunked_bytecode:
    encoded_data.append(dictionary[chunk])

return [len(dictionary), dictionary.keys(order=index asc), encoded_data]
</code></pre>
<h2 id="system-contract-compression-verification--publishing"><a class="header" href="#system-contract-compression-verification--publishing">System Contract Compression Verification &amp; Publishing</a></h2>
<p>The <a href="https://github.com/matter-labs/era-system-contracts/blob/main/contracts/Compressor.sol">Bytecode Compressor</a>
contract performs validation on the compressed bytecode generated on the server side. At the current moment, publishing
bytecode to L1 may only be called by the bootloader but in the future anyone will be able to publish compressed bytecode
with no change to the underlying algorithm.</p>
<h3 id="verification--publication"><a class="header" href="#verification--publication">Verification &amp; Publication</a></h3>
<p>The function <code>publishCompressBytecode</code> takes in both the original <code>_bytecode</code> and the <code>_rawCompressedData</code> , the latter
of which comes from the server’s compression algorithm output. Looping over the encoded data, derived from
<code>_rawCompressedData</code> , the corresponding chunks are retrieved from the dictionary and compared to the original byte
code, reverting if there is a mismatch. After the encoded data has been verified, it is published to L1 and marked
accordingly within the <code>KnownCodesStorage</code> contract.</p>
<p>Pseudo-code implementation:</p>
<pre><code class="language-python">length_of_dict = _rawCompressedData[:2]
dictionary = _rawCompressedData[2:2 + length_of_dict * 8] # need to offset by bytes used to store length (2) and multiply by 8 for chunk size
encoded_data = _rawCompressedData[2 + length_of_dict * 8:]

assert(len(dictionary) % 8 == 0) # each element should be 8 bytes
assert(num_entries(dictionary) &lt;= 2^16)
assert(len(encoded_data) * 4 == len(_bytecode)) # given that each chunk is 8 bytes and each index is 2 bytes they should differ by a factor of 4

for index in encoded_data:
    encoded_chunk = dictionary[index]
    real_chunk = _bytecode.readUint64(index * 4) # need to pull from index * 4 to account for difference in element size
    verify(encoded_chunk == real_chunk)

sendToL1(_rawCompressedBytecode)
markPublished(hash(_bytecode), hash(_rawCompressedData), len(_rawCompressedData))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zkevm-internals"><a class="header" href="#zkevm-internals">zkEVM internals</a></h1>
<h2 id="zkevm-clarifier"><a class="header" href="#zkevm-clarifier">zkEVM clarifier</a></h2>
<p>The ZKsync zkEVM plays a fundamentally different role in the zkStack than the EVM does in Ethereum. The EVM is used to
execute code in Ethereum’s state transition function. This STF needs a client to implement and run it. Ethereum has a
multi-client philosophy, there are multiple clients, and they are written in Go, Rust, and other traditional programming
languages, all running and verifying the same STF.</p>
<p>We have a different set of requirements, we need to produce a proof that some client executed the STF correctly. The
first consequence is that the client needs to be hard-coded, we cannot have the same multi-client philosophy. This
client is the zkEVM, it can run the STF efficiently, including execution of smart contracts similarly to the EVM. The
zkEVM was also designed to be proven efficiently.</p>
<p>For efficiency reasons it the zkEVM is similar to the EVM. This makes executing smart programs inside of it easy. It
also has special features that are not in the EVM but are needed for the rollup’s STF, storage, gas metering,
precompiles and other things. Some of these features are implemented as system contracts while others are built into the
VM. System Contracts are contracts with special permissions, deployed at predefined addresses. Finally, we have the
bootloader, which is also a contract, although it is not deployed at any address. This is the STF that is ultimately
executed by the zkEVM, and executes the transaction against the state.</p>
<!-- kl to do *Add different abstraction levels diagram here:*-->
<p>Full specification of the zkEVM is beyond the scope of this document. However, this section will give you most of the
details needed for understanding the L2 system smart contracts &amp; basic differences between EVM and zkEVM. Note also that
usually understanding the EVM is needed for efficient smart contract development. Understanding the zkEVM goes beyond
this, it is needed for developing the rollup itself.</p>
<h2 id="registers-and-memory-management"><a class="header" href="#registers-and-memory-management">Registers and memory management</a></h2>
<p>On EVM, during transaction execution, the following memory areas are available:</p>
<ul>
<li><code>memory</code> itself.</li>
<li><code>calldata</code> the immutable slice of parent memory.</li>
<li><code>returndata</code> the immutable slice returned by the latest call to another contract.</li>
<li><code>stack</code> where the local variables are stored.</li>
</ul>
<p>Unlike EVM, which is stack machine, zkEVM has 16 registers. Instead of receiving input from <code>calldata</code>, zkEVM starts by
receiving a <em>pointer</em> in its first register <em>(<em>basically a packed struct with 4 elements: the memory page id, start and
length of the slice to which it points to</em>)</em> to the calldata page of the parent. Similarly, a transaction can receive
some other additional data within its registers at the start of the program: whether the transaction should invoke the
constructor
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#contractdeployer--immutablesimulator">more about deployments here</a>,
whether the transaction has <code>isSystem</code> flag, etc. The meaning of each of these flags will be expanded further in this
section.</p>
<p><em>Pointers</em> are separate type in the VM. It is only possible to:</p>
<ul>
<li>Read some value within a pointer.</li>
<li>Shrink the pointer by reducing the slice to which pointer points to.</li>
<li>Receive the pointer to the <code>returndata</code> as a calldata.</li>
<li>Pointers can be stored only on stack/registers to make sure that the other contracts can not read <code>memory/returndata</code>
of contracts they are not supposed to.</li>
<li>A pointer can be converted to the u256 integer representing it, but an integer can not be converted to a pointer to
prevent unallowed memory access.</li>
<li>It is not possible to return a pointer that points to a memory page with id smaller than the one for the current page.
What this means is that it is only possible to <code>return</code> only pointer to the memory of the current frame or one of the
pointers returned by the subcalls of the current frame.</li>
</ul>
<h3 id="memory-areas-in-zkevm"><a class="header" href="#memory-areas-in-zkevm">Memory areas in zkEVM</a></h3>
<p>For each frame, the following memory areas are allocated:</p>
<ul>
<li><em>Heap</em> (plays the same role as <code>memory</code> on Ethereum).</li>
<li><em>AuxHeap</em> (auxiliary heap). It has the same properties as Heap, but it is used for the compiler to encode
calldata/copy the <code>returndata</code> from the calls to system contracts to not interfere with the standard Solidity memory
alignment.</li>
<li><em>Stack</em>. Unlike Ethereum, stack is not the primary place to get arguments for opcodes. The biggest difference between
stack on zkEVM and EVM is that on ZKsync stack can be accessed at any location (just like memory). While users do not
pay for the growth of stack, the stack can be fully cleared at the end of the frame, so the overhead is minimal.</li>
<li><em>Code</em>. The memory area from which the VM executes the code of the contract. The contract itself can not read the code
page, it is only done implicitly by the VM.</li>
</ul>
<p>Also, as mentioned in the previous section, the contract receives the pointer to the calldata.</p>
<h3 id="managing-returndata--calldata"><a class="header" href="#managing-returndata--calldata">Managing returndata &amp; calldata</a></h3>
<p>Whenever a contract finishes its execution, the parent’s frame receives a <em>pointer</em> as <code>returndata</code>. This pointer may
point to the child frame’s Heap/AuxHeap or it can even be the same <code>returndata</code> pointer that the child frame received
from some of its child frames.</p>
<p>The same goes with the <code>calldata</code>. Whenever a contract starts its execution, it receives the pointer to the calldata.
The parent frame can provide any valid pointer as the calldata, which means it can either be a pointer to the slice of
parent’s frame memory (heap or auxHeap) or it can be some valid pointer that the parent frame has received before as
calldata/returndata.</p>
<p>Contracts simply remember the calldata pointer at the start of the execution frame (it is by design of the compiler) and
remembers the latest received returndata pointer.</p>
<p>Some important implications of this is that it is now possible to do the following calls without any memory copying:</p>
<p>A → B → C</p>
<p>where C receives a slice of the calldata received by B.</p>
<p>The same goes for returning data:</p>
<p>A ← B ← C</p>
<p>There is no need to copy returned data if the B returns a slice of the returndata returned by C.</p>
<p>Note, that you can <em>not</em> use the pointer that you received via calldata as returndata (i.e. return it at the end of the
execution frame). Otherwise, it would be possible that returndata points to the memory slice of the active frame and
allow editing the <code>returndata</code>. It means that in the examples above, C could not return a slice of its calldata without
memory copying.</p>
<p>Some of these memory optimizations can be seen utilized in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/EfficientCall.sol">EfficientCall</a>
library that allows to perform a call while reusing the slice of calldata that the frame already has, without memory
copying.</p>
<h3 id="returndata--precompiles"><a class="header" href="#returndata--precompiles">Returndata &amp; precompiles</a></h3>
<p>Some of the operations which are opcodes on Ethereum, have become calls to some of the system contracts. The most
notable examples are <code>Keccak256</code>, <code>SystemContext</code>, etc. Note, that, if done naively, the following lines of code would
work differently on ZKsync and Ethereum:</p>
<pre><code class="language-solidity">pop(call(...))
keccak(...)
returndatacopy(...)
</code></pre>
<p>Since the call to keccak precompile would modify the <code>returndata</code>. To avoid this, our compiler does not override the
latest <code>returndata</code> pointer after calls to such opcode-like precompiles.</p>
<h2 id="zkevm-specific-opcodes"><a class="header" href="#zkevm-specific-opcodes">zkEVM specific opcodes</a></h2>
<p>While some Ethereum opcodes are not supported out of the box, some of the new opcodes were added to facilitate the
development of the system contracts.</p>
<p>Note, that this lists does not aim to be specific about the internals, but rather explain methods in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractHelper.sol">SystemContractHelper.sol</a></p>
<h3 id="only-for-kernel-space"><a class="header" href="#only-for-kernel-space"><strong>Only for kernel space</strong></a></h3>
<p>These opcodes are allowed only for contracts in kernel space (i.e. system contracts). If executed in other places they
result in <code>revert(0,0)</code>.</p>
<ul>
<li><code>mimic_call</code>. The same as a normal <code>call</code>, but it can alter the <code>msg.sender</code> field of the transaction.</li>
<li><code>to_l1</code>. Sends a system L2→L1 log to Ethereum. The structure of this log can be seen
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L47">here</a>.</li>
<li><code>event</code>. Emits an L2 log to ZKsync. Note, that L2 logs are not equivalent to Ethereum events. Each L2 log can emit 64
bytes of data (the actual size is 88 bytes, because it includes the emitter address, etc). A single Ethereum event is
represented with multiple <code>event</code> logs constitute. This opcode is only used by <code>EventWriter</code> system contract.</li>
<li><code>precompile_call</code>. This is an opcode that accepts two parameters: the uint256 representing the packed parameters for
it as well as the ergs to burn. Besides the price for the precompile call itself, it burns the provided ergs and
executes the precompile. The action that it does depend on <code>this</code> during execution:
<ul>
<li>If it is the address of the <code>ecrecover</code> system contract, it performs the ecrecover operation</li>
<li>If it is the address of the <code>sha256</code>/<code>keccak256</code> system contracts, it performs the corresponding hashing operation.</li>
<li>It does nothing (i.e. just burns ergs) otherwise. It can be used to burn ergs needed for L2→L1 communication or
publication of bytecodes onchain.</li>
</ul>
</li>
<li><code>setValueForNextFarCall</code> sets <code>msg.value</code> for the next <code>call</code>/<code>mimic_call</code>. Note, that it does not mean that the value
will be really transferred. It just sets the corresponding <code>msg.value</code> context variable. The transferring of ETH
should be done via other means by the system contract that uses this parameter. Note, that this method has no effect
on <code>delegatecall</code> , since <code>delegatecall</code> inherits the <code>msg.value</code> of the previous frame.</li>
<li><code>increment_tx_counter</code> increments the counter of the transactions within the VM. The transaction counter used mostly
for the VM’s internal tracking of events. Used only in bootloader after the end of each transaction.</li>
</ul>
<p>Note, that currently we do not have access to the <code>tx_counter</code> within VM (i.e. for now it is possible to increment it
and it will be automatically used for logs such as <code>event</code>s as well as system logs produced by <code>to_l1</code>, but we can not
read it). We need to read it to publish the <em>user</em> L2→L1 logs, so <code>increment_tx_counter</code> is always accompanied by the
corresponding call to the
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#systemcontext">SystemContext</a>
contract.</p>
<p>More on the difference between system and user logs can be read
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>. -
<code>set_pubdata_price</code> sets the price (in gas) for publishing a single byte of pubdata.</p>
<h3 id="generally-accessible"><a class="header" href="#generally-accessible"><strong>Generally accessible</strong></a></h3>
<p>Here are opcodes that can be generally accessed by any contract. Note that while the VM allows to access these methods,
it does not mean that this is easy: the compiler might not have convenient support for some use-cases yet.</p>
<ul>
<li><code>near_call</code>. It is basically a “framed” jump to some location of the code of your contract. The difference between the
<code>near_call</code> and ordinary jump are:
<ol>
<li>It is possible to provide an ergsLimit for it. Note, that unlike “<code>far_call</code>”s (i.e. calls between contracts) the
63/64 rule does not apply to them.</li>
<li>If the near call frame panics, all state changes made by it are reversed. Please note, that the memory changes will
<strong>not</strong> be reverted.</li>
</ol>
</li>
<li><code>getMeta</code>. Returns an u256 packed value of
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/libraries/SystemContractHelper.sol#L42">ZkSyncMeta</a>
struct. Note that this is not tight packing. The struct is formed by the
<a href="https://github.com/matter-labs/era-zkevm_opcode_defs/blob/c7ab62f4c60b27dfc690c3ab3efb5fff1ded1a25/src/definitions/abi/meta.rs#L4">following rust code</a>.</li>
<li><code>getCodeAddress</code> — receives the address of the executed code. This is different from <code>this</code> , since in case of
delegatecalls <code>this</code> is preserved, but <code>codeAddress</code> is not.</li>
</ul>
<h3 id="flags-for-calls"><a class="header" href="#flags-for-calls">Flags for calls</a></h3>
<p>Besides the calldata, it is also possible to provide additional information to the callee when doing <code>call</code> ,
<code>mimic_call</code>, <code>delegate_call</code>. The called contract will receive the following information in its first 12 registers at
the start of execution:</p>
<ul>
<li><em>r1</em> — the pointer to the calldata.</li>
<li><em>r2</em> — the pointer with flags of the call. This is a mask, where each bit is set only if certain flags have been set
to the call. Currently, two flags are supported: 0-th bit: <code>isConstructor</code> flag. This flag can only be set by system
contracts and denotes whether the account should execute its constructor logic. Note, unlike Ethereum, there is no
separation on constructor &amp; deployment bytecode. More on that can be read
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#contractdeployer--immutablesimulator">here</a>.
1-st bit: <code>isSystem</code> flag. Whether the call intends a system contracts’ function. While most of the system contracts’
functions are relatively harmless, accessing some with calldata only may break the invariants of Ethereum, e.g. if the
system contract uses <code>mimic_call</code>: no one expects that by calling a contract some operations may be done out of the
name of the caller. This flag can be only set if the callee is in kernel space.</li>
<li>The rest r3..r12 registers are non-empty only if the <code>isSystem</code> flag is set. There may be arbitrary values passed,
which we call <code>extraAbiParams</code>.</li>
</ul>
<p>The compiler implementation is that these flags are remembered by the contract and can be accessed later during
execution via special
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/overview.md">simulations</a>.</p>
<p>If the caller provides inappropriate flags (i.e. tries to set <code>isSystem</code> flag when callee is not in the kernel space),
the flags are ignored.</p>
<h3 id="onlysystemcall-modifier"><a class="header" href="#onlysystemcall-modifier"><code>onlySystemCall</code> modifier</a></h3>
<p>Some of the system contracts can act on behalf of the user or have a very important impact on the behavior of the
account. That’s why we wanted to make it clear that users can not invoke potentially dangerous operations by doing a
simple EVM-like <code>call</code>. Whenever a user wants to invoke some of the operations which we considered dangerous, they must
provide “<code>isSystem</code>” flag with them.</p>
<p>The <code>onlySystemCall</code> flag checks that the call was either done with the “isSystemCall” flag provided or the call is done
by another system contract (since Matter Labs is fully aware of system contracts).</p>
<h3 id="simulations-via-our-compiler"><a class="header" href="#simulations-via-our-compiler">Simulations via our compiler</a></h3>
<p>In the future, we plan to introduce our “extended” version of Solidity with more supported opcodes than the original
one. However, right now it was beyond the capacity of the team to do, so in order to represent accessing ZKsync-specific
opcodes, we use <code>call</code> opcode with certain constant parameters that will be automatically replaced by the compiler with
zkEVM native opcode.</p>
<p>Example:</p>
<pre><code class="language-solidity">function getCodeAddress() internal view returns (address addr) {
  address callAddr = CODE_ADDRESS_CALL_ADDRESS;
  assembly {
    addr := staticcall(0, callAddr, 0, 0xFFFF, 0, 0)
  }
}

</code></pre>
<p>In the example above, the compiler will detect that the static call is done to the constant <code>CODE_ADDRESS_CALL_ADDRESS</code>
and so it will replace it with the opcode for getting the code address of the current execution.</p>
<p>Full list of opcode simulations can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/call.md">here</a>.</p>
<p>We also use
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/How%20compiler%20works/instructions/extensions/verbatim.md">verbatim-like</a>
statements to access ZKsync-specific opcodes in the bootloader.</p>
<p>All the usages of the simulations in our Solidity code are implemented in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractHelper.sol">SystemContractHelper</a>
library and the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/code/system-contracts/contracts/libraries/SystemContractsCaller.sol">SystemContractsCaller</a>
library.</p>
<p><strong>Simulating</strong> <code>near_call</code> <strong>(in Yul only)</strong></p>
<p>In order to use <code>near_call</code> i.e. to call a local function, while providing a limit of ergs (gas) that this function can
use, the following syntax is used:</p>
<p>The function should contain <code>ZKSYNC_NEAR_CALL</code> string in its name and accept at least 1 input parameter. The first input
parameter is the packed ABI of the <code>near_call</code>. Currently, it is equal to the number of ergs to be passed with the
<code>near_call</code>.</p>
<p>Whenever a <code>near_call</code> panics, the <code>ZKSYNC_CATCH_NEAR_CALL</code> function is called.</p>
<p><em>Important note:</em> the compiler behaves in a way that if there is a <code>revert</code> in the bootloader, the
<code>ZKSYNC_CATCH_NEAR_CALL</code> is not called and the parent frame is reverted as well. The only way to revert only the
<code>near_call</code> frame is to trigger VM’s <em>panic</em> (it can be triggered with either invalid opcode or out of gas error).</p>
<p><em>Important note 2:</em> The 63/64 rule does not apply to <code>near_call</code>. Also, if 0 gas is provided to the near call, then
actually all of the available gas will go to it.</p>
<h3 id="notes-on-security"><a class="header" href="#notes-on-security">Notes on security</a></h3>
<p>To prevent unintended substitution, the compiler requires <code>--system-mode</code> flag to be passed during compilation for the
above substitutions to work.</p>
<h2 id="bytecode-hashes"><a class="header" href="#bytecode-hashes">Bytecode hashes</a></h2>
<p>On ZKsync the bytecode hashes are stored in the following format:</p>
<ul>
<li>The 0th byte denotes the version of the format. Currently the only version that is used is “1”.</li>
<li>The 1st byte is <code>0</code> for deployed contracts’ code and <code>1</code> for the contract code
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#constructing-vs-non-constructing-code-hash">that is being constructed</a>.</li>
<li>The 2nd and 3rd bytes denote the length of the contract in 32-byte words as big-endian 2-byte number.</li>
<li>The next 28 bytes are the last 28 bytes of the sha256 hash of the contract’s bytecode.</li>
</ul>
<p>The bytes are ordered in little-endian order (i.e. the same way as for <code>bytes32</code> ).</p>
<h3 id="bytecode-validity"><a class="header" href="#bytecode-validity">Bytecode validity</a></h3>
<p>A bytecode is valid if it:</p>
<ul>
<li>Has its length in bytes divisible by 32 (i.e. consists of an integer number of 32-byte words).</li>
<li>Has a length of less than 2^16 words (i.e. its length in words fits into 2 bytes).</li>
<li>Has an odd length in words (i.e. the 3rd byte is an odd number).</li>
</ul>
<p>Note, that it does not have to consist of only correct opcodes. In case the VM encounters an invalid opcode, it will
simply revert (similar to how EVM would treat them).</p>
<p>A call to a contract with invalid bytecode can not be proven. That is why it is <strong>essential</strong> that no contract with
invalid bytecode is ever deployed on ZKsync. It is the job of the
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/specs/zk_evm/system_contracts.md#knowncodestorage">KnownCodesStorage</a>
to ensure that all allowed bytecodes in the system are valid.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intuition-guide-to-zk-in-zkevm"><a class="header" href="#intuition-guide-to-zk-in-zkevm">Intuition guide to ZK in zkEVM</a></h1>
<p><strong>WARNING</strong>: This guide simplifies the complex details of how we use ZK in our systems, just to give you a better
understanding. We’re leaving out a lot of details to keep things brief.</p>
<h2 id="what-is-the-zero-knowledge"><a class="header" href="#what-is-the-zero-knowledge">What is the ‘Zero Knowledge’</a></h2>
<p>In our case, the prover takes public input and witness (which is huge - you’ll see below), and produces a proof, but the
verifier takes (public input, proof) only, without witness. This means that the huge witness doesn’t have to be
submitted to L1. This property can be used for many things, like privacy, but here we use it to implement an efficient
rollup that publishes the least required amount of data to L1.</p>
<h2 id="basic-overview"><a class="header" href="#basic-overview">Basic overview</a></h2>
<p>Let’s break down the basic steps involved when a transaction is made within our ZK system:</p>
<ul>
<li><strong>Execute transaction in State Keeper &amp; Seal the block:</strong> This part has been discussed in other articles.</li>
<li><strong>Generate witness:</strong> What’s that? Let’s find out below!</li>
<li><strong>Generate proof:</strong> This is where some fancy math and computing power comes in.</li>
<li><strong>Verify proof on L1:</strong> This means checking that the fancy math was done right on the Ethereum network (referred to as
L1).</li>
</ul>
<h2 id="what-it-means-to-generate-a-witness"><a class="header" href="#what-it-means-to-generate-a-witness">What It Means to Generate a Witness</a></h2>
<p>When our State Keeper processes a transaction, it carries out a bunch of operations and assumes certain conditions
without openly stating them. However, when it comes to ZK, we need to show clear evidence that these conditions hold.</p>
<p>Take this simple example where we have a command that retrieves some data from storage and assigns it to a variable.</p>
<p><code>a := SLOAD(0x100)</code></p>
<p>In normal circumstances, the system would just read the data from storage and assign it. But in ZK, we need to provide
evidence of what specific data was fetched and that it was indeed present in the storage beforehand.</p>
<p>From the ZK point of view, this looks like:</p>
<pre><code>circuit inputs:
* current_state_hash = 0x1234;
* read_value: 44
* merkle_path proving that (0x100, 44) exists in tree with storage hash 0x1234
circuit outputs:
* new state hash (that includes the leaf saying that variable 'a' has value 44)
</code></pre>
<p><strong>Note</strong>: In reality, we also use multiple Queues with hashes (together with merkle trees), to track all the memory &amp;
storage accesses.</p>
<p>So, in our example, what seems like a simple action actually requires us to create a bunch of hashes and merkle paths.
This is precisely what the Witness Generator does. It processes the transactions, one operation at a time, and generates
the necessary data that will be used later in circuits.</p>
<h3 id="a-closer-look"><a class="header" href="#a-closer-look">A Closer Look</a></h3>
<p>Now let’s dive into a specific example <a href="https://github.com/matter-labs/era-zkevm_test_harness/tree/main/src/witness/individual_circuits/decommit_code.rs#L24">witness_example</a>:</p>
<pre><pre class="playground"><code class="language-rust="><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn compute_decommitter_circuit_snapshots&lt;
    E: Engine,
    R: CircuitArithmeticRoundFunction&lt;E, 2, 3&gt;,
&gt;(
...
) -&gt; (
    Vec&lt;CodeDecommitterCircuitInstanceWitness&lt;E&gt;&gt;,
    CodeDecommitmentsDeduplicatorInstanceWitness&lt;E&gt;,
)
<span class="boring">}</span></code></pre></pre>
<p>In this code snippet, we’re looking at a function named <code>compute_decommitter_circuit_snapshots</code>. It uses some technical
terms and concepts that may seem daunting, but let’s break them down:</p>
<p><strong>Engine:</strong> This is a trait that specifically handles complex mathematical curves, called Elliptic curves. It’s like
your uint64 on steroids!</p>
<p><strong>CircuitArithmeticRoundFunction:</strong> This is a special kind of hashing function that’s more suited for the circuits we
are using than the regular ones like keccak. In our case, we use Franklin and Rescue from <a href="https://github.com/matter-labs/franklin-crypto">franklin repo</a>.</p>
<p>The function returns Witness classes, that contain queues such as <code>FixedWidthEncodingSpongeLikeQueueWitness</code> which hold
the hashes we mentioned earlier. This is similar merkle paths that we discussed above.</p>
<h3 id="where-is-the-code"><a class="header" href="#where-is-the-code">Where is the Code</a></h3>
<p>The job of generating witnesses, which we discussed earlier, is handled by the witness generator. Initially, this was
located in a module zksync core witness. However, for the new proof system, the team began to shift this function to a
new location called <a href="https://github.com/matter-labs/zksync-era/blob/main/prover/crates/bin/witness_generator/src/main.rs">separate witness binary</a>.</p>
<p>Inside this new location, after the necessary data is fetched from storage, the witness generator calls another piece of
code from <a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/fb47657ae3b6ff6e4bb5199964d3d37212978200/src/external_calls.rs#L579">zkevm_test_harness witness</a> named <code>run_with_fixed_params</code>. This code is responsible for creating the witnesses
themselves (which can get really HUGE).</p>
<h2 id="generating-the-proof"><a class="header" href="#generating-the-proof">Generating the Proof</a></h2>
<p>Once we have the witness data lined up, it’s time to crunch the numbers and create the proofs.</p>
<p>The main goal of this step is to take an operation (for example, a calculation called <code>ecrecover</code>) and break it down
into smaller pieces. Then, we represent this information as a special mathematical expression called a polynomial.</p>
<p>To construct these polynomials, we use something called a <code>ConstraintSystem</code>. The specific type that we use is called
zkSNARK, and our custom version of it is named bellman. You can find our code for this in the <a href="https://github.com/matter-labs/bellman">bellman repo</a>. Additionally,
we have an optimized version that’s designed to run faster on certain types of hardware (using CUDA technology), which you
can find in the <a href="https://github.com/matter-labs/era-bellman-cuda">bellman cuda repo</a>.</p>
<p>An <a href="https://github.com/matter-labs/era-sync_vm/blob/v1.3.2/src/glue/ecrecover_circuit/mod.rs#L157">example ecrecover circuit</a> might give you a clearer picture of what this looks like in practice.</p>
<p>The proof itself is generated by evaluating this polynomial expression at many different points. Because this involves
heavy calculations, we use GPUs to speed things up.</p>
<h3 id="where-is-the-code-1"><a class="header" href="#where-is-the-code-1">Where is the Code</a></h3>
<p>The main code that utilizes the GPUs to create proofs is located in a repository named <a href="https://github.com/matter-labs/era-heavy-ops-service">heavy_ops_service repo</a>. This code
combines elements from the <a href="https://github.com/matter-labs/era-bellman-cuda">bellman cuda repo</a> that we mentioned earlier, along with a huge amount of data generated by the
witness, to produce the final proofs.</p>
<h2 id="what-does-verify-proof-on-l1-mean"><a class="header" href="#what-does-verify-proof-on-l1-mean">What Does “Verify Proof on L1” Mean</a></h2>
<p>Finally, we reach the stage where we have to verify the proof on L1. But what does that really mean?</p>
<p>We need to ensure that four specific values match:</p>
<ul>
<li><strong>C</strong>: This is a value that represents our circuits, also known as verification keys. It’s like a fingerprint of the
circuit code and is hard-coded into the contract. Whenever the circuit changes, this value changes too.</li>
<li><strong>In</strong>: This represents the root hash before the transaction block.</li>
<li><strong>Out</strong>: This represents the root hash after the transaction block.</li>
<li><strong>P</strong>: This is the proof provided by the prover.</li>
</ul>
<p>The logic behind this is that there can only be a matching proof ‘P’ if <code>C(In) == Out</code>. In simple terms, it means that
the proof ‘P’ will only make sense if the values before and after the transaction block are consistent according to the
circuit represented by ‘C’.</p>
<p>If you’re eager to dive into the nitty-gritty, you can find the code in the <a href="https://github.com/matter-labs/era-contracts/blob/main/l1-contracts/contracts/zksync/Verifier.sol">verifier</a> repository. Also, if you’re
interested in learning even more, you can look up KZG commitments.</p>
<h2 id="a-heads-up-about-code-versions"><a class="header" href="#a-heads-up-about-code-versions">A Heads-Up About Code Versions</a></h2>
<p>Please be aware that there are multiple versions of the proving systems, such as v1.3.1, v1.3.2, and so on. When you’re
looking through the code, make sure you’re checking the version that’s relevant to what you’re working on. At the time
this guide was written, the latest version was 1.3.4, but there was also ongoing development on a new proof system in
version 1.4.0.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="proof-system-deeper-overview"><a class="header" href="#proof-system-deeper-overview">Proof System Deeper Overview</a></h1>
<p>The purpose of this section is to explain our new proof system from an engineering standpoint. We will examine the code
examples and how the libraries communicate.</p>
<p>Let’s begin by discussing our constraint system. In the previous prover, we utilized the Bellman repository. However, in
the new prover, the constraint system is implemented in Boojum.</p>
<h2 id="constraint-system"><a class="header" href="#constraint-system">Constraint system</a></h2>
<p>If you look at boojum repo (src/cs/traits/cs.rs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ConstraintSystem&lt;F: SmallField&gt;: Send + Sync {

    ...
    fn alloc_variable() -&gt; Variable;
    fn alloc_witness_without_value(&amp;mut self) -&gt; Witness;
    fn place_gate&lt;G: Gate&lt;F&gt;&gt;(&amp;mut self, gate: &amp;G, row: usize);
    ...
}
<span class="boring">}</span></code></pre></pre>
<p>We have three main components: <code>Variable</code>, <code>Witness</code>, and <code>Gate</code>.</p>
<p>To understand the constraint system, imagine it as a list of “placeholders” called Variables. We define rules, referred
to as “gates” for these Variables. The Witness represents a specific assignment of values to these Variables, ensuring
that the rules still hold true.</p>
<p>Conceptually, this is similar to how we implement functions. Consider the following example:</p>
<pre><code>fn fun(x) {
  y = x + A;
  z = y * B;
  w = if y { z } else { y }
}
</code></pre>
<p>In this code snippet, <code>A</code>, <code>B</code>, <code>y</code>, <code>z</code>, and <code>w</code> are Variables (with <code>A</code> and <code>B</code> being constants). We establish rules,
or gates, specifying that the Variable <code>z</code> must equal <code>y</code> multiplied by the Variable <code>B</code>.</p>
<p>Example Witness assignment would be:</p>
<pre><code> x = 1; A = 3; y = 3; B = 0; z = 0; w = 3;
</code></pre>
<p>Gates can become more complex. For instance, the <code>w</code> case demonstrates a “selection” gate, which chooses one of two
options depending on a condition.</p>
<p>Now, let’s delve into this gate for a more detailed examination:</p>
<h3 id="selection-gate"><a class="header" href="#selection-gate">Selection gate</a></h3>
<p>The code is in boojum/src/cs/gates/selection_gate.rs</p>
<p>Let’s delve deeper into the concept. Our goal is to create a gate that implements the logic
<code>result = if selector == true a else b;</code>. To accomplish this, we will require four variables.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SelectionGate {
    pub a: Variable,
    pub b: Variable,
    pub selector: Variable,
    pub result: Variable,
}
<span class="boring">}</span></code></pre></pre>
<p>Internally the <code>Variable</code> object is <code>pub struct Variable(pub(crate) u64);</code> - so it is an index to the position within
the constraint system object.</p>
<p>And now let’s see how we can add this gate into the system.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select&lt;F: SmallField, CS: ConstraintSystem&lt;F&gt;&gt;(
    cs: &amp;mut CS,
    a: Variable,
    b: Variable,
    selector: Variable,
) -&gt; Variable {
  //  First, let's allocate the output variable:
  let output_variable = cs.alloc_variable_without_value();
  ...
}
<span class="boring">}</span></code></pre></pre>
<p>And then there is a block of code for witness evaluation (let’s skip it for now), and the final block that adds the gate
to the constraint system <code>cs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    if &lt;CS::Config as CSConfig&gt;::SetupConfig::KEEP_SETUP {
        let gate = Self {
            a,
            b,
            selector,
            result: output_variable,
        };
        gate.add_to_cs(cs);
    }

    output_variable
<span class="boring">}</span></code></pre></pre>
<p>So to recap - we took 3 ‘Variables’, created the output one, created a <code>SelectionGate</code> object out of them, which we
added to the system (by calling <code>add_to_cs</code>) - and the finally returned the output variable.</p>
<p>But where is the ‘logic’? Where do we actually enforce the constraint?</p>
<p>For this, we have to look at the <code>Evaluator</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: SmallField&gt; Gate&lt;F&gt; for SelectionGate {
    type Evaluator = SelectionGateConstraitEvaluator;

    #[inline]
    fn evaluator(&amp;self) -&gt; Self::Evaluator {
        SelectionGateConstraitEvaluator
    }
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: PrimeField&gt; GateConstraitEvaluator&lt;F&gt; for SelectionGateConstraitEvaluator {
  fn evaluate_once{
    let a = trace_source.get_variable_value(0);
    let b = trace_source.get_variable_value(1);
    let selector = trace_source.get_variable_value(2);
    let result = trace_source.get_variable_value(3);

    // contribution = a * selector
    let mut contribution = a;
    contribution.mul_assign(&amp;selector, ctx);

    // tmp = 1 - selector
    let mut tmp = P::one(ctx);
    tmp.sub_assign(&amp;selector, ctx);

    // contribution += tmp * b
    // So:
    // contribution = a*selector + (1-selector) * b
    P::mul_and_accumulate_into(&amp;mut contribution, &amp;tmp, &amp;b, ctx);

    // contribution = a*selector + (1-selector) * b - result
    contribution.sub_assign(&amp;result, ctx);

    // And if we're successful, the contribution == 0.
    // Because result == a * selector + (1-selector) * b
    destination.push_evaluation_result(contribution, ctx);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This evaluator is actually operating on top of the <code>Field</code> objects, trying to build &amp; evaluate the correct polynomials.
The details of it will be covered in a separate article.</p>
<p>Congratulations, you hopefully understood the code for the first gate. To recap - we created the ‘output’ Variable, and
added the Gate to the CS system. Later when CS system ‘computes’ all the dependencies, it will run the constraint
evaluator, to add the ‘raw’ dependency (which is basically an equation) to the list.</p>
<p>You can look into other files in <code>src/cs/gates</code> to see other examples.</p>
<h2 id="structures"><a class="header" href="#structures">Structures</a></h2>
<p>Now, that we handled the basic variables, let’s see what we can do with more complex structures. Boojum has added a
bunch of derive macros, to make development easier.</p>
<p>Let’s look at the example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Derivative, CSSelectable, CSAllocatable, CSVarLengthEncodable, WitnessHookable)]
pub struct VmLocalState&lt;F: SmallField&gt; {
    pub previous_code_word: UInt256&lt;F&gt;,
    pub registers: [VMRegister&lt;F&gt;; REGISTERS_COUNT],
    pub flags: ArithmeticFlagsPort&lt;F&gt;,
    pub timestamp: UInt32&lt;F&gt;,
    pub memory_page_counter: UInt32&lt;F&gt;,
<span class="boring">}</span></code></pre></pre>
<p>First - all the UInt that you see above, are actually implemented in Boojum:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt32&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
}
impl&lt;F: SmallField&gt; CSAllocatable&lt;F&gt; for UInt32&lt;F&gt; {
    // So the 'witness' type (concrete value) for U32 is u32 - no surprises ;-)
    type Witness = u32;
    ...
}

pub struct UInt256&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 8],
}
<span class="boring">}</span></code></pre></pre>
<h3 id="witnesshookable"><a class="header" href="#witnesshookable">WitnessHookable</a></h3>
<p>In the example above, the Witness type for U32, was u32 - easy. But what should we do, when we have more complex struct
(like VmLocalState)?</p>
<p>This derive will automatically create a new struct named XXWitness (in example above <code>VmLocalStateWitness</code>), that can be
filled with concrete values.</p>
<h3 id="csallocatable"><a class="header" href="#csallocatable">CsAllocatable</a></h3>
<p>Implements CsAllocatable - which allows you to directly ‘allocate’ this struct within constraint system (similarly to
how we were operating on regular ‘Variables’ above).</p>
<h3 id="csselectable"><a class="header" href="#csselectable">CSSelectable</a></h3>
<p>Implements the <code>Selectable</code> trait - that allows this struct to participate in operations like conditionally select (so
it can be used as ‘a’ or ‘b’ in the Select gate example above).</p>
<h3 id="csvarlengthencodable"><a class="header" href="#csvarlengthencodable">CSVarLengthEncodable</a></h3>
<p>Implements CircuitVarLengthEncodable - which allows encoding the struct into a vector of variables (think about it as
serializing to Bytes).</p>
<h3 id="summary-5"><a class="header" href="#summary-5">Summary</a></h3>
<p>Now with the tools above, we can do operations on our constraint system using more complex structures. So we have gates
as ‘complex operators’ and structures as complex object. Now we’re ready to start taking it to the next level: Circuits.</p>
<h2 id="circuits"><a class="header" href="#circuits">Circuits</a></h2>
<p>Circuit’s definitions are spread across 2 separate repositories: <code>zkevm_circuits</code> and <code>zkevm_test_harness</code>.</p>
<p>While we have around 9 different circuits (log_sorter, ram_permutation etc) - in this article we’ll focus only on the
one: MainVM - which is responsible for handling almost all of the VM operations (other circuits are used to handle some
of the precompiles, and operations that happen after VM was run - like preparing pubdata etc).</p>
<p>Looking at zkevm_test_harness, we can see the definition:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub type VMMainCircuit&lt;F, W, R&gt; =
    ZkSyncUniformCircuitInstance&lt;F, VmMainInstanceSynthesisFunction&lt;F, W, R&gt;&gt;;
<span class="boring">}</span></code></pre></pre>
<h3 id="so-what-is-a-circuit"><a class="header" href="#so-what-is-a-circuit">So what is a circuit</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ZkSyncUniformCircuitInstance&lt;F: SmallField, S: ZkSyncUniformSynthesisFunction&lt;F&gt;&gt; {
    // Assignment of values to all the Variables.
    pub witness: AtomicCell&lt;Option&lt;S::Witness&gt;&gt;,

    // Configuration - that is circuit specific, in case of MainVM - the configuration
    // is simply the amount of opcodes that we put within 1 circuit.
    pub config: std::sync::Arc&lt;S::Config&gt;,

    // Circuit 'friendly' hash function.
    pub round_function: std::sync::Arc&lt;S::RoundFunction&gt;,

    // Inputs to the circuits.
    pub expected_public_input: Option&lt;[F; INPUT_OUTPUT_COMMITMENT_LENGTH]&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>Notice - that circuit doesn’t have any ‘explicit’ outputs.</p>
<p>Where ZkSyncUniformCircuitInstance is a proxy, so let’s look deeper, into the main function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl VmMainInstanceSynthesisFunction {
    fn synthesize_into_cs_inner&lt;CS: ConstraintSystem&lt;F&gt;&gt;(
        cs: &amp;mut CS,
        witness: Self::Witness,
        round_function: &amp;Self::RoundFunction,
        config: Self::Config,
    ) -&gt; [Num&lt;F&gt;; INPUT_OUTPUT_COMMITMENT_LENGTH] {
        main_vm_entry_point(cs, witness, round_function, config)
    }
}

<span class="boring">}</span></code></pre></pre>
<p>This is the main logic, that takes the witness (remember - Witness is a concrete assignment of values to Variables) -
and returns the public input.</p>
<p>If we look deeper into ‘main_vm_entry_point’ (which is already in zkevm_circuits repo), we can see:</p>
<pre><pre class="playground"><code class="language-rust">pub fn main_vm_entry_point(
    cs: &amp;mut CS,
    witness: VmCircuitWitness&lt;F, W&gt;,
    round_function: &amp;R,
    limit: usize,
) -&gt; [Num&lt;F&gt;; INPUT_OUTPUT_COMMITMENT_LENGTH]</code></pre></pre>
<p>And in this function we do following operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Prepare current 'state'
    //
    // First - unpack the witness
    let VmCircuitWitness {
        closed_form_input,
        witness_oracle,
    } = witness;

    // And add it to the constraint system
    let mut structured_input =
        VmCircuitInputOutput::alloc_ignoring_outputs(cs, closed_form_input.clone());

    let mut state =
        VmLocalState::conditionally_select(cs, start_flag, &amp;bootloader_state, &amp;hidden_fsm_input);

    // Notice, that now state is a VmLocalState object -- which contains 'Variables' inside.

    // And now run the cycles
    for _cycle_idx in 0..limit {
        state = vm_cycle(
            cs,
            state,
            &amp;synchronized_oracle,
            &amp;per_block_context,
            round_function,
        );
    }
<span class="boring">}</span></code></pre></pre>
<p>The <code>vm_cycle</code> method is where the magic is - it takes a given opcode, and creates all the necessary gates, temporary
Variables etc inside the Constraint system. This method is around 800 lines long, so I’d encourage you to take a sneak
peek if you’re interested.</p>
<p>Now that we’ve added all the constraints for the ‘limit’ number of opcodes, we have to do some additional housekeeping -
like storing the Queue hashes (for memory, code decommitment etc).</p>
<p>And then we’re ready to prepare the result of this method (input_commitment).</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Prepare compact form (that contains just the hashes of values, rather than full values).
    let compact_form =
        ClosedFormInputCompactForm::from_full_form(cs, &amp;structured_input, round_function);

    // And serialize it.
    let input_commitment: [_; INPUT_OUTPUT_COMMITMENT_LENGTH] =
        commit_variable_length_encodable_item(cs, &amp;compact_form, round_function);
    input_commitment
<span class="boring">}</span></code></pre></pre>
<h2 id="and-now-putting-it-all-together"><a class="header" href="#and-now-putting-it-all-together">And now putting it all together</a></h2>
<p>Now let’s look at the zkevm_test_harness repo, ‘/src/external_calls.rs’ run method. This is used in many tests, and
tries to execute the whole flow end to end.</p>
<p>And while the signature is quite scary - let’s walk through this together:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn run&lt;
    F: SmallField,
    R: BuildableCircuitRoundFunction&lt;F, 8, 12, 4&gt; + AlgebraicRoundFunction&lt;F, 8, 12, 4&gt; + serde::Serialize + serde::de::DeserializeOwned,
    H: RecursiveTreeHasher&lt;F, Num&lt;F&gt;&gt;,
    EXT: FieldExtension&lt;2, BaseField = F&gt;,
    S: Storage
&gt;(
caller: Address, // for real block must be zero
entry_point_address: Address, // for real block must be the bootloader
entry_point_code: Vec&lt;[u8; 32]&gt;, // for read lobkc must be a bootloader code
initial_heap_content: Vec&lt;u8&gt;, // bootloader starts with non-deterministic heap
    zk_porter_is_available: bool,
    default_aa_code_hash: U256,
used_bytecodes: std::collections::HashMap&lt;U256, Vec&lt;[u8; 32]&gt;&gt;, // auxiliary information to avoid passing a full set of all used codes
ram_verification_queries: Vec&lt;(u32, U256)&gt;, // we may need to check that after the bootloader's memory is filled
    cycle_limit: usize,
round_function: R, // used for all queues implementation
    geometry: GeometryConfig,
    storage: S,
    tree: &amp;mut impl BinarySparseStorageTree&lt;256, 32, 32, 8, 32, Blake2s256, ZkSyncStorageLeaf&gt;,
) -&gt; (
    BlockBasicCircuits&lt;F, R&gt;,
    BlockBasicCircuitsPublicInputs&lt;F&gt;,
    BlockBasicCircuitsPublicCompactFormsWitnesses&lt;F&gt;,
    SchedulerCircuitInstanceWitness&lt;F, H, EXT&gt;,
    BlockAuxilaryOutputWitness&lt;F&gt;,
)
    where [(); &lt;crate::zkevm_circuits::base_structures::log_query::LogQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::base_structures::memory_query::MemoryQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::base_structures::decommit_query::DecommitQuery&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::boojum::gadgets::u256::UInt256&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::boojum::gadgets::u256::UInt256&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN + 1]:,
    [(); &lt;crate::zkevm_circuits::base_structures::vm_state::saved_context::ExecutionContextRecord&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,
    [(); &lt;crate::zkevm_circuits::storage_validity_by_grand_product::TimestampedStorageLogRecord&lt;F&gt; as CSAllocatableExt&lt;F&gt;&gt;::INTERNAL_STRUCT_LEN]:,

<span class="boring">}</span></code></pre></pre>
<p>The first section, is adding some decommitments (explain later).</p>
<p>Then we create a vm:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let mut out_of_circuit_vm =
        create_out_of_circuit_vm(&amp;mut tools, &amp;block_properties, caller, entry_point_address);
<span class="boring">}</span></code></pre></pre>
<p>And we’ll run it over all the operands:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    for _cycle in 0..cycle_limit {

        out_of_circuit_vm
            .cycle(&amp;mut tracer)
            .expect("cycle should finish successfully");
    }
<span class="boring">}</span></code></pre></pre>
<p>While doing it, we collect ‘snapshots’ - the detailed information of the state of the system between each operand.</p>
<p>Then we create a <code>Vec&lt;VmInstanceWitness&gt;</code> - let’s see what’s inside:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct VmInstanceWitness&lt;F: SmallField, O: WitnessOracle&lt;F&gt;&gt; {
    // we need everything to start a circuit from this point of time

    // initial state (state of registers etc)
    pub initial_state: VmLocalState,
    pub witness_oracle: O,
    pub auxilary_initial_parameters: VmInCircuitAuxilaryParameters&lt;F&gt;,
    pub cycles_range: std::ops::Range&lt;u32&gt;,

    // final state for test purposes
    pub final_state: VmLocalState,
    pub auxilary_final_parameters: VmInCircuitAuxilaryParameters&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>With this, let’s finally start creating circuits (via <code>create_leaf_level_circuits_and_scheduler_witness</code>)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span>
<span class="boring">fn main() {
</span>    for (instance_idx, vm_instance) in vm_instances_witness.into_iter().enumerate() {
         let instance = VMMainCircuit {
            witness: AtomicCell::new(Some(circuit_input)),
            config: Arc::new(geometry.cycles_per_vm_snapshot as usize),
            round_function: round_function.clone(),
            expected_public_input: Some(proof_system_input),
        };

        main_vm_circuits.push(instance);
    }
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prover-and-keys"><a class="header" href="#prover-and-keys">Prover and keys</a></h1>
<p>You might have come across terms like “prover”, “keys”, and phrases like “regenerating the verification key” or “why is
the key 8GB in size?” or even “the proof failed because the verification key hash was different.” It can all seem a bit
complex, but don’t worry, we’re here to make it simple.</p>
<p>In this article, we’re going to break down the different types of keys, explain their purposes, and show you how they
are created.</p>
<p>Our main focus will be on the boojum, a new proof system. But if you’re familiar with the old proof system, the
principles we’ll discuss apply there as well.</p>
<h2 id="circuits-1"><a class="header" href="#circuits-1">Circuits</a></h2>
<p><img src="https://user-images.githubusercontent.com/128217157/275817097-0a543476-52e5-437b-a7d3-10603d5833fa.png" alt="circuits" /></p>
<p>We offer 13 distinct types of <strong>‘base’ circuits</strong>, including Vm, Decommitter, and others, which you can view in the
<a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/circuit_definitions/base_layer/mod.rs#L77">full list here</a>. In addition, there are 15 <strong>‘recursive’ circuits</strong>. Out of these, 13 are ‘leaves,’
each corresponding to a basic type, while one is a ‘node,’ and another is a ‘scheduler’ that oversees all others. You
can find more details in the <a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/circuit_definitions/recursion_layer/mod.rs#L29">full list here</a>.</p>
<p>In our new proof system, there’s also a final steps known as the compressor and <strong>snark wrapper</strong>, representing an
additional type of circuit.</p>
<p>It’s essential to note that each circuit type requires its unique set of keys.</p>
<p>Also, the base circuits, leaves, node and scheduler are STARK based with FRI commitments, while the snark wrapper is
SNARK based with KZG commitment. This results in slightly different contents of the keys, but their role stays the same.
More info about commitment schemes can be found <a href="https://en.wikipedia.org/wiki/Commitment_scheme">here</a>.</p>
<h2 id="keys-1"><a class="header" href="#keys-1">Keys</a></h2>
<h3 id="setup-keys-big-700mb-each"><a class="header" href="#setup-keys-big-700mb-each">Setup keys (big, &gt;700MB each)</a></h3>
<p>The following <a href="https://github.com/matter-labs/zksync-era/blob/main/prover/setup-data-gpu-keys.json">link</a> provides the
GCS buckets containing the latest setup keys.</p>
<p>The primary key for a given circuit is called <code>setup key</code>. These keys can be substantial in size - approximately 700MB
for our circuits. Due to their size, we don’t store them directly on GitHub; instead, they need to be generated.</p>
<p>If you’re wondering what these setup keys contain, think of them as the ‘source code of the circuit.’</p>
<p>This implies that any modifications to a circuit necessitate the regeneration of the setup keys to align with the
changes made.</p>
<h3 id="verification-key-small-8kb"><a class="header" href="#verification-key-small-8kb">Verification key (small, 8kb)</a></h3>
<p>To generate the proof, we need the setup key. However, to verify the proof, a much smaller key, known as the
<code>verification key</code>, is required.</p>
<p>These verification keys are available on GitHub, and you can view them <a href="https://github.com/matter-labs/zksync-era/tree/main/prover/data/keys">here</a>. Each verification
key is stored in a separate file. They are named in the format <code>verification_X_Y_key.json</code>, for example,
<code>verification_basic_4_key.json</code>.</p>
<p>Comparing these files with the list of circuits mentioned earlier, you’ll notice there are 13 files named
<code>verification_basic_Y</code>, 15 files for leaf, one each for node and scheduler, and an additional one for wrapper.</p>
<p>In simpler terms, each verification key contains multiple ‘hashes’ or commitments, derived from different sections of
the setup key. These hashes enable the proof verification process.</p>
<h3 id="verification-key-hash-very-small-32-bytes"><a class="header" href="#verification-key-hash-very-small-32-bytes">Verification key hash (very small, 32 bytes)</a></h3>
<p>The hash of the verification key serves a quick reference to ensure that both parties involved are using the same keys.
For instance:</p>
<ul>
<li>Our state keeper uses this hash to confirm that the L1 contract possesses the correct key.</li>
<li>The witness generator refers to this hash to determine which jobs it should take on.</li>
</ul>
<p>Typically, we embed these hashes directly into an environment variable for easy access. You can find an example of this
<a href="https://github.com/matter-labs/zksync-era/blob/6d18061df4a18803d3c6377305ef711ce60317e1/etc/env/base/contracts.toml#L61">here for SNARK_WRAPPER_VK_HASH</a>.</p>
<h2 id="crs-files-setup_226key-8gb-files"><a class="header" href="#crs-files-setup_226key-8gb-files">CRS files (setup_2^26.key, 8GB files)</a></h2>
<p>These keys, also referred to as Common Reference Strings (CRS), are essential for KZG commitments and were a crucial
part of our old proving system.</p>
<p>With the introduction of the new prover, CRS is only utilized in the final step, specifically during the snark_wrapper
phase. However, since the computational requirements in this stage are significantly reduced compared to the past, we
can rely on a smaller CRS file, namely the setup_2^24.key.</p>
<h2 id="advanced"><a class="header" href="#advanced">Advanced</a></h2>
<h3 id="whats-inside-the-key"><a class="header" href="#whats-inside-the-key">What’s inside the key</a></h3>
<h4 id="setup-key"><a class="header" href="#setup-key">Setup key</a></h4>
<p>Setup keys house the <a href="https://github.com/matter-labs/zksync-era/blob/d2ca29bf20b4ec2d9ec9e327b4ba6b281d9793de/prover/vk_setup_data_generator_server_fri/src/lib.rs#L61">ProverSetupData object</a>, which in turn contains the full Merkle tree. This is
part of the reason why setup keys can be quite large in size.</p>
<p>To put it in simpler terms, if we consider the circuits as a massive collection of linear equations, the setup key
essentially contains all the parameters for these equations. Every detail that defines and regulates these equations is
stored within the setup key, making it a comprehensive and crucial component in the proving process.</p>
<h4 id="verification-key"><a class="header" href="#verification-key">Verification key</a></h4>
<p>Verification keys are stored in a more accessible format, as JSON files, making it relatively easy to explore their
contents.</p>
<p>Inside, you’ll find numerous configuration fields related to the circuit. These include the size of the circuit, the
number of columns, the locations of constants, where to insert the public input, and the size of the public input, among
other details.</p>
<p>Additionally, at the end of the file, there’s a Merkle tree hash. In our case, there are actually 16 hashes because our
proving system utilizes a ‘Cap’ Merkle tree. Imagine a Merkle tree with 16 roots instead of just one; this design
ensures that each Merkle path is slightly shorter, improving efficiency.</p>
<h4 id="verification-key-hash"><a class="header" href="#verification-key-hash">Verification key hash</a></h4>
<p>As previously stated, the verification key hash is derived from hash function applied to the data contained in the
verification key. You can view the exact process of how the keccak hash is computed in the
<a href="https://github.com/matter-labs/era-contracts/blob/d85a73a1eeb5557343b7b44c6543aaf391d8b984/l1-contracts/contracts/zksync/Verifier.sol#L267">Verifier.sol</a> file.</p>
<p>For SNARK circuits (like snark_wrapper), we use keccak as hash function. For START based circuits, we use more circuit
friendly hash function (currently Poseidon2).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decentralization"><a class="header" href="#decentralization">Decentralization</a></h1>
<p>To enable support for synchronization over p2p network, the main node needs to have the “consensus” component configured
and enabled as follows:</p>
<h2 id="generating-the-consensus-secrets"><a class="header" href="#generating-the-consensus-secrets">Generating the consensus secrets</a></h2>
<p>Run the following to generate consensus secrets:</p>
<pre><code>docker run --entrypoint /usr/bin/zksync_external_node "matterlabs/external-node:2.0-v25.0.0" generate-secrets
</code></pre>
<p>That will output something like this (but with different keys obviously):</p>
<pre><code>#validator:public:bls12_381:84fe19a96b6443ca7ce...98dec0870f6d8aa95c8164102f0d62e4c47e3566c4e5c32354d
validator_key: validator:secret:bls12_381:1de85683e6decbfcf6c12aa42a5c8bfa98d7ae796dee068ae73dc784a58f5213
# node:public:ed25519:acb7e350cf53e3b4c2042e2c8044734384cee51f58a0fa052fd7e0c9c3f4b20d
node_key: node:secret:ed25519:0effb1d7c335d23606f656ca1ba87566144d5af2984bd7486379d4f83a204ba2
</code></pre>
<p>You then have two different paths depending if your main node is using file-based or env-based configuration.</p>
<h2 id="configuring-consensus"><a class="header" href="#configuring-consensus">Configuring consensus</a></h2>
<h3 id="file-based-configuration"><a class="header" href="#file-based-configuration">File-based configuration</a></h3>
<p>If you are using the recommended file-based configuration then you’ll need to add the following information to your
<code>general.yaml</code> config file (see <a href="guides/advanced/../launch.html#ecosystem-configuration">Ecosystem Configuration</a>):</p>
<pre><code class="language-yaml">consensus:
  server_addr: '0.0.0.0:3054'
  public_addr:
    '???'
    # Address under which the node is accessible to the other nodes.
    # It can be a public domain, like `example.com:3054`, in case the main node is accessible from the internet,
    # or it can be a kubernetes cluster domain, like `server-v2-core.&lt;cluster name&gt;.svc.cluster.local:3054` in
    # case the main node should be only accessible within the cluster.
  debug_page_addr: '0.0.0.0:5000'
  max_payload_size: 3200000
  gossip_dynamic_inbound_limit: 10
  genesis_spec:
    chain_id: ??? # chain id
    protocol_version: 1 # consensus protocol version
    validators:
      - key: validator:public:??? # validator public key of the main node (copy this PUBLIC key from the secrets you generated)
        weight: 1
    leader: validator:public:??? # same as above - main node will be the only validator and the only leader.
    seed_peers:
      - key: 'node:public:ed25519:...' # node public key of the main node (copy this PUBLIC key from the secrets you generated)
        addr: '???' # same as public_addr above
</code></pre>
<p>And the secrets you generated to your <code>secrets.yaml</code> config file:</p>
<pre><code class="language-yaml">consensus:
  validator_key: validator:secret:???
  node_key: node:secret:???
</code></pre>
<h3 id="env-based-configuration"><a class="header" href="#env-based-configuration">Env-based configuration</a></h3>
<p>If you are using the env-based configuration you’ll need to create a <code>consensus_config.yaml</code> file with the following
content:</p>
<pre><code class="language-yaml">server_addr: '0.0.0.0:3054'
public_addr:
  '???'
  # Address under which the node is accessible to the other nodes.
  # It can be a public domain, like `example.com:3054`, in case the main node is accessible from the internet,
  # or it can be a kubernetes cluster domain, like `server-v2-core.&lt;cluster name&gt;.svc.cluster.local:3054` in
  # case the main node should be only accessible within the cluster.
debug_page_addr: '0.0.0.0:5000'
max_payload_size: 3200000
gossip_dynamic_inbound_limit: 10
genesis_spec:
  chain_id: ??? # chain id
  protocol_version: 1 # consensus protocol version
  validators:
    - key: validator:public:??? # validator public key of the main node (copy this PUBLIC key from the secrets you generated)
      weight: 1
  leader: validator:public:??? # same as above - main node will be the only validator and the only leader.
  seed_peers:
    - key: 'node:public:ed25519:...' # node public key of the main node (copy this PUBLIC key from the secrets you generated)
      addr: '???' # same as public_addr above
</code></pre>
<p>And a <code>consensus_secrets.yaml</code> file with the with the secrets you generated previously:</p>
<pre><code class="language-yaml">validator_key: validator:secret:???
node_key: node:secret:???
</code></pre>
<p>Don’t forget to set secure permissions to it:</p>
<pre><code>chmod 600 consensus_secrets.yaml
</code></pre>
<p>Then you’ll need to pass the paths to these files as env vars <code>CONSENSUS_CONFIG_PATH</code> and <code>CONSENSUS_SECRETS_PATH</code>.</p>
<h2 id="running-the-zksync_server"><a class="header" href="#running-the-zksync_server">Running the <code>zksync_server</code></a></h2>
<p>Finally, to enable the consensus component for the main node you just need to append
<code>--components=&lt;whatever components you were running until now&gt;,consensus</code> to the <code>zksync_server</code> command line arguments.</p>
<h2 id="gitops-repo-config"><a class="header" href="#gitops-repo-config">Gitops repo config</a></h2>
<p>If you are using the matterlabs gitops repo to configure the main node, you’ll need to add this information to your
kubernetes config for the core server, <code>server-v2-core.yaml</code> file (see
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/environments/era-stage-proofs/server-v2/server-v2-core.yaml#L23-L35">example</a>):</p>
<pre><code class="language-yaml">spec:
  values:
    args:
      - --components=state_keeper,consensus
    service:
      main:
        ports:
          consensus:
            enabled: true
            port: 3054
</code></pre>
<p>Then again you have two paths depending if the deployment is using file-based or env-based configuration. Although by
default you should be using file-based configuration.</p>
<h3 id="file-based-configuration-1"><a class="header" href="#file-based-configuration-1">File-based configuration</a></h3>
<p>Just like before you’ll add the consensus config information to the <code>general.yaml</code> config file (see
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/environments/era-stage-proofs/server-v2-config/general.yaml#L353-L368">example</a>).</p>
<p>And the secrets you generated to your whatever secrets managing system you are using (see an example
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/clusters/era-stage-proofs/stage2/secrets/server-v2-secrets.yaml">here</a>
using SOPS).</p>
<pre><code class="language-yaml">consensus:
  validator_key: validator:secret:???
  node_key: node:secret:???
</code></pre>
<h3 id="env-based-configuration-1"><a class="header" href="#env-based-configuration-1">Env-based configuration</a></h3>
<p>It is even more complicated because the <code>consensus_config.yaml</code> file is rendered from a helm chart. See the
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/environments/mainnet2/server-v2/server-v2-core.yaml#L37-L92">example</a>,
to see where you have to paste the content of the <code>consensus_config.yaml</code> file.</p>
<p>You also need to add the following sections to your <code>server-v2-core.yaml</code> file:</p>
<pre><code class="language-yaml">spec:
  values:
    persistence:
      consensus-secrets-volume:
        name: consensus-secrets # this is the name of the secret kubernetes object we defined above
        enabled: true
        type: secret
        mountPath: '/etc/consensus_secrets/'
    configMap:
      consensus:
        enabled: true
        data:
          consensus_config.yaml: &lt;here goes the content of the consensus_config.yaml file&gt;
    env:
      - name: CONSENSUS_CONFIG_PATH
        value: /etc/consensus_config.yaml # this is the location rendered by the helm chart, you can't change it
      - name: CONSENSUS_SECRETS_PATH
        value: /etc/consensus_secrets/.consensus_secrets.yaml
</code></pre>
<p>You need to embed the <code>consensus_secrets.yaml</code> file into a kubernetes config (see how to do it
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/environments/mainnet2/zksync-v2-secret/kustomization.yaml#L3-L4">here</a>
and
<a href="https://github.com/matter-labs/gitops-kubernetes/blob/177dcd575c6ab446e70b9a9ced8024766095b516/apps/environments/mainnet2/zksync-v2-secret/consensus_secrets.yaml">here</a>):</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: consensus-secrets
type: Opaque
stringData:
  .consensus_secrets.yaml: &lt;here goes the content of the consensus_secrets.yaml file&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l1-batch-reversion"><a class="header" href="#l1-batch-reversion">L1 Batch reversion</a></h1>
<h3 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h3>
<p>In extremly rare circumstances due to an operator mistake or a bug zksync-era you may revert unexecuted L1 batches. This
could be usefull if an unprovable batch would be committed or a commit tranasction failed due to mismatch between server
and blockchain state.</p>
<h3 id="overview-2"><a class="header" href="#overview-2">Overview</a></h3>
<p>The purpose of block reverter is to revert all stateful components state and uncommit the L1 batch/-es.</p>
<p>Block reverter performs following operations:</p>
<ul>
<li><code>rollback-db</code> command:
<ul>
<li>revert state keeper cache</li>
<li>revert vm runners’ caches</li>
<li>revert all the trees</li>
<li>revert Postgres state</li>
</ul>
</li>
<li><code>send-eth-transaction</code> command: revert L1 commit</li>
<li><code>clear-failed-transactions</code> command: removing failed L1 transaction from DB</li>
</ul>
<p><strong>Note: this doesn’t cover reverting the prover subsystem.</strong></p>
<h3 id="procedure"><a class="header" href="#procedure">Procedure</a></h3>
<ul>
<li>Verify that <code>stuck_tx_timeout</code> is high enough that the relevant transactions are not deleted</li>
<li>Stop <code>state_keeper</code>, <code>tree</code>, all vm runners (<code>protective-reads-writer</code>, <code>bwip</code>, <code>vm-playground</code> ) and snapshot creator</li>
<li>We’ll have to connect to the <strong>machines</strong> that have RocksDB local state ( <code>tree</code> and <code>state_keeper</code>). But their
components cannot function. One way to achieve this is to run respective containers with <code>-components=http_api</code> (
<code>-components=</code> won’t work - we have to set something) instead of the normal components (e.g. <code>state_keeper</code>)</li>
<li>connect to the server/container that is running the tree</li>
<li>Run <code>print-suggested-values</code> pay attention to log message - committed ≠ verified ≠ executed**</li>
</ul>
<pre><code class="language-bash">root@server-0:/# RUST_LOG=info ./usr/bin/block_reverter print-suggested-values \
--operator-address CHANGE_ME \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml

{"timestamp":"2024-04-15T17:42:40.913018741Z","level":"INFO","fields":{"message":"Last L1 batch numbers on contract: committed 517339, verified 516945, executed 516945"},"target":"zksync_core::block_reverter","filename":"core/lib/zksync_core/src/block_reverter/mod.rs","line_number":446}
Suggested values for rollback: SuggestedRollbackValues {
 last_executed_l1_batch_number: L1BatchNumber(
  516945,
 ),
 nonce: 575111,
 priority_fee: 1000000000,
}
</code></pre>
<ul>
<li>Rollback blocks on contract: run <code>send-eth-transaction</code> (Only if revert on L1 is needed). Note: here and later
<code>--l1-batch-number</code> is the number of latest batch <strong>remining</strong> after revert.</li>
</ul>
<pre><code class="language-bash">root@server-0:/# RUST_LOG=info ETH_SENDER_SENDER_OPERATOR_PRIVATE_KEY=CHANGE_ME ./usr/bin/block_reverter send-eth-transaction \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server-v2-secrets/server-v2-secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
--nonce CHANGE_ME
Mar 27 11:50:36.347  INFO zksync_eth_client::ethereum_gateway: Operator address: 0x01239bb69d0c5f795de5c86fd00d25f9cd3b9deb
waiting for L1 transaction confirmation...
revert transaction has completed
</code></pre>
<ul>
<li>Run <code>print-suggested-values</code> again to make sure the contract has updated values: <strong>pay attention to log</strong> - now it
should say that <code>committed</code> equals <code>verified</code> that equals <code>executed</code>:</li>
</ul>
<pre><code>{"timestamp":"2024-04-15T18:38:36.015134635Z","level":"INFO","fields":{"message":"Last L1 batch numbers on contract: committed 516945, verified 516945, executed 516945 516945"},"target":"zksync_core::block_reverter","filename":"core/lib/zksync_core/src/block_reverter/mod.rs","line_number":446}
</code></pre>
<ul>
<li>Rollback tree: run <code>rollback-db</code> with <code>--rollback-tree</code> flag</li>
</ul>
<pre><code class="language-bash">root@server-0:/# RUST_LOG=info ./usr/bin/block_reverter rollback-db \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
--rollback-tree

{"timestamp":"2022-08-30T10:32:11.312280583Z","level":"INFO","fields":{"message":"Operator address: 0x85b7b2fcfd6d3e5c063e7a5063359f4e6b3fec29"},"target":"zksync_eth_client::clients::http_client"}
getting logs that should be applied to rollback state...
{"timestamp":"2022-08-30T10:32:11.554904678Z","level":"INFO","fields":{"message":"fetching keys that were changed after given block number"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:32:11.608430505Z","level":"INFO","fields":{"message":"loaded 2844 keys"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:32:12.933738644Z","level":"INFO","fields":{"message":"processed 1000 values"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:32:14.243366764Z","level":"INFO","fields":{"message":"processed 2000 values"},"target":"zksync_dal::storage_logs_dedup_dal"}
rolling back merkle tree...
checking match of the tree root hash and root hash from Postgres...
saving tree changes to disk...
</code></pre>
<ul>
<li>connect to the server/container that is running the state keeper</li>
<li>Rollback state keeper cache: run <code>rollback-db</code> with <code>--rollback-sk-cache</code></li>
</ul>
<pre><code class="language-bash">root@server-0:/# RUST_LOG=info ./usr/bin/block_reverter rollback-db \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
--rollback-sk-cache

{"timestamp":"2022-08-30T10:37:17.99102197Z","level":"INFO","fields":{"message":"Operator address: 0x85b7b2fcfd6d3e5c063e7a5063359f4e6b3fec29"},"target":"zksync_eth_client::clients::http_client"}
getting logs that should be applied to rollback state...
{"timestamp":"2022-08-30T10:37:18.364586826Z","level":"INFO","fields":{"message":"fetching keys that were changed after given block number"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:37:18.386431879Z","level":"INFO","fields":{"message":"loaded 2844 keys"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:37:19.107738185Z","level":"INFO","fields":{"message":"processed 1000 values"},"target":"zksync_dal::storage_logs_dedup_dal"}
{"timestamp":"2022-08-30T10:37:19.823963293Z","level":"INFO","fields":{"message":"processed 2000 values"},"target":"zksync_dal::storage_logs_dedup_dal"}
opening DB with state keeper cache...
getting contracts and factory deps that should be removed...
rolling back state keeper cache...
</code></pre>
<ul>
<li>Rollback vm runners’ caches: connect to each vm-runner instance and run command</li>
</ul>
<pre><code class="language-bash">./usr/bin/block_reverter rollback-db \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
--rollback-vm-runners-cache
</code></pre>
<ul>
<li>Rollback postgres:</li>
</ul>
<pre><code class="language-bash">root@server-0:/# ./r/bin/block_reverter rollback-db \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
--rollback-postgres
</code></pre>
<ul>
<li>
<p>Clear failed l1 txs</p>
<pre><code class="language-bash">root@server-0:/# ./usr/bin/block_reverter clear-failed-transactions \
--genesis-path=/config/genesis/genesis.yaml \
--wallets-path=/config/wallets/wallets.yaml \
--config-path=/config/general/general.yaml \
--contracts-config-path=/config/contracts/contracts.yaml \
--secrets-path=/config/server/secrets.yaml \
--l1-batch-number CHANGE_ME_LAST_TO_KEEP \
</code></pre>
</li>
<li>
<p>Resume deployments for the relevant components</p>
</li>
<li>
<p>Restart API nodes to clear cache</p>
</li>
<li>
<p>⚠️ Check external nodes to see whether they have performed blocks rollback. If it they did not - do it manually</p>
</li>
</ul>
<h2 id="issues"><a class="header" href="#issues">ISSUES</a></h2>
<ul>
<li>If you faced issues with bootloader e.g:
<code>The bootloader failed to set previous block hash. Reason: The provided block number is not correct</code></li>
</ul>
<p>You have to remove State Keeper cache. Run server-core in a safe regime. (see the first steps of this document). Login
to the server and remove state-keeper files</p>
<pre><code class="language-bash">root@server-0:/# rm -rf /db/state_keeper/
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-debugging"><a class="header" href="#advanced-debugging">Advanced debugging</a></h1>
<h2 id="debugging-backend-in-vscode"><a class="header" href="#debugging-backend-in-vscode">Debugging backend in vscode</a></h2>
<p>Our backend takes configuration from environment variables, so before starting the debugging, we must make sure that
they are properly set.</p>
<p>You should create the following file in your <code>$workspaceFolder/.vscode/</code> called <code>prelaunch.py</code>:</p>
<pre><code class="language-python">import os
import lldb

# Read the .env file and store the key-value pairs in a array with format ["key=value"]
env_array = []
with open(os.path.join("etc/env/l2-inits/dev.init.env")) as f:
    for line in f:
        if line.strip() and line.strip()[0] != "#":
            env_array.append(line.strip())

with open(os.path.join("etc/env/targets/dev.env")) as f:
    for line in f:
        if line.strip() and line.strip()[0] != "#":
            env_array.append(line.strip())

target = lldb.debugger.GetSelectedTarget()

launch_info = target.GetLaunchInfo()
launch_info.SetEnvironmentEntries(env_array, True)
target.SetLaunchInfo(launch_info)
</code></pre>
<p>This file will load environment variables from <code>dev.init.env</code> and <code>dev.env</code> before starting the binary (notice that we
do this in a particular order, as values in dev.env should be overwriting the ones in dev.init.env).</p>
<p>Afterwards you need to add something like this to your launch.json:</p>
<pre><code> "configurations": [
        {
            "type": "lldb",
            "request": "launch",
            "name": "Debug executable 'zksync_server' DEV ENV",
            "cargo": {
                "args": [
                    "build",
                    "--bin=zksync_server",
                    "--package=zksync_core"
                ],
                "filter": {
                    "name": "zksync_server",
                    "kind": "bin"
                }
            },
            "args": [],
            "cwd": "${workspaceFolder}",
            "preRunCommands": [
                "command script import ${workspaceFolder}/.vscode/prelaunch.py"
            ]
        },
        ...
    ]
</code></pre>
<h2 id="debugging-contracts-in-vscode-using-hardhat"><a class="header" href="#debugging-contracts-in-vscode-using-hardhat">Debugging contracts in vscode (using hardhat)</a></h2>
<p>Assuming that you created project in hardhat, that you’d normally test with <code>hardhat test</code> - you also also test it with
vscode (which is super powerful - especially as you can have both binaries’ debug sessions running in VSCode at the same
time).</p>
<p>in package.json, make sure to have:</p>
<pre><code class="language-json">"scripts": {
        //...
        "test": "hardhat test",
        //...
    }
</code></pre>
<p>and then in VSCode’s launch.json:</p>
<pre><code class="language-json">{
  "version": "0.2.0",
  "configurations": [
    {
      "type": "node",
      "request": "launch",
      "name": "Launch Program",
      "console": "integratedTerminal",
      "runtimeExecutable": "yarn",
      "runtimeArgs": ["test", "${workspaceFolder}/test/try.js"]
    }
  ]
}
</code></pre>
<p>where <code>test/try.js</code> is your test code.</p>
<h2 id="performance-analysis-of-rust-binaries-flame-graphs"><a class="header" href="#performance-analysis-of-rust-binaries-flame-graphs">Performance analysis of rust binaries (flame graphs)</a></h2>
<p>If you’d like to analyze the CPU performance of your rust binary, you can use ‘perf’ to compute the flame graphs.</p>
<p>First - run the binary with perf enabled (this will make the binary a little bit slower):</p>
<pre><code>sudo perf record -F500 --call-graph=dwarf,65528  /path/to/binary --other --flags
</code></pre>
<p>(you can also connect to already running binary - by providing its PID with <code>-p</code> option)</p>
<p>When you’re done collecting records, you have to convert them into flame-graph friendly format, by running:</p>
<pre><code>sudo perf script -F +pid &gt; perfbench.script
</code></pre>
<p>This will create the perfbench.script file, that you can later upload to <a href="https://profiler.firefox.com/">https://profiler.firefox.com/</a> and see the
detailed flame graph.</p>
<h2 id="debuggingunderstandingtracing-zkevm-assembly"><a class="header" href="#debuggingunderstandingtracing-zkevm-assembly">Debugging/understanding/tracing zkEVM assembly</a></h2>
<p>Currently this is quite a complex process, but we’re working on making it a little bit smoother.</p>
<p>You start by installing the ‘compiler-tester’ repo (see its README.md instructions for details) - it is quite heavy as
it needs the LLVM etc etc.</p>
<p>Afterwards, you can look at one of the tests (for example
<a href="https://github.com/matter-labs/era-compiler-tests/blob/main/solidity/simple/default.sol">tests/solidity/simple/default.sol</a>).</p>
<pre><code class="language-solidity">//! { "cases": [ {
//!     "name": "first",
//!     "inputs": [
//!         {
//!             "method": "first",
//!             "calldata": [
//!             ]
//!         }
//!     ],
//!     "expected": [
//!         "42"
//!     ]
//! }, ] }

// SPDX-License-Identifier: MIT

pragma solidity &gt;=0.4.16;

contract Test {
  function first() public pure returns (uint64) {
    uint64 result = 42;
    return result;
  }
}

</code></pre>
<p>As you can see - it is self-contained - it has the solidity code at the bottom, and the top comments are used to define
the test case - and expected result.</p>
<p>You can run it by calling:</p>
<pre><code class="language-shell">cargo run --release --bin compiler-tester -- -DT \
        --path='tests/solidity/simple/default.sol' \
        --mode='Y+M3B3 0.8.19'
</code></pre>
<p>And then collect the detailed tracing information from trace directory. You’ll notice that you have 2 files for each
test - one covering the deployment, and one covering the test run.</p>
<p>You can take test run one and upload it to <a href="https://explorer.zksync.io/tools/debugger">our debugger</a> to see detailed
zkAssembler and state of memory, heap, stack and registers at each execution step.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="docker-and-ci"><a class="header" href="#docker-and-ci">Docker and CI</a></h1>
<p>How to efficiently debug CI issues locally.</p>
<p>This document will be useful in case you struggle with reproducing some CI issues on your local machine.</p>
<p>In most cases, this is due to the fact that your local machine has some arifacts, configs, files that you might have set
in the past, that are missing from the CI.</p>
<h2 id="basic-docker-commands"><a class="header" href="#basic-docker-commands">Basic docker commands</a></h2>
<ul>
<li><code>docker ps</code> - prints the list of currently running containers</li>
<li><code>docker run</code> - starts a new docker container</li>
<li><code>docker exec</code> - connects to a running container and executes the command.</li>
<li><code>docker kill</code> - stops the container.</li>
<li><code>docker cp</code> - allows copying files between your system and docker container.</li>
</ul>
<p>Usually docker containers have a specific binary that they run, but for debugging we often want to start a bash instead.</p>
<p>The command below starts a new docker containers, and instead of running its binary - runs <code>/bin/bash</code> in interactive
mode.</p>
<pre><code>docker run  -it matterlabs/zk-environment:latest2.0-lightweight-nightly /bin/bash
</code></pre>
<p>Connects to <strong>already running</strong> job, and gets you the interactive shell.</p>
<pre><code>docker exec -i -it local-setup-zksync-1 /bin/bash
</code></pre>
<h2 id="debugging-ci"><a class="header" href="#debugging-ci">Debugging CI</a></h2>
<p>Many of the tests require postgres &amp; reth - you initialize them with:</p>
<pre><code>docker compose up -d

</code></pre>
<p>You should see something like this:</p>
<pre><code>[+] Running 3/3
 ⠿ Network zksync-era_default       Created   0.0s
 ⠿ Container zksync-era-postgres-1  Started   0.3s
 ⠿ Container zksync-era-reth-1      Started   0.3s
</code></pre>
<p>Start the docker with the ‘basic’ imge</p>
<pre><code># We tell it to connect to the same 'subnetwork' as other containers (zksync-era_default).
# the IN_DOCKER variable is changing different urls (like postgres) from localhost to postgres - so that it can connect to those
# containers above.
docker run --network zksync-era_default -e IN_DOCKER=1   -it matterlabs/zk-environment:latest2.0-lightweight-nightly /bin/bash
# and then inside, run:

git clone https://github.com/matter-labs/zksync-era.git .
git checkout YOUR_BRANCH
zk
</code></pre>
<p>After this, you can run any commands you need.</p>
<p>When you see a command like <code>ci_run zkstack dev contracts</code> in the CI - this simply means that it executed
<code>zkstack dev contracts</code> inside that docker container.</p>
<p><strong>IMPORTANT</strong> - by default, docker is running in the mode, where it does NOT persist the changes. So if you exit that
shell, all the changes will be removed (so when you restart, you’ll end up in the same pristine condition). You can
‘commit’ your changes into a new docker image, using <code>docker commit XXX some_name</code>, where XXX is your container id from
<code>docker ps</code>. Afterwards you can ‘start’ this docker image with <code>docker run ... some_name</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-documentation"><a class="header" href="#node-documentation">Node Documentation</a></h1>
<p>The Node (sometimes referred to as External Node or EN) is a read-only replica of the main node.</p>
<h2 id="what-is-the-node"><a class="header" href="#what-is-the-node">What is the Node</a></h2>
<p>The Node is a read-replica of the main (centralized) node that can be run by external parties. It functions by receiving
blocks from the given ZK Stack chain and re-applying transactions locally, starting from the genesis block. The Node
shares most of its codebase with the main node. Consequently, when it re-applies transactions, it does so exactly as the
main node did in the past.</p>
<p><strong>It has three modes of initialization:</strong></p>
<ul>
<li>recovery from genesis (Not supported on ZKsync Era), in Ethereum terms this corresponds to archival node, this option
is slower than recovery from DB dump, but is the easiest way to spin up new Node.</li>
<li>recovery from a DB dump, in Ethereum terms this corresponds to archival node.</li>
<li>recovery from a snapshot, in Ethereum terms this corresponds to light node, such nodes will only have access to
transactions data from after the node was initialized. The database can be pruned on such nodes.</li>
</ul>
<h2 id="high-level-overview-1"><a class="header" href="#high-level-overview-1">High-level overview</a></h2>
<p>At a high level, the Node can be seen as an application that has the following modules:</p>
<ul>
<li>API server that provides the publicly available Web3 interface.</li>
<li>Consensus layer (ZKsync Era only for now) that interacts with the peer network and retrieves transactions and blocks
to re-execute.</li>
<li>Sequencer component that actually executes and persists transactions received from the synchronization layer.</li>
<li>Several checker modules that ensure the consistency of the Node state.</li>
</ul>
<p>With the EN, you are able to:</p>
<ul>
<li>Locally recreate and verify a ZK Stack chain’s, for example ZKsync Era’s mainnet/testnet state.</li>
<li>Interact with the recreated state in a trustless way. The validity is locally verified.</li>
<li>Use the Web3 API without having to query the main node.</li>
<li>Send L2 transactions (that will be proxied to the main node).</li>
</ul>
<p>With the EN, you <em>can not</em>:</p>
<ul>
<li>Create L2 blocks or L1 batches on your own.</li>
<li>Generate proofs.</li>
<li>Submit data to L1.</li>
</ul>
<p>A more detailed overview of the Node’s components is provided in the <a href="guides/external-node/06_components.html">components</a> section.</p>
<h2 id="api-overview"><a class="header" href="#api-overview">API overview</a></h2>
<p>API exposed by the Node strives to be Web3-compliant. If some method is exposed but behaves differently compared to
Ethereum, it should be considered a bug. Please <a href="https://zksync.io/contact">report</a> such cases.</p>
<h3 id="eth-namespace"><a class="header" href="#eth-namespace"><code>eth</code> namespace</a></h3>
<p>Data getters in this namespace operate in the L2 space: require/return L2 block numbers, check balances in L2, etc.</p>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>eth_blockNumber</code></td><td></td></tr>
<tr><td><code>eth_chainId</code></td><td></td></tr>
<tr><td><code>eth_call</code></td><td></td></tr>
<tr><td><code>eth_estimateGas</code></td><td></td></tr>
<tr><td><code>eth_gasPrice</code></td><td></td></tr>
<tr><td><code>eth_newFilter</code></td><td>Maximum amount of installed filters is configurable</td></tr>
<tr><td><code>eth_newBlockFilter</code></td><td>Same as above</td></tr>
<tr><td><code>eth_newPendingTransactionsFilter</code></td><td>Same as above</td></tr>
<tr><td><code>eth_uninstallFilter</code></td><td></td></tr>
<tr><td><code>eth_getLogs</code></td><td>Maximum amount of returned entities can be configured</td></tr>
<tr><td><code>eth_getFilterLogs</code></td><td>Same as above</td></tr>
<tr><td><code>eth_getFilterChanges</code></td><td>Same as above</td></tr>
<tr><td><code>eth_getBalance</code></td><td></td></tr>
<tr><td><code>eth_getBlockByNumber</code></td><td></td></tr>
<tr><td><code>eth_getBlockByHash</code></td><td></td></tr>
<tr><td><code>eth_getBlockTransactionCountByNumber</code></td><td></td></tr>
<tr><td><code>eth_getBlockTransactionCountByHash</code></td><td></td></tr>
<tr><td><code>eth_getCode</code></td><td></td></tr>
<tr><td><code>eth_getStorageAt</code></td><td></td></tr>
<tr><td><code>eth_getTransactionCount</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByHash</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByBlockHashAndIndex</code></td><td></td></tr>
<tr><td><code>eth_getTransactionByBlockNumberAndIndex</code></td><td></td></tr>
<tr><td><code>eth_getTransactionReceipt</code></td><td></td></tr>
<tr><td><code>eth_protocolVersion</code></td><td></td></tr>
<tr><td><code>eth_sendRawTransaction</code></td><td></td></tr>
<tr><td><code>eth_syncing</code></td><td>Node is considered synced if it’s less than 11 blocks behind the main node.</td></tr>
<tr><td><code>eth_coinbase</code></td><td>Always returns a zero address</td></tr>
<tr><td><code>eth_accounts</code></td><td>Always returns an empty list</td></tr>
<tr><td><code>eth_getCompilers</code></td><td>Always returns an empty list</td></tr>
<tr><td><code>eth_hashrate</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_getUncleCountByBlockHash</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_getUncleCountByBlockNumber</code></td><td>Always returns zero</td></tr>
<tr><td><code>eth_mining</code></td><td>Always returns false</td></tr>
</tbody></table>
</div>
<h3 id="pubsub"><a class="header" href="#pubsub">PubSub</a></h3>
<p>Only available on the WebSocket servers.</p>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>eth_subscribe</code></td><td>Maximum amount of subscriptions is configurable</td></tr>
<tr><td><code>eth_subscription</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="net-namespace"><a class="header" href="#net-namespace"><code>net</code> namespace</a></h3>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>net_version</code></td><td></td></tr>
<tr><td><code>net_peer_count</code></td><td>Always returns 0</td></tr>
<tr><td><code>net_listening</code></td><td>Always returns false</td></tr>
</tbody></table>
</div>
<h3 id="web3-namespace"><a class="header" href="#web3-namespace"><code>web3</code> namespace</a></h3>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>web3_clientVersion</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="debug-namespace"><a class="header" href="#debug-namespace"><code>debug</code> namespace</a></h3>
<p>The <code>debug</code> namespace gives access to several non-standard RPC methods, which will allow developers to inspect and debug
calls and transactions.</p>
<p>This namespace is disabled by default and can be configured via setting <code>EN_API_NAMESPACES</code> as described in the
<a href="guides/external-node/prepared_configs/mainnet-config.env">example config</a>.</p>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/01_intro.html#admonition-note"></a>
</div>
<div>
<p>The traces will only start being generated for blocks synced after the debug namespace is enabled, they will not be
backfilled! The only way to get traces for historical blocks is to fully re-sync the node</p>
</div>
</div>
<p>Available methods:</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>debug_traceBlockByNumber</code></td><td></td></tr>
<tr><td><code>debug_traceBlockByHash</code></td><td></td></tr>
<tr><td><code>debug_traceCall</code></td><td></td></tr>
<tr><td><code>debug_traceTransaction</code></td><td></td></tr>
</tbody></table>
</div>
<h3 id="zks-namespace"><a class="header" href="#zks-namespace"><code>zks</code> namespace</a></h3>
<p>This namespace contains rollup-specific extensions to the Web3 API. Note that <em>only methods</em> specified in the
<a href="https://docs.zksync.io/build/api-reference/zks-rpc">documentation</a> are considered public. There may be other methods exposed in this namespace, but undocumented
methods come without any kind of stability guarantees and can be changed or removed without notice.</p>
<p>Always refer to the documentation linked above to see the list of stabilized methods in this namespace.</p>
<h3 id="en-namespace"><a class="header" href="#en-namespace"><code>en</code> namespace</a></h3>
<p>This namespace contains methods that Nodes call on the main node while syncing. If this namespace is enabled, other ENs
can sync from this node.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<h2 id="prerequisites-3"><a class="header" href="#prerequisites-3">Prerequisites</a></h2>
<p>Install <code>docker compose</code> and <code>Docker</code></p>
<h2 id="running-zksync-node-locally"><a class="header" href="#running-zksync-node-locally">Running ZKsync node locally</a></h2>
<p>These commands start ZKsync Node locally inside docker.</p>
<p>For adjusting the Dockerfiles to use them with other chains setup using ZK Stack, see
<a href="guides/external-node/11_setup_for_other_chains.html">setup_for_other_chains</a></p>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/00_quick_start.html#admonition-note"></a>
</div>
<div>
<p>If you want to run Node for a chain different than ZKsync ERA, you can ask the company hosting the chains for the
ready docker-compose files.</p>
</div>
</div>
<p>To start a mainnet instance, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file mainnet-external-node-docker-compose.yml up
</code></pre>
<p>To reset its state, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file mainnet-external-node-docker-compose.yml down --volumes
</code></pre>
<p>To start a testnet instance, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file testnet-external-node-docker-compose.yml up
</code></pre>
<p>To reset its state, run:</p>
<pre><code class="language-sh">cd docker-compose-examples
sudo docker compose --file testnet-external-node-docker-compose.yml down --volumes
</code></pre>
<h3 id="observability"><a class="header" href="#observability">Observability</a></h3>
<p>You can see the status of the node (after recovery) in <a href="http://localhost:3000/dashboards">local grafana dashboard</a>. You
can also access a debug page with more information about the node <a href="http://localhost:5000">here</a>.</p>
<p>The HTTP JSON-RPC API can be accessed on port <code>3060</code> and WebSocket API can be accessed on port <code>3061</code>.</p>
<div id="admonition-note-1" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/00_quick_start.html#admonition-note-1"></a>
</div>
<div>
<p>The node will recover from a snapshot on it’s first run, this may take up to 10h. Before the recovery is finished, the
API server won’t serve any requests.</p>
<p>If you need access to historical transaction data, please use recovery from DB dumps (see Advanced setup section)</p>
</div>
</div>
<h3 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h3>
<div id="admonition-note-2" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-2-title">
<div class="admonition-title">
<div id="admonition-note-2-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/00_quick_start.html#admonition-note-2"></a>
</div>
<div>
<p>Those are requirements for nodes that use snapshots recovery and history pruning (the default for docker-compose
setup).</p>
<p>For requirements for nodes running from DB dump see the <a href="guides/external-node/03_running.html">running</a> section. DB dumps are a way to start
Node with full historical transactions history.</p>
<p>For nodes with pruning disabled, expect the storage requirements on mainnet to grow at 1TB per month. If you want to
stop historical DB pruning you can read more about this in the <a href="guides/external-node/08_pruning.html">pruning</a> section.</p>
</div>
</div>
<div id="admonition-note-3" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-3-title">
<div class="admonition-title">
<div id="admonition-note-3-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/00_quick_start.html#admonition-note-3"></a>
</div>
<div>
<p>For chains other than ZKSync Era, the system requirements can be slightly lower (CPU and RAM) or even much lower
(storage), depending on the chain.</p>
</div>
</div>
<ul>
<li>32 GB of RAM and a relatively modern CPU</li>
<li>50 GB of storage for testnet nodes</li>
<li>500 GB of storage for mainnet nodes</li>
<li>100 Mbps connection (1 Gbps+ recommended)</li>
</ul>
<h2 id="advanced-setup"><a class="header" href="#advanced-setup">Advanced setup</a></h2>
<p>If you need monitoring, backups, to recover from DB dump or a more customized PostgreSQL settings, etc, please see:
<a href="https://github.com/matter-labs/ansible-en-role">ansible-en-role repo</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-configuration"><a class="header" href="#node-configuration">Node Configuration</a></h1>
<p>This document outlines various configuration options for the EN. Currently, the Node requires the definition of numerous
environment variables. To streamline this process, we provide prepared configs for the ZKsync Era - for both
<a href="guides/external-node/prepared_configs/mainnet-config.env">mainnet</a> and <a href="guides/external-node/prepared_configs/testnet-sepolia-config.env">testnet</a>. You can use
these files as a starting point and modify only the necessary sections.</p>
<p><strong>You can also see directory docker-compose-examples if you want to run external-node on your machine with recommended
default settings.</strong></p>
<h2 id="database"><a class="header" href="#database">Database</a></h2>
<p>The Node uses two databases: PostgreSQL and RocksDB.</p>
<p>PostgreSQL serves as the main source of truth in the EN, so all the API requests fetch the state from there. The
PostgreSQL connection is configured by the <code>DATABASE_URL</code>. Additionally, the <code>DATABASE_POOL_SIZE</code> variable defines the
size of the connection pool.</p>
<p>RocksDB is used in components where IO is a bottleneck, such as the State Keeper and the Merkle tree. If possible, it is
recommended to use an NVME SSD for RocksDB. RocksDB requires two variables to be set: <code>EN_STATE_CACHE_PATH</code> and
<code>EN_MERKLE_TREE_PATH</code>, which must point to different directories. When running a Node inside Docker Compose, Kubernetes
etc., these paths should point to dirs in a persistent volume (or 2 separate volumes). Persistent volumes <strong>must</strong> be
exclusive to a node; i.e., they <strong>must not</strong> be shared among nodes concurrently or transferred from one node to another.
Failing to adhere to this rule may result in RocksDB corruption or
<a href="guides/external-node/05_troubleshooting.html#rocksdb-issues">the Node crashing on start</a>.</p>
<h2 id="l1-web3-client"><a class="header" href="#l1-web3-client">L1 Web3 client</a></h2>
<p>Node requires a connection to an Ethereum node. The corresponding env variable is <code>EN_ETH_CLIENT_URL</code>. Make sure to set
the URL corresponding to the correct L1 network (L1 mainnet for L2 mainnet and L1 sepolia for L2 testnet).</p>
<p>Note: Currently, the Node makes 2 requests to the L1 per L1 batch, so the Web3 client usage for a synced node should not
be high. However, during the synchronization phase the new batches would be persisted on the Node quickly, so make sure
that the L1 client won’t exceed any limits (e.g. in case you use Infura).</p>
<h2 id="exposed-ports"><a class="header" href="#exposed-ports">Exposed ports</a></h2>
<p>The dockerized version of the server exposes the following ports:</p>
<ul>
<li>HTTP JSON-RPC: 3060</li>
<li>WebSocket JSON-RPC: 3061</li>
<li>Prometheus listener: 3322</li>
<li>Healthcheck server: 3081</li>
</ul>
<p>While the configuration variables for them exist, you are not expected to change them unless you want to use the Node
outside provided Docker environment (not supported at the time of writing).</p>
<h2 id="api-limits"><a class="header" href="#api-limits">API limits</a></h2>
<p>There are variables that allow you to fine-tune the limits of the RPC servers, such as limits on the number of returned
entries or the limit for the accepted transaction size. Provided files contain sane defaults that are recommended for
use, but these can be edited, e.g. to make the Node more/less restrictive.</p>
<p><strong>Some common API limits config:</strong></p>
<ul>
<li><code>EN_MAX_RESPONSE_BODY_SIZE_MB</code> (default is 10 i.e. 10MB) controls max size of a single response. Hitting the limit
will result in errors similar to: “Response is too big (…)”.</li>
<li><code>EN_MAX_RESPONSE_BODY_SIZE_OVERRIDES_MB</code> overrides max response size for specific RPC methods. E.g., setting this var
to <code>eth_getLogs=100,eth_getBlockReceipts=None</code> sets max response size for <code>eth_getLogs</code> to 100MB and disables size
limiting for <code>eth_getBlockReceipts</code>, while other RPC methods will use the <code>EN_MAX_RESPONSE_BODY_SIZE_MB</code> setting.</li>
<li><code>EN_REQ_ENTITIES_LIMIT</code> (default 10,000) controls max possible limit of entities to be requested at once. Hitting the
limit will result in errors similar to: “Query returned more than 10000 results (…)”</li>
</ul>
<h2 id="json-rpc-api-namespaces"><a class="header" href="#json-rpc-api-namespaces">JSON-RPC API namespaces</a></h2>
<p>There are 7 total supported API namespaces: <code>eth</code>, <code>net</code>, <code>web3</code>, <code>debug</code> - standard ones; <code>zks</code> - rollup-specific one;
<code>pubsub</code> - a.k.a. <code>eth_subscribe</code>; <code>en</code> - used by Nodes while syncing. You can configure what namespaces you want to
enable using <code>EN_API_NAMESPACES</code> and specifying namespace names in a comma-separated list. By default, all but the
<code>debug</code> namespace are enabled.</p>
<h2 id="logging-and-observability"><a class="header" href="#logging-and-observability">Logging and observability</a></h2>
<ul>
<li><code>MISC_LOG_FORMAT</code> defines the format in which logs are shown: <code>plain</code> corresponds to the human-readable format, while
the other option is <code>json</code> (recommended for deployments).</li>
<li><code>RUST_LOG</code> variable allows you to set up the logs granularity (e.g. make the Node emit fewer logs). You can read about
the format <a href="https://docs.rs/env_logger/0.10.0/env_logger/#enabling-logging">here</a>.</li>
<li><code>MISC_SENTRY_URL</code> and <code>MISC_OTLP_URL</code> variables can be configured to set up Sentry and OpenTelemetry exporters.</li>
<li>If Sentry is configured, you also have to set <code>EN_SENTRY_ENVIRONMENT</code> variable to configure the environment in events
reported to sentry.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-node"><a class="header" href="#running-the-node">Running the Node</a></h1>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/03_running.html#admonition-note"></a>
</div>
<div>
<p>If you want to just run ZKSync node with recommended default setting, please see the <a href="guides/external-node/00_quick_start.html">quick start</a>
page.</p>
</div>
</div>
<p>This section assumes that you have prepared a configuration file as described on the
<a href="guides/external-node/02_configuration.html">previous page</a>.</p>
<h2 id="system-requirements-for-nodes-started-from-db-dumps"><a class="header" href="#system-requirements-for-nodes-started-from-db-dumps">System Requirements for nodes started from DB dumps</a></h2>
<p>This configuration is approximate and should be considered as <strong>minimal</strong> requirements.</p>
<ul>
<li>32-core CPU</li>
<li>64GB RAM</li>
<li>SSD storage (NVME recommended):
<ul>
<li>ZKsync Sepolia Testnet - 10GB Node + 50GB PostgreSQL (at the time of writing, will grow over time, so should be
constantly monitored)</li>
<li>ZKsync Mainnet - 3TB Node + 8TB PostgreSQL (at the time of writing, will grow over time, so should be constantly
monitored)</li>
</ul>
</li>
<li>100 Mbps connection (1 Gbps+ recommended)</li>
</ul>
<p>For smaller chains, less powerful hardware may be sufficient, especially in terms of disk space.</p>
<h2 id="a-note-about-postgresql-storage"><a class="header" href="#a-note-about-postgresql-storage">A note about PostgreSQL storage</a></h2>
<p>By far, the heaviest table to maintain is the <code>call_traces</code> table. This table is only required for the <code>debug</code>
namespace. If you want to clear some space and aren’t using the <code>debug</code> namespace, you can</p>
<ul>
<li>clear it with a simple query <code>DELETE FROM call_traces;</code></li>
<li>leave the <code>debug</code> namespace disabled via the <code>EN_API_NAMESPACES</code> env var as described in the
<a href="guides/external-node/prepared_configs/mainnet-config.env">example config</a>.</li>
</ul>
<h2 id="infrastructure"><a class="header" href="#infrastructure">Infrastructure</a></h2>
<p>You need to set up a PostgreSQL server, however it is out of the scope of these docs, but the popular choice is to run
it in Docker. There are many of guides on that,
<a href="https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/">here’s one example</a>.</p>
<p>Note however that if you run PostgresSQL as a stand-alone Docker image (e.g. not in Docker-compose with a network shared
between Node and Postgres), Node won’t be able to access Postgres via <code>localhost</code> or <code>127.0.0.1</code> URLs. To make it work,
you’ll have to either run it with a <code>--network host</code> (on Linux) or use <code>host.docker.internal</code> instead of <code>localhost</code> in
the Node configuration (<a href="https://docs.docker.com/desktop/networking/#i-want-to-connect-from-a-container-to-a-service-on-the-host">official docs</a>).</p>
<p>Besides running Postgres, you are expected to have a DB dump from a corresponding env. You can restore it using
<code>pg_restore -O -C &lt;DUMP_PATH&gt; --dbname=&lt;DB_URL&gt;</code>.</p>
<p>You can also refer to
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/src/guides/external-node/00_quick_start.md#advanced-setup">Node configuration management blueprint</a>
for advanced DB instance configurations.</p>
<h2 id="running"><a class="header" href="#running">Running</a></h2>
<p>Assuming you have the Node Docker image, an env file with the prepared configuration, and you have restored your DB with
the pg dump, that is all you need.</p>
<p>Sample running command:</p>
<pre><code class="language-sh">docker run --env-file &lt;path_to_env_file&gt; --mount type=bind,source=&lt;local_rocksdb_data_path&gt;,target=&lt;configured_rocksdb_data_path&gt; &lt;image&gt;
</code></pre>
<p>Helm charts and other infrastructure configuration options, if required, would be available later.</p>
<h2 id="first-start"><a class="header" href="#first-start">First start</a></h2>
<p>When you start the node for the first time, the state in PostgreSQL corresponds to the dump you have used, but the state
in RocksDB (mainly the Merkle tree) is absent. Before the node can make any progress, it has to rebuild the state in
RocksDB and verify consistency. The exact time required for that depends on the hardware configuration, but it is
reasonable to expect the state rebuild on the mainnet to take more than 20 hours.</p>
<h2 id="genesis-synchronization"><a class="header" href="#genesis-synchronization">Genesis synchronization</a></h2>
<p>While it’s technically possible to synchronize an External Node from genesis, this process is not recommended for any
users. The synchronization algorithm used for External Nodes is designed for keeping the live node up-to-date but
doesn’t implement optimizations to re-execute large amounts of historical blocks. As a result, genesis synchronization
on ZKsync Era can take multiple months for a single node, and it is not being continuously tested for each release. The
node is guaranteed to never be in an incorrect state, but issues unrelated to correctness may occur (such as
synchronization getting stuck).</p>
<p>The recommended ways to run the External Node are:</p>
<ul>
<li><a href="guides/external-node/./07_snapshots_recovery.html">Snapshot synchronization</a> for full nodes (if you don’t need historical data).</li>
<li><a href="guides/external-node/./00_quick_start.html#advanced-setup">Restoration from a DB dump</a> for archival nodes.</li>
</ul>
<h2 id="redeploying-the-node-with-a-new-pg-dump"><a class="header" href="#redeploying-the-node-with-a-new-pg-dump">Redeploying the Node with a new PG dump</a></h2>
<p>If you’ve been running the Node for some time and are going to redeploy it using a new PG dump, you should</p>
<ul>
<li>Stop the EN</li>
<li>Remove SK cache (corresponding to <code>EN_STATE_CACHE_PATH</code>)</li>
<li>Remove your current DB</li>
<li>Restore with the new dump</li>
<li>Start the EN</li>
</ul>
<p>Monitoring the node behavior and analyzing the state it’s in is covered in the
<a href="guides/external-node/04_observability.html">observability section</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-observability"><a class="header" href="#node-observability">Node Observability</a></h1>
<p>The Node provides several options for setting up observability. Configuring logs and sentry is described in the
<a href="guides/external-node/02_configuration.html">configuration</a> section, so this section focuses on the exposed metrics.</p>
<p>This section is written with the assumption that you’re familiar with
<a href="https://prometheus.io/docs/introduction/overview/">Prometheus</a> and <a href="https://grafana.com/docs/">Grafana</a>.</p>
<h2 id="buckets"><a class="header" href="#buckets">Buckets</a></h2>
<p>By default, latency histograms are distributed in the following buckets (in seconds):</p>
<pre><code>[0.001, 0.005, 0.025, 0.1, 0.25, 1.0, 5.0, 30.0, 120.0]
</code></pre>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>Node exposes a lot of metrics, a significant amount of which aren’t interesting outside the development flow. This
section’s purpose is to highlight metrics that may be worth observing in the external setup.</p>
<p>If you are not planning to scrape Prometheus metrics, please unset <code>EN_PROMETHEUS_PORT</code> environment variable to prevent
memory leaking.</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>external_node_synced</code></td><td>Gauge</td><td>-</td><td>1 if synced, 0 otherwise. Matches <code>eth_call</code> behavior</td></tr>
<tr><td><code>external_node_sync_lag</code></td><td>Gauge</td><td>-</td><td>How many blocks behind the main node the Node is</td></tr>
<tr><td><code>external_node_fetcher_requests</code></td><td>Histogram</td><td><code>stage</code>, <code>actor</code></td><td>Duration of requests performed by the different fetcher components</td></tr>
<tr><td><code>external_node_fetcher_cache_requests</code></td><td>Histogram</td><td>-</td><td>Duration of requests performed by the fetcher cache layer</td></tr>
<tr><td><code>external_node_fetcher_miniblock</code></td><td>Gauge</td><td><code>status</code></td><td>The number of the last L2 block update fetched from the main node</td></tr>
<tr><td><code>external_node_fetcher_l1_batch</code></td><td>Gauge</td><td><code>status</code></td><td>The number of the last batch update fetched from the main node</td></tr>
<tr><td><code>external_node_action_queue_action_queue_size</code></td><td>Gauge</td><td>-</td><td>Amount of fetched items waiting to be processed</td></tr>
<tr><td><code>server_miniblock_number</code></td><td>Gauge</td><td><code>stage</code>=<code>sealed</code></td><td>Last locally applied L2 block number</td></tr>
<tr><td><code>server_block_number</code></td><td>Gauge</td><td><code>stage</code>=<code>sealed</code></td><td>Last locally applied L1 batch number</td></tr>
<tr><td><code>server_block_number</code></td><td>Gauge</td><td><code>stage</code>=<code>tree_lightweight_mode</code></td><td>Last L1 batch number processed by the tree</td></tr>
<tr><td><code>server_processed_txs</code></td><td>Counter</td><td><code>stage</code>=<code>mempool_added, state_keeper</code></td><td>Can be used to show incoming and processing TPS values</td></tr>
<tr><td><code>api_web3_call</code></td><td>Histogram</td><td><code>method</code></td><td>Duration of Web3 API calls</td></tr>
<tr><td><code>sql_connection_acquire</code></td><td>Histogram</td><td>-</td><td>Time to get an SQL connection from the connection pool</td></tr>
</tbody></table>
</div>
<p>Metrics can be used to detect anomalies in configuration, which is described in more detail in the
<a href="guides/external-node/05_troubleshooting.html">next section</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-troubleshooting"><a class="header" href="#node-troubleshooting">Node Troubleshooting</a></h1>
<p>The Node tries to follow the fail-fast principle: if an anomaly is discovered, instead of attempting state recovery, in
most cases it will restart. Most of the time it will manifest as crashes, and if it happens once, it shouldn’t be
treated as a problem.</p>
<p>However, if the node enters the crash loop or otherwise behaves unexpectedly, it may indicate either a bug in the
implementation or a problem with configuration. This section tries to cover common problems.</p>
<h2 id="panics"><a class="header" href="#panics">Panics</a></h2>
<p>Panics is the Rust programming language notion of irrecoverable errors, and normally if panic happens, the application
will immediately crash.</p>
<ul>
<li>Panic matching <code>called Result::unwrap() on an Err value: Database(PgDatabaseError</code>: problem communicating with the
PostgreSQL, most likely some of the connections have died.</li>
<li>Panic matching <code>failed to init rocksdb: Error { message: "IO error: No space left on device</code>: more space on SSD is
required.</li>
<li>Anything that mentions “Poison Error”: a “secondary” panic that may occur if one of the components panicked first. If
you see this panic, look for a panic that happened shortly before it to find the real cause.</li>
</ul>
<p>Other kinds of panic aren’t normally expected. While in most cases, the state will be recovered after a restart, please
<a href="https://zksync.io/contact">report</a> such cases to Matter Labs regardless.</p>
<h2 id="genesis-issues"><a class="header" href="#genesis-issues">Genesis Issues</a></h2>
<p>On Era, a Node is supposed to start with an applied Postgres dump, or
<a href="guides/external-node/07_snapshots_recovery.html">recover from a snapshot</a> (the latter requires an opt-in by changing the node config; see the
linked article for details). If you see any genesis-related errors for an Era node without snapshot recovery activated,
it may mean the Node was started without an applied dump. For other networks, a Node may be able to sync from the
genesis.</p>
<h2 id="rocksdb-issues"><a class="header" href="#rocksdb-issues">RocksDB Issues</a></h2>
<p>When using Docker Compose, Kubernetes or other container orchestrators to run a Node, it is important to follow the
rules regarding persistent volumes for <a href="guides/external-node/02_configuration.html#database">RocksDB instances</a>. Volumes must be attached to a
specific Node (effectively, to a specific Postgres database). Volumes <strong>must not</strong> be shared across nodes at the same
time (this may lead to RocksDB corruption), and <strong>must not</strong> be transferred from one node to another.</p>
<p>A symptom of incorrect persistent volume usage is a Node crashing on start with an error like this appearing in the
logs:</p>
<pre><code class="language-text">ERROR zksync_node_framework::service: Task oneshot_runner failed: Oneshot task state_keeper/rocksdb_catchup_task failed
Caused by:
    0: Failed to catch up RocksDB to Postgres
    1: L1 batch number in state keeper cache (5153) is greater than the requested batch number (4652)
</code></pre>
<p>(Obviously, L1 batch numbers may differ.)</p>
<p>To fix the issue, allocate an exclusive persistent volume for the Node, ensure that the volume is empty and restart the
Node.</p>
<h2 id="logs"><a class="header" href="#logs">Logs</a></h2>
<p><em>Note: logs with the <code>error</code> level are reported to Sentry if it’s configured. If you notice unneeded alerts there that
you don’t consider actionable, you may disable logs for a component by tweaking the configuration.</em></p>
<div class="table-wrapper"><table><thead><tr><th>Level</th><th>Log substring</th><th>Interpretation</th></tr></thead><tbody>
<tr><td>ERROR</td><td>“One of the tokio actors unexpectedly finished”</td><td>One of the components crashed, and the node is restarting.</td></tr>
<tr><td>WARN</td><td>“Stop signal received, <component> is shutting down”</td><td>Satellite log of the message above</td></tr>
<tr><td>ERROR</td><td>“A lot of requests to the remote API failed in a row”</td><td>The remote API used to update token lists is probably down. Logs should disappear once API is available.</td></tr>
<tr><td>WARN</td><td>“Server returned an error status code: 429”</td><td>The main API rate limits are too strict. <a href="https://zksync.io/contact">Contact</a> Matter Labs to discuss the situation.</td></tr>
<tr><td>WARN</td><td>“Following transport error occurred”</td><td>There was a problem with fetching data from the main node.</td></tr>
<tr><td>WARN</td><td>“Unable to get the gas price”</td><td>There was a problem with fetching data from the main node.</td></tr>
<tr><td>WARN</td><td>“Consistency checker error”</td><td>There are problems querying L1, check the Web3 URL you specified in the config.</td></tr>
<tr><td>WARN</td><td>“Reorg detected”</td><td>Reorg was detected on the main node, the Node will rollback and restart</td></tr>
</tbody></table>
</div>
<p>Same as with panics, normally it’s only a problem if a WARN+ level log appears many times in a row.</p>
<h2 id="metrics-anomalies"><a class="header" href="#metrics-anomalies">Metrics anomalies</a></h2>
<p>The following common anomalies can be discovered by observing metrics <em>after the tree is rebuilt to match the DB
snapshot</em>:</p>
<ul>
<li><code>external_node_sync_lag</code> doesn’t decrease and <code>external_node_action_queue_action_queue_size</code> is near 0. Cause: The
fetcher can’t fetch new blocks quickly enough. Most likely, the network connection is too slow.</li>
<li><code>external_node_sync_lag</code> doesn’t decrease and <code>external_node_action_queue_action_queue_size</code> is at some high level.
Cause: The State Keeper doesn’t process fetched data quickly enough. Most likely, a more powerful CPU is needed.</li>
<li><code>sql_connection_acquire</code> skyrockets. Probably, there are not enough connections in the pool to match the demand.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-components"><a class="header" href="#node-components">Node components</a></h1>
<p>This section contains an overview of the EN’s main components.</p>
<h2 id="api"><a class="header" href="#api">API</a></h2>
<p>The Node can serve both the HTTP and the WS Web3 API, as well as PubSub. Whenever possible, it provides data based on
the local state, with a few exceptions:</p>
<ul>
<li>Submitting transactions: Since it is a read replica, submitted transactions are proxied to the main node, and the
response is returned from the main node. -<a href="guides/external-node/06_components.html">06_components.md</a> Querying transactions: The Node is not
aware of the main node’s mempool, and it does not sync rejected transactions. Therefore, if a local lookup for a
transaction or its receipt fails, the Node will attempt the same query on the main node.</li>
</ul>
<p>Apart from these cases, the API does not depend on the main node. Even if the main node is temporarily unavailable, the
Node can continue to serve the state it has locally.</p>
<h2 id="fetcher"><a class="header" href="#fetcher">Fetcher</a></h2>
<p>The Fetcher component is responsible for maintaining synchronization between the Node and the main node. Its primary
task is to fetch new blocks in order to update the local chain state. However, its responsibilities extend beyond that.
For instance, the Fetcher is also responsible for keeping track of L1 batch statuses. This involves monitoring whether
locally applied batches have been committed, proven, or executed on L1.</p>
<p>It is worth noting that in addition to fetching the <em>state</em>, the Node also retrieves the L1 gas price from the main node
for the purpose of estimating fees for L2 transactions (since this also happens based on the local state). This
information is necessary to ensure that gas estimations are performed in the exact same manner as the main node, thereby
reducing the chances of a transaction not being included in a block.</p>
<h2 id="state-keeper--vm"><a class="header" href="#state-keeper--vm">State Keeper / VM</a></h2>
<p>The State Keeper component serves as the “sequencer” part of the node. It shares most of its functionality with the main
node, with one key distinction. The main node retrieves transactions from the mempool and has the authority to decide
when a specific L2 block or L1 batch should be sealed. On the other hand, the Node retrieves transactions from the queue
populated by the Fetcher and seals the corresponding blocks/batches based on the data obtained from the Fetcher queue.</p>
<p>The actual execution of batches takes place within the VM, which is identical in both the Main and Nodes.</p>
<h2 id="reorg-detector"><a class="header" href="#reorg-detector">Reorg Detector</a></h2>
<p>In a ZK Stack chain, it is theoretically possible for L1 batches to be reverted before the corresponding “execute”
operation is applied on L1, that is before the block is <a href="https://docs.zksync.io/zk-stack/concepts/finality">final</a>. Such situations are highly uncommon and
typically occur due to significant issues: e.g. a bug in the sequencer implementation preventing L1 batch commitment.
Prior to batch finality, the chain operator can perform a rollback, reverting one or more batches and restoring the
blockchain state to a previous point. Finalized batches cannot be reverted at all.</p>
<p>However, even though such situations are rare, the Node must handle them correctly.</p>
<p>To address this, the Node incorporates a Reorg Detector component. This module keeps track of all L1 batches that have
not yet been finalized. It compares the locally obtained state root hashes with those provided by the main node’s API.
If the root hashes for the latest available L1 batch do not match, the Reorg Detector searches for the specific L1 batch
responsible for the divergence. Subsequently, it rolls back the local state and restarts the node. Upon restart, the EN
resumes normal operation.</p>
<h2 id="consistency-checker"><a class="header" href="#consistency-checker">Consistency Checker</a></h2>
<p>The main node API serves as the primary source of information for the EN. However, relying solely on the API may not
provide sufficient security since the API data could potentially be incorrect due to various reasons. The primary source
of truth for the rollup system is the L1 smart contract. Therefore, to enhance the security of the EN, each L1 batch
undergoes cross-checking against the L1 smart contract by a component called the Consistency Checker.</p>
<p>When the Consistency Checker detects that a particular batch has been sent to L1, it recalculates a portion of the input
known as the “block commitment” for the L1 transaction. The block commitment contains crucial data such as the state
root and batch number, and is the same commitment that is used for generating a proof for the batch. The Consistency
Checker then compares the locally obtained commitment with the actual commitment sent to L1. If the data does not match,
it indicates a potential bug in either the main node or Node implementation or that the main node API has provided
incorrect data. In either case, the state of the Node cannot be trusted, and the Node enters a crash loop until the
issue is resolved.</p>
<h2 id="health-check-server"><a class="header" href="#health-check-server">Health check server</a></h2>
<p>The Node also exposes an additional server that returns HTTP 200 response when the Node is operating normally, and HTTP
503 response when some of the health checks don’t pass (e.g. when the Node is not fully initialized yet). This server
can be used, for example, to implement the readiness probe in an orchestration solution you use.</p>
<h2 id="data-availability-fetcher"><a class="header" href="#data-availability-fetcher">Data Availability fetcher</a></h2>
<p>The Data Availability fetcher is responsible for fetching the DA-related information from the main node. It is only used
in Validiums, where the pubdata is not stored on L1, but rather in a separate data availability layer.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="snapshots-recovery"><a class="header" href="#snapshots-recovery">Snapshots Recovery</a></h1>
<p>Instead of initializing a node using a Postgres dump, it’s possible to configure a node to recover from a protocol-level
snapshot. This process is much faster and requires much less storage. Postgres database of a mainnet node recovered from
a snapshot is less than 500GB. Note that without <a href="guides/external-node/08_pruning.html">pruning</a> enabled, the node state will continuously grow
at a rate about 15GB per day.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<p>A snapshot is effectively a point-in-time snapshot of the VM state at the end of a certain L1 batch. Snapshots are
created for the latest L1 batches periodically (roughly twice a day) and are stored in a public GCS bucket.</p>
<p>Recovery from a snapshot consists of several parts.</p>
<ul>
<li><strong>Postgres</strong> recovery is the initial stage. The node API is not functioning during this stage. The stage is expected
to take about 1 hour on the mainnet.</li>
<li><strong>Merkle tree</strong> recovery starts once Postgres is fully recovered. Merkle tree recovery can take about 3 hours on the
mainnet. Ordinarily, Merkle tree recovery is a blocker for node synchronization; i.e., the node will not process
blocks newer than the snapshot block until the Merkle tree is recovered. If the <a href="guides/external-node/09_treeless_mode.html">treeless mode</a>
is enabled, tree recovery is not performed, and the node will start catching up blocks immediately after Postgres
recovery. This is still true if the tree data fetcher is enabled <em>together</em> with a Merkle tree; tree recovery is
asynchronous in this case.</li>
<li>Recovering RocksDB-based <strong>VM state cache</strong> is concurrent with Merkle tree recovery and also depends on Postgres
recovery. It takes about 1 hour on the mainnet. Unlike Merkle tree recovery, VM state cache is not necessary for node
operation (the node will get the state from Postgres is if it is absent), although it considerably speeds up VM
execution.</li>
</ul>
<p>After Postgres recovery is completed, the node becomes operational, providing Web3 API etc. It still needs some time to
catch up executing blocks after the snapshot (i.e, roughly several hours worth of blocks / transactions). This may take
order of 1–2 hours on the mainnet. In total, recovery process and catch-up thus should take roughly 5–6 hours with a
Merkle tree, or 3–4 hours in the treeless mode / with a tree data fetcher.</p>
<h2 id="current-limitations"><a class="header" href="#current-limitations">Current limitations</a></h2>
<p>Nodes recovered from snapshot don’t have any historical data from before the recovery. There is currently no way to
back-fill this historic data. E.g., if a node has recovered from a snapshot for L1 batch 500,000; then, it will not have
data for L1 batches 499,999, 499,998, etc. The relevant Web3 methods, such as <code>eth_getBlockByNumber</code>, will return an
error mentioning the first locally retained block or L1 batch if queried this missing data. The same error messages are
used for <a href="guides/external-node/08_pruning.html">pruning</a> because logically, recovering from a snapshot is equivalent to pruning node storage to
the snapshot L1 batch.</p>
<h2 id="configuration-for-zksync-era"><a class="header" href="#configuration-for-zksync-era">Configuration (for ZKsync Era)</a></h2>
<p>To enable snapshot recovery on ZKsync mainnet, you need to set environment variables for a node before starting it for
the first time:</p>
<pre><code class="language-yaml">EN_SNAPSHOTS_RECOVERY_ENABLED: 'true'
EN_SNAPSHOTS_OBJECT_STORE_BUCKET_BASE_URL: 'zksync-era-mainnet-external-node-snapshots'
EN_SNAPSHOTS_OBJECT_STORE_MODE: 'GCSAnonymousReadOnly'
</code></pre>
<p>For the ZKsync Sepolia testnet, use:</p>
<pre><code class="language-yaml">EN_SNAPSHOTS_RECOVERY_ENABLED: 'true'
EN_SNAPSHOTS_OBJECT_STORE_BUCKET_BASE_URL: 'zksync-era-boojnet-external-node-snapshots'
EN_SNAPSHOTS_OBJECT_STORE_MODE: 'GCSAnonymousReadOnly'
</code></pre>
<p>For a working examples of a fully configured ZKsync Nodes recovering from snapshots, see
<a href="https://github.com/matter-labs/zksync-era/tree/main/docs/src/guides/external-node/docker-compose-examples">Docker Compose examples</a>
and <a href="guides/external-node/00_quick_start.html"><em>Quick Start</em></a>.</p>
<p>If a node is already recovered (does not matter whether from a snapshot or from a Postgres dump), setting these env
variables will have no effect; the node will never reset its state.</p>
<h2 id="monitoring-recovery"><a class="header" href="#monitoring-recovery">Monitoring recovery</a></h2>
<p>Snapshot recovery information is logged with the following targets:</p>
<ul>
<li><strong>Recovery orchestration:</strong> <code>zksync_external_node::init</code></li>
<li><strong>Postgres recovery:</strong> <code>zksync_snapshots_applier</code></li>
<li><strong>Merkle tree recovery:</strong> <code>zksync_metadata_calculator::recovery</code>, <code>zksync_merkle_tree::recovery</code></li>
</ul>
<p>An example of snapshot recovery logs during the first node start:</p>
<pre><code class="language-text">2024-06-20T07:25:32.466926Z  INFO zksync_external_node::init: Node has neither genesis L1 batch, nor snapshot recovery info
2024-06-20T07:25:32.466946Z  INFO zksync_external_node::init: Chosen node initialization strategy: SnapshotRecovery
2024-06-20T07:25:32.466951Z  WARN zksync_external_node::init: Proceeding with snapshot recovery. This is an experimental feature; use at your own risk
2024-06-20T07:25:32.475547Z  INFO zksync_snapshots_applier: Found snapshot with data up to L1 batch #7, L2 block #27, version 0, storage logs are divided into 10 chunk(s)
2024-06-20T07:25:32.516142Z  INFO zksync_snapshots_applier: Applied factory dependencies in 27.768291ms
2024-06-20T07:25:32.527363Z  INFO zksync_snapshots_applier: Recovering storage log chunks with 10 max concurrency
2024-06-20T07:25:32.608539Z  INFO zksync_snapshots_applier: Recovered 3007 storage logs in total; checking overall consistency...
2024-06-20T07:25:32.612967Z  INFO zksync_snapshots_applier: Retrieved 2 tokens from main node
2024-06-20T07:25:32.616142Z  INFO zksync_external_node::init: Recovered Postgres from snapshot in 148.523709ms
2024-06-20T07:25:32.645399Z  INFO zksync_metadata_calculator::recovery: Recovering Merkle tree from Postgres snapshot in 1 chunks with max concurrency 10
2024-06-20T07:25:32.650478Z  INFO zksync_metadata_calculator::recovery: Filtered recovered key chunks; 1 / 1 chunks remaining
2024-06-20T07:25:32.681327Z  INFO zksync_metadata_calculator::recovery: Recovered 1/1 Merkle tree chunks, there are 0 left to process
2024-06-20T07:25:32.784597Z  INFO zksync_metadata_calculator::recovery: Recovered Merkle tree from snapshot in 144.040125ms
</code></pre>
<p>(Obviously, timestamps and numbers in the logs will differ.)</p>
<p>Recovery logic also exports some metrics, the main of which are as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>snapshots_applier_storage_logs_chunks_left_to_process</code></td><td>Gauge</td><td>-</td><td>Number of storage log chunks left to process during Postgres recovery</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="pruning"><a class="header" href="#pruning">Pruning</a></h1>
<p>It is possible to configure a Node to periodically prune all data from L1 batches older than a configurable threshold.
Data is pruned both from Postgres and from tree (RocksDB). Pruning happens continuously (i.e., does not require stopping
the node) in the background during normal node operation. It is designed to not significantly impact node performance.</p>
<p>Types of pruned data in Postgres include:</p>
<ul>
<li>Block and L1 batch headers</li>
<li>Transactions</li>
<li>EVM logs aka events</li>
<li>Overwritten storage logs</li>
<li>Transaction traces</li>
</ul>
<p>Pruned data is no longer available via Web3 API of the node. The relevant Web3 methods, such as <code>eth_getBlockByNumber</code>,
will return an error mentioning the first retained block or L1 batch if queried pruned data.</p>
<h2 id="interaction-with-snapshot-recovery"><a class="header" href="#interaction-with-snapshot-recovery">Interaction with snapshot recovery</a></h2>
<p>Pruning and <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> are independent features. Pruning works both for archival
nodes restored from a Postgres dump, and nodes recovered from a snapshot. Conversely, a node recovered from a snapshot
may have pruning disabled; this would mean that it retains all data starting from the snapshot indefinitely (but not
earlier data, see <a href="guides/external-node/07_snapshots_recovery.html#current-limitations">snapshot recovery limitations</a>).</p>
<p>A rough guide whether to choose the recovery option and/or pruning is as follows:</p>
<ul>
<li>If you need a node with data retention period of up to a few days, set up a node from a snapshot with pruning enabled
and wait for it to have enough data.</li>
<li>If you need a node with the entire rollup history, using a Postgres dump is the only option, and pruning should be
disabled.</li>
<li>If you need a node with significant data retention (order of months), the best option right now is using a Postgres
dump. You may enable pruning for such a node, but beware that full pruning may take significant amount of time (order
of weeks or months). In the future, we intend to offer pre-pruned Postgres dumps with a few months of data.</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>You can enable pruning by setting the environment variable</p>
<pre><code class="language-yaml">EN_PRUNING_ENABLED: 'true'
</code></pre>
<p>By default, the node will keep L1 batch data for 7 days determined by the batch timestamp (always equal to the timestamp
of the first block in the batch). You can configure the retention period using:</p>
<pre><code class="language-yaml">EN_PRUNING_DATA_RETENTION_SEC: '259200' # 3 days
</code></pre>
<p>The retention period can be set to any value, but for mainnet values under 24h will be ignored because a batch can only
be pruned after it has been executed on Ethereum.</p>
<p>Pruning can be disabled or enabled and the data retention period can be freely changed during the node lifetime.</p>
<div id="admonition-warning" class="admonition admonish-warning" role="note" aria-labelledby="admonition-warning-title">
<div class="admonition-title">
<div id="admonition-warning-title">
<p>Warning</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/08_pruning.html#admonition-warning"></a>
</div>
<div>
<p>Pruning should be disabled when recovering the Merkle tree (e.g., if a node ran in
<a href="guides/external-node/09_treeless_mode.html">the treeless mode</a> before, or if its tree needs a reset for whatever reason). Otherwise, tree
recovery will with almost definitely result in an error, or worse, in a corrupted tree.</p>
</div>
</div>
<h2 id="storage-requirements-for-pruned-nodes"><a class="header" href="#storage-requirements-for-pruned-nodes">Storage requirements for pruned nodes</a></h2>
<p>The storage requirements depend on how long you configure to retain the data, but are roughly:</p>
<ul>
<li><strong>40GB + ~5GB/day of retained data</strong> of disk space needed on machine that runs the node</li>
<li><strong>300GB + ~15GB/day of retained data</strong> of disk space for Postgres</li>
</ul>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/08_pruning.html#admonition-note"></a>
</div>
<div>
<p>When pruning an existing archival node, Postgres will be unable to reclaim disk space automatically. To reclaim disk
space, you need to manually run <code>VACUUM FULL</code>, which requires an <code>ACCESS EXCLUSIVE</code> lock. You can read more about it
in <a href="https://www.postgresql.org/docs/current/sql-vacuum.html">Postgres docs</a>.</p>
</div>
</div>
<h2 id="monitoring-pruning"><a class="header" href="#monitoring-pruning">Monitoring pruning</a></h2>
<p>Pruning information is logged with the following targets:</p>
<ul>
<li><strong>Postgres pruning:</strong> <code>zksync_node_db_pruner</code></li>
<li><strong>Merkle tree pruning:</strong> <code>zksync_metadata_calculator::pruning</code>, <code>zksync_merkle_tree::pruning</code>.</li>
</ul>
<p>To check whether Postgres pruning works as intended, you should look for logs like this:</p>
<pre><code class="language-text">2024-06-20T07:26:03.415382Z  INFO zksync_node_db_pruner: Soft pruned db l1_batches up to 8 and L2 blocks up to 29, operation took 14.850042ms
2024-06-20T07:26:04.433574Z  INFO zksync_node_db_pruner::metrics: Performed pruning of database, deleted 1 L1 batches, 2 L2 blocks, 68 storage logs, 383 events, 27 call traces, 12 L2-to-L1 logs
2024-06-20T07:26:04.436516Z  INFO zksync_node_db_pruner: Hard pruned db l1_batches up to 8 and L2 blocks up to 29, operation took 18.653083ms
</code></pre>
<p>(Obviously, timestamps and numbers in the logs will differ.)</p>
<p>Pruning logic also exports some metrics, the main of which are as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>db_pruner_not_pruned_l1_batches_count</code></td><td>Gauge</td><td>-</td><td>Number of retained L1 batches</td></tr>
<tr><td><code>db_pruner_pruning_chunk_duration_seconds</code></td><td>Histogram</td><td><code>prune_type</code></td><td>Latency of a single pruning iteration</td></tr>
<tr><td><code>merkle_tree_pruning_deleted_stale_key_versions</code></td><td>Gauge</td><td><code>bound</code></td><td>Versions (= L1 batches) pruned from the Merkle tree</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="treeless-operation-mode"><a class="header" href="#treeless-operation-mode">Treeless Operation Mode</a></h1>
<p>Normally, a Node needs to run the Merkle tree component (aka <em>metadata calculator</em>) in order to compute L1 batch state
root hashes. A state root hash from the previous batch can be accessed by L2 contracts, so processing transactions in an
L1 batch cannot start until the state root hash of the previous L1 batch is computed. Merkle tree requires non-trivial
storage space and RAM (roughly 3 TB and 32 GB respectively for an archival mainnet node as of July 2024). While storage
and RAM requirements can be significantly lowered with <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> and
<a href="guides/external-node/08_pruning.html">pruning</a>, <strong>treeless operation mode</strong> allows to run a node without a local Merkle tree instance at all.</p>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How it works</a></h2>
<p>The relevant logic is encapsulated in the <em>tree fetcher</em> component that can be run instead, or concurrently with the
Merkle tree component. Tree fetcher continuously loads L1 batch state root hashes for batches persisted to the node
storage. It uses the following 2 sources of hashes:</p>
<ul>
<li>The rollup contract on L1 (e.g. on the Ethereum mainnet for the Era mainnet). The state root hash for a batch is
submitted to this contract as a part of the batch commitment.</li>
<li>Main L2 node (or more generally, the L2 node that the current node is configured to sync from). Only used if the L1
data source does not work (e.g., very recent L1 batches may be not yet committed to L1).</li>
</ul>
<p>If the tree fetcher run concurrently to the Merkle tree, the tree will still compute state root hashes for all batches.
If the tree is slower than the fetcher (which is expected in most cases), it will compare the computed hash against the
state root hash from the tree fetcher and crash on a mismatch.</p>
<h2 id="tradeoffs"><a class="header" href="#tradeoffs">Tradeoffs</a></h2>
<ul>
<li>Tree fetcher requires limited trust to the L1 Web3 provider and the main L2 node (note that trust in them is required
for other node components, such as <a href="guides/external-node/06_components.html#consistency-checker">the consistency checker</a> and
<a href="guides/external-node/06_components.html#reorg-detector">reorg detector</a>). This trust is limited in time; mismatched L1 batch root hashes
will eventually be detected by the 2 aforementioned components and the Merkle tree (if it is run concurrently).</li>
<li>Tree fetcher only loads root hashes of the Merkle tree, not other tree data. That is, it cannot replace the Merkle
tree if a node needs to serve the <code>zks_getProof</code> endpoint, since it fetches proofs from the Merkle tree.</li>
</ul>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>The tree fetcher is disabled by default. You can enable it by specifying <code>tree_fetcher</code> in the list of components that a
node should run in the <code>--components</code> command-line arg. For example, to run all standard components and the tree
fetcher:</p>
<pre><code class="language-shell"># Assume that the node binary in in $PATH
zksync_external_node --components=all,tree_fetcher
</code></pre>
<p>To run all standard components without the Merkle tree and the tree fetcher:</p>
<pre><code class="language-shell">zksync_external_node --components=core,api,tree_fetcher
</code></pre>
<p>The tree fetcher currently does not have configurable parameters.</p>
<p>The tree fetcher can be freely switched on or off during the node lifetime; i.e., it’s not required to commit to running
or not running it when initializing a node.</p>
<div id="admonition-tip" class="admonition admonish-tip" role="note" aria-labelledby="admonition-tip-title">
<div class="admonition-title">
<div id="admonition-tip-title">
<p>Tip</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/09_treeless_mode.html#admonition-tip"></a>
</div>
<div>
<p>Switching on the tree fetcher during <a href="guides/external-node/07_snapshots_recovery.html">snapshot recovery</a> can significantly speed it up
(order of 2–3 hours for the mainnet) because the node no longer needs to recover the Merkle tree before starting
catching up.</p>
</div>
</div>
<h2 id="monitoring-tree-fetcher"><a class="header" href="#monitoring-tree-fetcher">Monitoring tree fetcher</a></h2>
<p>Tree fetcher information is logged with the <code>zksync_node_sync::tree_data_fetcher</code> target.</p>
<p>Tree fetcher exports some metrics:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>external_node_tree_data_fetcher_last_updated_batch_number</code></td><td>Gauge</td><td>-</td><td>Last L1 batch with tree data updated by the fetcher</td></tr>
<tr><td><code>external_node_tree_data_fetcher_step_outcomes</code></td><td>Counter</td><td><code>kind</code></td><td>Number of times a fetcher step resulted in a certain outcome (e.g., update, no-op, or transient error)</td></tr>
<tr><td><code>external_node_tree_data_fetcher_root_hash_sources</code></td><td>Counter</td><td><code>source</code></td><td>Number of root hashes fetched from a particular source (L1 or L2).</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="decentralization-1"><a class="header" href="#decentralization-1">Decentralization</a></h1>
<p>In the default setup, the Node will fetch data from the ZKsync API endpoint maintained by Matter Labs. To reduce the
reliance on this centralized endpoint we have developed a decentralized p2p networking stack (aka gossipnet) which will
eventually be used instead of ZKsync API for synchronizing data.</p>
<p>On the gossipnet, the data integrity will be protected by the BFT (byzantine fault-tolerant) consensus algorithm
(currently data is signed just by the main node though).</p>
<h2 id="enabling-gossipnet-on-your-node"><a class="header" href="#enabling-gossipnet-on-your-node">Enabling gossipnet on your node</a></h2>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/10_decentralization.html#admonition-note"></a>
</div>
<div>
<p>The minimal supported server version for this is
<a href="https://github.com/matter-labs/zksync-era/releases/tag/core-v24.11.0">24.11.0</a></p>
</div>
</div>
<h3 id="generating-secrets"><a class="header" href="#generating-secrets">Generating secrets</a></h3>
<p>Each participant node of the gossipnet has to have an identity (a public/secret key pair). When running your node for
the first time, generate the secrets by running:</p>
<pre><code>docker run --entrypoint /usr/bin/zksync_external_node "matterlabs/external-node:2.0-v25.1.0" generate-secrets &gt; consensus_secrets.yaml
chmod 600 consensus_secrets.yaml
</code></pre>
<div id="admonition-danger" class="admonition admonish-danger" role="note" aria-labelledby="admonition-danger-title">
<div class="admonition-title">
<div id="admonition-danger-title">
<p>Danger</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/10_decentralization.html#admonition-danger"></a>
</div>
<div>
<p>NEVER reveal the secret keys used by your node. Otherwise, someone can impersonate your node on the gossipnet. If you
suspect that your secret key has been leaked, you can generate fresh keys using the same tool.</p>
<p>If you want someone else to connect to your node, give them your PUBLIC key instead. Both public and secret keys are
present in the <code>consensus_secrets.yaml</code> (public keys are in comments).</p>
</div>
</div>
<h3 id="preparing-configuration-file"><a class="header" href="#preparing-configuration-file">Preparing configuration file</a></h3>
<p>Copy the template of the consensus configuration file (for
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/src/guides/external-node/prepared_configs/mainnet_consensus_config.yaml">mainnet</a>
or
<a href="https://github.com/matter-labs/zksync-era/blob/main/docs/src/guides/external-node/prepared_configs/testnet_consensus_config.yaml">testnet</a>
).</p>
<div id="admonition-note-1" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/10_decentralization.html#admonition-note-1"></a>
</div>
<div>
<p>You need to fill in the <code>public_addr</code> field. This is the address that will (not implemented yet) be advertised over
gossipnet to other nodes, so that they can establish connections to your node. If you don’t want to expose your node
to the public internet, you can use IP in your local network.</p>
</div>
</div>
<p>Currently the config contains the following fields (refer to config
<a href="https://github.com/matter-labs/zksync-era/blob/990676c5f84afd2ff8cd337f495c82e8d1f305a4/core/lib/protobuf_config/src/proto/core/consensus.proto#L66">schema</a>
for more details):</p>
<ul>
<li><code>server_addr</code> - local TCP socket address that the node should listen on for incoming connections. Note that this is an
additional TCP port that will be opened by the node.</li>
<li><code>public_addr</code> - the public address of your node that will be advertised over the gossipnet.</li>
<li><code>max_payload_size</code> - limit (in bytes) on the sized of the ZKsync ERA block received from the gossipnet. This protects
your node from getting DoS`ed by too large network messages. Use the value from the template.</li>
<li><code>gossip_dynamic_inbound_limit</code> - maximal number of unauthenticated concurrent inbound connections that can be
established to your node. This is a DDoS protection measure.</li>
</ul>
<h3 id="setting-environment-variables"><a class="header" href="#setting-environment-variables">Setting environment variables</a></h3>
<p>Uncomment (or add) the following lines in your <code>.env</code> config:</p>
<pre><code>EN_CONSENSUS_CONFIG_PATH=...
EN_CONSENSUS_SECRETS_PATH=...
</code></pre>
<p>These variables should point to your consensus config and secrets files that we have just created. Tweak the paths to
the files if you have placed them differently.</p>
<h3 id="add---enable-consensus-flag-to-your-entry-point-command"><a class="header" href="#add---enable-consensus-flag-to-your-entry-point-command">Add <code>--enable-consensus</code> flag to your entry point command</a></h3>
<p>For the consensus configuration to take effect you have to add <code>--enable-consensus</code> flag to the command line when
running the node, for example:</p>
<pre><code>docker run "matterlabs/external-node:2.0-v24.12.0" &lt;all the other flags&gt; --enable-consensus
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="steps-to-modify-the-docker-compose-files-to-support-other-chains"><a class="header" href="#steps-to-modify-the-docker-compose-files-to-support-other-chains">Steps to modify the docker-compose files to support Other Chains</a></h1>
<p>Below are the steps for adjusting ZKsync Era docker-compose files from <a href="guides/external-node/00_quick_start.html">here</a> to support chains other
than ZKsync Era.</p>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="guides/external-node/11_setup_for_other_chains.html#admonition-note"></a>
</div>
<div>
<p>If you want to run Node for a given chain, you can first ask the company hosting the chains for the Dockerfiles.</p>
</div>
</div>
<h2 id="1-update-en_l2_chain_id"><a class="header" href="#1-update-en_l2_chain_id">1. Update <code>EN_L2_CHAIN_ID</code></a></h2>
<p>The <code>EN_L2_CHAIN_ID</code> environment variable specifies the Layer 2 chain ID of the blockchain.</p>
<p>You can get it using main node rpc call <code>eth_chainId</code> or by asking the company hosting the chain. For example:</p>
<pre><code>curl -X POST https://mainnet.era.zksync.io \
-H "Content-Type: application/json" \
-d '{"jsonrpc": "2.0", "method": "eth_chainId", "params": [], "id": 1}'
</code></pre>
<p>returns</p>
<pre><code>{ "jsonrpc": "2.0", "result": "0x144", "id": 1}
</code></pre>
<p>where <code>0x144</code> is the chain ID (324 in decimal)</p>
<h2 id="2-update-en_main_node_url"><a class="header" href="#2-update-en_main_node_url">2. Update <code>EN_MAIN_NODE_URL</code></a></h2>
<p>The <code>EN_MAIN_NODE_URL</code> The EN_MAIN_NODE_URL environment variable should point to the main node URL of the target chain</p>
<h2 id="3-update-snapshots-recovery-settings"><a class="header" href="#3-update-snapshots-recovery-settings">3. Update snapshots recovery settings</a></h2>
<p>Snapshots recovery is a feature that allows faster Node startup at the cost of no transaction history. By default the
ZKsync Era docker-compose file has this feature enabled, but it’s only recommended to use if the Node first startup time
is too slow. It can be disabled by changing <code>EN_SNAPSHOTS_RECOVERY_ENABLED</code> to <code>false</code></p>
<p>If you want to keep this feature enabled for a Node, ask the company hosting the chain for the bucket name where the
snapshots are stored and update the value of <code>EN_SNAPSHOTS_OBJECT_STORE_BUCKET_BASE_URL</code></p>
<h2 id="4-disable-consensus"><a class="header" href="#4-disable-consensus">4. Disable consensus</a></h2>
<p>Chains other than ZKsync Era aren’t currently running consensus(as of December 2024). You need to disable it by removing
<code>--enable-consensus</code> flag from <code>entrypoint.sh</code> invocation in docker-compose</p>
<h2 id="5-validium-chains-only-enable-and-configure-da-fetcher"><a class="header" href="#5-validium-chains-only-enable-and-configure-da-fetcher">5. (Validium chains only) Enable and configure DA fetcher</a></h2>
<p>For Validium ENs to function properly, the Data Availability fetcher must be enabled and configured. It has to be added
to the components list, e.g. for a standard list of components and DA fetcher it would be <code>--components=all,da_fetcher</code>.</p>
<p>To configure the DA fetcher, you need to add the <code>da_client</code> config if the file-based config is used or configure in via
the environment variables, they need to have an <code>EN_</code> prefix. If the DA client in use needs a secret to be configured -
you need to set it in your secrets config or a corresponding environment variable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h1>
<p>The goal of the ZK Stack is to power the internet of value. Value needs to be secured, and only blockchains are able
provide the level of security that the internet needs. The ZK Stack can be used to launch zero-knowledge rollups, which
are extra secure blockchains.</p>
<p>ZK Rollups use advanced mathematics called zero-knowledge proofs to show that the execution of the rollup was done
correctly. They also send (“roll up”) their data to another chain, in our case this is Ethereum. The ZK Stack uses the
zkEVM to execute transactions, making it Ethereum compatible.</p>
<p>These two techniques allow the rollup to be verified externally. Unlike traditional blockchains, where you have to run a
node to verify all transactions, the state of the rollup can be easily checked by external participants by validating
the proof.</p>
<p>These external validators of a rollup can be other rollups. This means we can connect rollups trustlessly, and create a
network of rollups. This network is called the ZK Chain ecosystem.</p>
<p>These specs will provide a high level overview of the zkEVM and a full specification of its more technical components,
such as the prover, compiler, and the VM itself. We also specify the foundations of the ZK Chain ecosystem.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="overview-3"><a class="header" href="#overview-3">Overview</a></h1>
<p>As stated in the introduction, the ZK Stack can be used to launch rollups. These rollups have some operators that are
needed to run it, these are the sequencer and the prover, they create blocks and proofs, and submit them to the L1
contract.</p>
<p>A user submits their transaction to the sequencer. The job of the sequencer is to collect transactions and execute them
using the zkEVM, and to provide a soft confirmation to the user that their transaction was executed. If the user chooses
they can force the sequencer to include their transaction by submitting it via L1. After the sequencer executes the
block, it sends it over to the prover, who creates a cryptographic proof of the block’s execution. This proof is then
sent to the L1 contract alongside the necessary data. On the L1 a <a href="specs/./l1_smart_contracts.html">smart contract</a> verifies
that the proof is valid and all the data has been submitted, and the rollup’s state is also updated in the contract.</p>
<p><img src="specs/./img/L2_Components.png" alt="Components" /></p>
<p>The core of this mechanism was the execution of transactions. The ZK Stack uses the zkEVM for this, which is similar to
the EVM, but its role is different than the EVM’s role in Ethereum.</p>
<p>Transactions can also be submitted via L1. This happens via the same process that allows
<a href="specs/./contracts/settlement_contracts/priority_queue/l1_l2_communication/l1_to_l2.html">L1&lt;&gt;L2 communication</a>. This method
provides the rollup with censorship resistance, and allows trustless bridges to the L1.</p>
<p>The sequencer collects transactions into blocks [blocks][TODO], similarly to Ethereum. To provide the best UX the
protocol has small blocks with quick soft confirmations for the users. Unlike Ethereum, the zkEVM does not just have
blocks, but also batches, which are just a collection of blocks. A batch is the unit that the prover processes.</p>
<p>Before we submit a proof we send the <a href="specs/./contracts/settlement_contracts/data_availability/README.html">data</a> to L1. Instead
of submitting the data of each transaction, we submit how the state of the blockchain changes, this change is called the
state diff. This approach allows the transactions that change the same storage slots to be very cheap, since these
transactions don’t incur additional data costs.</p>
<p>Finally at the end of the process, we create the proofs and send them to L1. Our Boojum proof system provides excellent
performance, and can be run on just 16Gb of GPU RAM. This will enable the proof generation to be truly decentralized.</p>
<p>Up to this point we have only talked about a single chain. We will connect these chains into a single ecosystem, called
[ZK Chain ecosystem][TODO].</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blocks--batches---how-we-package-transactions"><a class="header" href="#blocks--batches---how-we-package-transactions">Blocks &amp; Batches - How we package transactions</a></h1>
<p>In this article, we will explore the processing of transactions, how we group them into blocks, what it means to “seal”
a block, and why it is important to have rollbacks in our virtual machine (VM).</p>
<p>At the basic level, we have individual transactions. However, to execute them more efficiently, we group them together
into blocks &amp; batches.</p>
<h2 id="l1-batch-vs-l2-block-aka-miniblock-vs-transaction"><a class="header" href="#l1-batch-vs-l2-block-aka-miniblock-vs-transaction">L1 Batch vs L2 Block (a.k.a MiniBlock) vs Transaction</a></h2>
<p>To help visualize the concept, here are two images:</p>
<p><img src="https://user-images.githubusercontent.com/128217157/236494232-aeed380c-78f6-4fda-ab2a-8de26c1089ff.png" alt="Block layout" title="block layout" /></p>
<p>You can refer to the Block layout image to see how the blocks are organized. It provides a graphical representation of
how transactions are arranged within the blocks and the arrangement of L2 blocks within L1 “batches.”</p>
<p><img src="https://user-images.githubusercontent.com/128217157/236500717-165470ad-30b8-4ad6-97ed-fc29c8eb1fe0.png" alt="Explorer example" title="explorer example" /></p>
<h3 id="l2-blocks-aka-miniblocks"><a class="header" href="#l2-blocks-aka-miniblocks">L2 blocks (aka Miniblocks)</a></h3>
<p>Currently, the L2 blocks do not have a major role in the system, until we transition to a decentralized sequencer. We
introduced them mainly as a “compatibility feature” to accommodate various tools, such as Metamask, which expect a block
that changes frequently. This allows these tools to provide feedback to users, confirming that their transaction has
been added.</p>
<p>As of now, an L2 block is created every 2 seconds (controlled by StateKeeper’s config <code>miniblock_commit_deadline_ms</code>),
and it includes all the transactions received during that time period. This periodic creation of L2 blocks ensures that
transactions are processed and included in the blocks regularly.</p>
<h3 id="l1-batches"><a class="header" href="#l1-batches">L1 batches</a></h3>
<p>L1 batches play a crucial role because they serve as the fundamental unit for generating proofs. From the perspective of
the virtual machine (VM), each L1 batch represents the execution of a single program, specifically the Bootloader. The
Bootloader internally processes all the transactions belonging to that particular batch. Therefore, the L1 batch serves
as the container for executing the program and handling the transactions within it.</p>
<h4 id="so-how-large-can-l1-batch-be"><a class="header" href="#so-how-large-can-l1-batch-be">So how large can L1 batch be</a></h4>
<p>Most blockchains use factors like time and gas usage to determine when a block should be closed or sealed. However, our
case is a bit more complex because we also need to consider prover capacity and limits related to publishing to L1.</p>
<p>The decision of when to seal the block is handled by the code in the <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/seal_criteria/conditional_sealer.rs#20" title="Conditional Sealer">conditional_sealer</a> module. It
maintains a list of <code>SealCriterion</code> and at the time of writing this article, <a href="https://github.com/matter-labs/zksync-era/blob/main/core/lib/zksync_core/src/state_keeper/seal_criteria/mod.rs#L106" title="Reasons for Sealing">we have 9 reasons to seal the
block</a>, which include:</p>
<ul>
<li>Transaction slots limit (currently set to 750 transactions in <code>StateKeeper</code>’s config - <code>transaction_slots</code>).</li>
<li>Gas limit (currently set to <code>MAX_L2_TX_GAS_LIMIT</code> = 80M).</li>
<li>Published data limit (as each L1 batch must publish information about the changed slots to L1, so all the changes must
fit within the L1 transaction limit, currently set to <code>MAX_PUBDATA_PER_L1_BATCH</code>= 120k).</li>
<li>zkEVM Geometry limits - For certain operations like merklelization, there is a maximum number of circuits that can be
included in a single L1 batch. If this limit is exceeded, we wouldn’t be able to generate the proof.</li>
</ul>
<p>We also have a <code>TimeoutCriterion</code> - but it is not enabled.</p>
<p>However, these sealing criteria pose a significant challenge because it is difficult to predict in advance whether
adding a given transaction to the current batch will exceed the limits or not. This unpredictability adds complexity to
the process of determining when to seal the block.</p>
<h4 id="what-if-a-transaction-doesnt-fit"><a class="header" href="#what-if-a-transaction-doesnt-fit">What if a transaction doesn’t fit</a></h4>
<p>To handle situations where a transaction exceeds the limits of the currently active L1 batch, we employ a “try and
rollback” approach. This means that we attempt to add the transaction to the active L1 batch, and if we receive a
<code>ExcludeAndSeal</code> response indicating that it doesn’t fit, we roll back the virtual machine (VM) to the state before the
transaction was attempted.</p>
<p>Implementing this approach introduces a significant amount of complexity in the <code>oracles</code> (also known as interfaces) of
the VM. These oracles need to support snapshotting and rolling back operations to ensure consistency when handling
transactions that don’t fit.</p>
<p>In a separate article, we will delve into more details about how these oracles and the VM work, providing a
comprehensive understanding of their functionality and interactions.</p>
<h2 id="deeper-dive"><a class="header" href="#deeper-dive">Deeper dive</a></h2>
<h3 id="glossary"><a class="header" href="#glossary">Glossary</a></h3>
<ul>
<li>Batch - a set of transactions that the bootloader processes (<code>commitBatches</code>, <code>proveBatches</code>,
and <code>executeBatches</code> work with it). A batch consists of multiple transactions.</li>
<li>L2 block - a non-intersecting sub-set of consecutively executed transactions. This is the kind of block you see in the
API. This is the one that will <em>eventually</em> be used for <code>block.number</code>/<code>block.timestamp</code>/etc. This will happen
<em>eventually</em>, since at the time of this writing the virtual block migration is being
<a href="specs/blocks_batches.html#migration--virtual-blocks-logic">run</a>.</li>
<li>Virtual block — blocks the data of which will be returned in the contract execution environment during the migration.
They are called “virtual”, since they have no trace in our API, i.e. it is not possible to query information about
them in any way.</li>
</ul>
<h3 id="motivation-2"><a class="header" href="#motivation-2">Motivation</a></h3>
<p>Before the recent upgrade, <code>block.number</code>, <code>block.timestamp</code>, as well as <code>blockhash</code> in Solidity, returned information
about <em>batches</em>, i.e. large blocks that are proven on L1 and which consist of many small L2 blocks. At the same time,
API returns <code>block.number</code> and <code>block.timestamp</code> as for L2 blocks.</p>
<p>L2 blocks were created for fast soft confirmation on wallets and block explorer. For example, MetaMask shows
transactions as confirmed only after the block in which transaction execution was mined. So if the user needs to wait
for the batch confirmation it would take at least minutes (for soft confirmation) and hours for full confirmation which
is very bad UX. But API could return soft confirmation much earlier through L2 blocks.</p>
<p>There was a huge outcry in the community for us to return the information for L2 blocks in <code>block.number</code>,
<code>block.timestamp</code>, as well as <code>blockhash</code>, because of discrepancy of runtime execution and returned data by API.</p>
<p>However, there were over 15mln L2 blocks, while less than 200k batches, meaning that if we simply “switched” from
returning L1 batches’ info to L2 block’s info, some contracts (especially those that use <code>block.number</code> for measuring
time intervals instead of <code>block.timestamp</code>) would break. For that, we decided to have an accelerated migration process,
i.e. the <code>block.number</code> will grow faster and faster, until it becomes roughly 8x times the L2 block production speed,
allowing it to gradually reach the L2 block number, after which the information on the L2 <code>block.number</code> will be
returned. The blocks the info of which will be returned during this process are called “virtual blocks”. Their
information will never be available in any of our APIs, which should not be a major breaking change, since our API
already mostly works with L2 blocks, while L1 batches’s information is returned in the runtime.</p>
<h3 id="adapting-for-solidity"><a class="header" href="#adapting-for-solidity">Adapting for Solidity</a></h3>
<p>In order to get the returned value for <code>block.number</code>, <code>block.timestamp</code>, <code>blockhash</code> our compiler used the following
functions:</p>
<ul>
<li><code>getBlockNumber</code></li>
<li><code>getBlockTimestamp</code></li>
<li><code>getBlockHashEVM</code></li>
</ul>
<p>During the migration process, these will return the values of the virtual blocks. After the migration is complete, they
will return values for L2 blocks.</p>
<h3 id="migration-status"><a class="header" href="#migration-status">Migration status</a></h3>
<p>At the time of this writing, the migration has been complete on testnet, i.e. there we already have only the L2 block
information returned. However, the <a href="https://github.com/zkSync-Community-Hub/zksync-developers/discussions/87">migration</a>
on mainnet is still ongoing and most likely will end on late October / early November.</p>
<h2 id="blocks-processing-and-consistency-checks"><a class="header" href="#blocks-processing-and-consistency-checks">Blocks’ processing and consistency checks</a></h2>
<p>Our <code>SystemContext</code> contract allows to get information about batches and L2 blocks. Some of the information is hard to
calculate onchain. For instance, time. The timing information (for both batches and L2 blocks) are provided by the
operator. In order to check that the operator provided some realistic values, certain checks are done on L1. Generally
though, we try to check as much as we can on L2.</p>
<h2 id="initializing-l1-batch"><a class="header" href="#initializing-l1-batch">Initializing L1 batch</a></h2>
<p>At the start of the batch, the operator
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3636">provides</a>
the timestamp of the batch, its number and the hash of the previous batch.. The root hash of the Merkle tree serves as
the root hash of the batch.</p>
<p>The SystemContext can immediately check whether the provided number is the correct batch number. It also immediately
sends the previous batch hash to L1, where it will be checked during the commit operation. Also, some general
consistency checks are performed. This logic can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L416">here</a>.</p>
<h2 id="l2-blocks-processing-and-consistency-checks"><a class="header" href="#l2-blocks-processing-and-consistency-checks">L2 blocks processing and consistency checks</a></h2>
<h3 id="setl2block"><a class="header" href="#setl2block"><code>setL2Block</code></a></h3>
<p>Before each transaction, we call <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2605">method</a>.
There we will provide some data about the L2 block that the transaction belongs to:</p>
<ul>
<li><code>_l2BlockNumber</code> The number of the new L2 block.</li>
<li><code>_l2BlockTimestamp</code> The timestamp of the new L2 block.</li>
<li><code>_expectedPrevL2BlockHash</code> The expected hash of the previous L2 block.</li>
<li><code>_isFirstInBatch</code> Whether this method is called for the first time in the batch.</li>
<li><code>_maxVirtualBlocksToCreate</code> The maximum number of virtual block to create with this L2 block.</li>
</ul>
<p>If two transactions belong to the same L2 block, only the first one may have non-zero <code>_maxVirtualBlocksToCreate</code>. The
rest of the data must be same.</p>
<p>The <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L312">performs</a>
a lot of similar consistency checks to the ones for the L1 batch.</p>
<h3 id="l2-blockhash-calculation-and-storage"><a class="header" href="#l2-blockhash-calculation-and-storage">L2 blockhash calculation and storage</a></h3>
<p>Unlike L1 batch’s hash, the L2 blocks’ hashes can be checked on L2.</p>
<p>The hash of an L2 block is
<code>keccak256(abi.encode(_blockNumber, _blockTimestamp, _prevL2BlockHash, _blockTxsRollingHash))</code>. Where
<code>_blockTxsRollingHash</code> is defined in the following way:</p>
<p><code>_blockTxsRollingHash = 0</code> for an empty block.</p>
<p><code>_blockTxsRollingHash = keccak(0, tx1_hash)</code> for a block with one tx.</p>
<p><code>_blockTxsRollingHash = keccak(keccak(0, tx1_hash), tx2_hash)</code> for a block with two txs, etc.</p>
<p>To add a transaction hash to the current miniblock we use the <code>appendTransactionToCurrentL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L373">function</a>.</p>
<p>Since ZKsync is a state-diff based rollup, there is no way to deduce the hashes of the L2 blocks based on the
transactions’ in the batch (because there is no access to the transaction’s hashes). At the same time, in order to
server <code>blockhash</code> method, the VM requires the knowledge of some of the previous L2 block hashes. In order to save up on
pubdata (by making sure that the same storage slots are reused, i.e. we only have repeated writes) we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L70">store</a>
only the last 257 block hashes. You can read more on what are the repeated writes and how the pubdata is processed
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>We store only the last 257 blocks, since the EVM requires only 256 previous ones and we use 257 as a safe margin.</p>
<h3 id="legacy-blockhash"><a class="header" href="#legacy-blockhash">Legacy blockhash</a></h3>
<p>When initializing L2 blocks that do not have their hashes stored on L2 (basically these are blocks before the migration
upgrade), we use the following formula for their hash:</p>
<p><code>keccak256(abi.encodePacked(uint32(_blockNumber)))</code></p>
<h3 id="timing-invariants"><a class="header" href="#timing-invariants">Timing invariants</a></h3>
<p>While the timestamp of each L2 block is provided by the operator, there are some timing invariants that the system
preserves:</p>
<ul>
<li>For each L2 block its timestamp should be &gt; the timestamp of the previous L2 block</li>
<li>For each L2 block its timestamp should be ≥ timestamp of the batch it belongs to</li>
<li>Each batch must start with a new L2 block (i.e. an L2 block can not span across batches).</li>
<li>The timestamp of a batch must be ≥ the timestamp of the latest L2 block which belonged to the previous batch.</li>
<li>The timestamp of the last miniblock in batch can not go too far into the future. This is enforced by publishing an
L2→L1 log, with the timestamp which is then checked on L1.</li>
</ul>
<h2 id="fictive-l2-block--finalizing-the-batch"><a class="header" href="#fictive-l2-block--finalizing-the-batch">Fictive L2 block &amp; finalizing the batch</a></h2>
<p>At the end of the batch, the bootloader calls the <code>setL2Block</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3812">one more time</a>
to allow the operator to create a new empty block. This is done purely for some of the technical reasons inside the
node, where each batch ends with an empty L2 block.</p>
<p>We do not enforce that the last block is empty explicitly as it complicates the development process and testing, but in
practice, it is, and either way, it should be secure.</p>
<p>Also, at the end of the batch we send the timestamps of the batch as well as the timestamp of the last miniblock in
order to check on L1 that both of these are realistic. Checking any other L2 block’s timestamp is not required since all
of them are enforced to be between those two.</p>
<h2 id="migration--virtual-blocks-logic"><a class="header" href="#migration--virtual-blocks-logic">Migration &amp; virtual blocks’ logic</a></h2>
<p>As already explained above, for a smoother upgrade for the ecosystem, there is a migration being performed during which
instead of returning either batch information or L2 block information, we will return the virtual block information
until they catch up with the L2 block’s number.</p>
<h3 id="production-of-the-virtual-blocks"><a class="header" href="#production-of-the-virtual-blocks">Production of the virtual blocks</a></h3>
<ul>
<li>In each batch, there should be at least one virtual block created.</li>
<li>Whenever a new L2 block is created, the operator can select how many virtual blocks it wants to create. This can be
any number, however, if the number of the virtual block exceeds the L2 block number, the migration is considered
complete and we switch to the mode where the L2 block information will be returned.</li>
</ul>
<h2 id="additional-note-on-blockhashes"><a class="header" href="#additional-note-on-blockhashes">Additional note on blockhashes</a></h2>
<p>Note, that if we used some complex formula for virtual blocks’ hashes (like we do for L2 blocks), we would have to put
all of these into storage for the data availability. Even if we used the same storage trick that we used for the L2
blocks, where we store only the last 257’s block’s hashes under the current load/migration plans it would be expected
that we have roughly ~250 virtual blocks per batch, practically meaning that we will publish all of these anyway. This
would be too expensive. That is why we have to use a simple formula of <code>keccak(uint256(number))</code> for now. Note, that
they do not collide with the legacy miniblock hash, since legacy miniblock hashes are calculated as
<code>keccak(uint32(number))</code>.</p>
<p>Also, we need to keep the consistency of previous blockhashes, i.e. if <code>blockhash(X)</code> returns a non-zero value, it
should be consistent among the future blocks. For instance, let’s say that the hash of batch <code>1000</code> is <code>1</code>,
i.e. <code>blockhash(1000) = 1</code>. Then, when we migrate to virtual blocks, we need to ensure that <code>blockhash(1000)</code> will
return either 0 (if and only if the block is more than 256 blocks old) or <code>1</code>. Because of that for <code>blockhash</code> we will
have the following complex
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L103">logic</a>:</p>
<ul>
<li>For blocks that were created before the virtual block upgrade, use the batch hashes</li>
<li>For blocks that were created during the virtual block upgrade, use <code>keccak(uint256(number))</code>.</li>
<li>For blocks that were created after the virtual blocks have caught up with the L2 blocks, use L2 block hashes.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="l1-smart-contracts"><a class="header" href="#l1-smart-contracts">L1 Smart contracts</a></h1>
<p>This document presumes familiarity with Rollups. For a better understanding, consider reading the overview
<a href="specs/./overview.html">here</a>.</p>
<p>Rollups inherit security and decentralization guarantees from Ethereum, on which they store information about changes in
their own state, providing validity proofs for state transition, implementing a communication mechanism, etc. In
practice, all this is achieved by Smart Contracts built on top of Ethereum. This document details the architecture of
the L2 contracts on Ethereum Layer 1. We also have contracts that support the ZK Chain ecosystem, we cover those in the
[Shared Bridge][TODO] section. The Shared Bridge relies on these individual contracts.</p>
<h2 id="diamond"><a class="header" href="#diamond">Diamond</a></h2>
<p>Technically, this L1 smart contract acts as a connector between Ethereum (L1) and a single L2. It checks the validity
proof and data availability, handles L2 &lt;-&gt; L1 communication, finalizes L2 state transition, and more.</p>
<p><img src="specs/./img/diamondProxy.jpg" alt="diamondProxy.png" /></p>
<h3 id="diamondproxy"><a class="header" href="#diamondproxy">DiamondProxy</a></h3>
<p>The main contract uses <a href="https://eips.ethereum.org/EIPS/eip-2535">EIP-2535</a> diamond proxy pattern. It is an in-house
implementation that is inspired by the <a href="https://github.com/mudgen/Diamond">mudgen reference implementation</a>. It has no
external functions, only the fallback that delegates a call to one of the facets (target/implementation contract). So
even an upgrade system is a separate facet that can be replaced.</p>
<p>One of the differences from the reference implementation is access freezable. Each of the facets has an associated
parameter that indicates if it is possible to freeze access to the facet. Privileged actors can freeze the <strong>diamond</strong>
(not a specific facet!) and all facets with the marker <code>isFreezable</code> should be inaccessible until the governor or admin
unfreezes the diamond. Note that it is a very dangerous thing since the diamond proxy can freeze the upgrade system and
then the diamond will be frozen forever.</p>
<p>The diamond proxy pattern is very flexible and extendable. For now, it allows splitting implementation contracts by
their logical meaning, removes the limit of bytecode size per contract and implements security features such as
freezing. In the future, it can also be viewed as <a href="https://eips.ethereum.org/EIPS/eip-6900">EIP-6900</a> for
<a href="https://blog.matter-labs.io/introducing-the-zk-stack-c24240c2532a">ZK Stack</a>, where each ZK Chain can implement a
sub-set of allowed implementation contracts.</p>
<h3 id="gettersfacet"><a class="header" href="#gettersfacet">GettersFacet</a></h3>
<p>Separate facet, whose only function is providing <code>view</code> and <code>pure</code> methods. It also implements
<a href="https://eips.ethereum.org/EIPS/eip-2535#diamond-loupe">diamond loupe</a> which makes managing facets easier. This contract
must never be frozen.</p>
<h3 id="adminfacet"><a class="header" href="#adminfacet">AdminFacet</a></h3>
<p>Controls changing the privileged addresses such as governor and validators or one of the system parameters (L2
bootloader bytecode hash, verifier address, verifier parameters, etc), and it also manages the freezing/unfreezing and
execution of upgrades in the diamond proxy.</p>
<p>The admin facet is controlled by two entities:</p>
<ul>
<li>Governance - Separate smart contract that can perform critical changes to the system as protocol upgrades. This
contract controlled by two multisigs, one managed by Matter Labs team and another will be multisig with well-respected
contributors in the crypto space. Only together they can perform an instant upgrade, the Matter Labs team can only
schedule an upgrade with delay.</li>
<li>Admin - Multisig smart contract managed by Matter Labs that can perform non-critical changes to the system such as
granting validator permissions. Note, that the Admin is the same multisig as the owner of the governance.</li>
</ul>
<h3 id="mailboxfacet"><a class="header" href="#mailboxfacet">MailboxFacet</a></h3>
<p>The facet that handles L2 &lt;-&gt; L1 communication, an overview for which can be found in
<a href="https://docs.zksync.io/build/developer-reference/l1-l2-interoperability">docs</a>.</p>
<p>The Mailbox performs three functions:</p>
<ul>
<li>L1 &lt;-&gt; L2 communication.</li>
<li>Bridging native Ether to the L2 (with the launch of the Shared Bridge this will be moved)</li>
<li>Censorship resistance mechanism (in the research stage).</li>
</ul>
<p>L1 -&gt; L2 communication is implemented as requesting an L2 transaction on L1 and executing it on L2. This means a user
can call the function on the L1 contract to save the data about the transaction in some queue. Later on, a validator can
process it on L2 and mark it as processed on the L1 priority queue. Currently, it is used for sending information from
L1 to L2 or implementing multi-layer protocols.</p>
<p><em>NOTE</em>: While user requests the transaction from L1, the initiated transaction on L2 will have such a <code>msg.sender</code>:</p>
<pre><code class="language-solidity">  address sender = msg.sender;
  if (sender != tx.origin) {
      sender = AddressAliasHelper.applyL1ToL2Alias(msg.sender);
  }
</code></pre>
<p>where</p>
<pre><code class="language-solidity">uint160 constant offset = uint160(0x1111000000000000000000000000000000001111);

function applyL1ToL2Alias(address l1Address) internal pure returns (address l2Address) {
  unchecked {
    l2Address = address(uint160(l1Address) + offset);
  }
}

</code></pre>
<p>For most of the rollups the address aliasing needs to prevent cross-chain exploits that would otherwise be possible if
we simply reused the same L1 addresses as the L2 sender. In zkEVM address derivation rule is different from the
Ethereum, so cross-chain exploits are already impossible. However, the zkEVM may add full EVM support in the future, so
applying address aliasing leaves room for future EVM compatibility.</p>
<p>The L1 -&gt; L2 communication is also used for bridging ether. The user should include a <code>msg.value</code> when initiating a
transaction request on the L1 contract. Before executing a transaction on L2, the specified address will be credited
with the funds. To withdraw funds user should call <code>withdraw</code> function on the <code>L2EtherToken</code> system contracts. This will
burn the funds on L2, allowing the user to reclaim them through the <code>finalizeEthWithdrawal</code> function on the
<code>MailboxFacet</code>.</p>
<p>More about L1-&gt;L2 operations can be found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20L1%E2%86%92L2%20ops%20on%20zkSync.md">here</a>.</p>
<p>L2 -&gt; L1 communication, in contrast to L1 -&gt; L2 communication, is based only on transferring the information, and not on
the transaction execution on L1. The full description of the mechanism for sending information from L2 to L1 can be
found
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">here</a>.</p>
<h3 id="executorfacet"><a class="header" href="#executorfacet">ExecutorFacet</a></h3>
<p>A contract that accepts L2 batches, enforces data availability and checks the validity of zk-proofs.</p>
<p>The state transition is divided into three stages:</p>
<ul>
<li><code>commitBatches</code> - check L2 batch timestamp, process the L2 logs, save data for a batch, and prepare data for zk-proof.</li>
<li><code>proveBatches</code> - validate zk-proof.</li>
<li><code>executeBatches</code> - finalize the state, marking L1 -&gt; L2 communication processing, and saving Merkle tree with L2 logs.</li>
</ul>
<p>Each L2 -&gt; L1 system log will have a key that is part of the following:</p>
<pre><code class="language-solidity">enum SystemLogKey {
  L2_TO_L1_LOGS_TREE_ROOT_KEY,
  TOTAL_L2_TO_L1_PUBDATA_KEY,
  STATE_DIFF_HASH_KEY,
  PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY,
  PREV_BATCH_HASH_KEY,
  CHAINED_PRIORITY_TXN_HASH_KEY,
  NUMBER_OF_LAYER_1_TXS_KEY,
  EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY
}

</code></pre>
<p>When a batch is committed, we process L2 -&gt; L1 system logs. Here are the invariants that are expected there:</p>
<ul>
<li>In a given batch there will be either 7 or 8 system logs. The 8th log is only required for a protocol upgrade.</li>
<li>There will be a single log for each key that is contained within <code>SystemLogKey</code></li>
<li>Three logs from the <code>L2_TO_L1_MESSENGER</code> with keys:</li>
<li><code>L2_TO_L1_LOGS_TREE_ROOT_KEY</code></li>
<li><code>TOTAL_L2_TO_L1_PUBDATA_KEY</code></li>
<li><code>STATE_DIFF_HASH_KEY</code></li>
<li>Two logs from <code>L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR</code> with keys:
<ul>
<li><code>PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY</code></li>
<li><code>PREV_BATCH_HASH_KEY</code></li>
</ul>
</li>
<li>Two or three logs from <code>L2_BOOTLOADER_ADDRESS</code> with keys:
<ul>
<li><code>CHAINED_PRIORITY_TXN_HASH_KEY</code></li>
<li><code>NUMBER_OF_LAYER_1_TXS_KEY</code></li>
<li><code>EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY</code></li>
</ul>
</li>
<li>None logs from other addresses (may be changed in the future).</li>
</ul>
<h3 id="diamondinit"><a class="header" href="#diamondinit">DiamondInit</a></h3>
<p>It is a one-function contract that implements the logic of initializing a diamond proxy. It is called only once on the
diamond constructor and is not saved in the diamond as a facet.</p>
<p>Implementation detail - function returns a magic value just like it is designed in
<a href="https://eips.ethereum.org/EIPS/eip-1271">EIP-1271</a>, but the magic value is 32 bytes in size.</p>
<h2 id="bridges"><a class="header" href="#bridges">Bridges</a></h2>
<p>Bridges are completely separate contracts from the Diamond. They are a wrapper for L1 &lt;-&gt; L2 communication on contracts
on both L1 and L2. Upon locking assets on L1, a request is sent to mint these bridged assets on L2. Upon burning assets
on L2, a request is sent to unlock them on L2.</p>
<p>Unlike the native Ether bridging, all other assets can be bridged by the custom implementation relying on the trustless
L1 &lt;-&gt; L2 communication.</p>
<h3 id="l1erc20bridge"><a class="header" href="#l1erc20bridge">L1ERC20Bridge</a></h3>
<p>The “legacy” implementation of the ERC20 token bridge. Works only with regular ERC20 tokens, i.e. not with
fee-on-transfer tokens or other custom logic for handling user balances.</p>
<ul>
<li><code>deposit</code> - lock funds inside the contract and send a request to mint bridged assets on L2.</li>
<li><code>claimFailedDeposit</code> - unlock funds if the deposit was initiated but then failed on L2.</li>
<li><code>finalizeWithdrawal</code> - unlock funds for the valid withdrawal request from L2.</li>
</ul>
<p>The owner of the L1ERC20Bridge is the Governance contract.</p>
<h3 id="l1assetrouter"><a class="header" href="#l1assetrouter">L1AssetRouter</a></h3>
<p>The main bridge implementation handles transfers Ether, ERC20 tokens and of WETH tokens between the two domains. It is
designed to streamline and enhance the user experience for bridging WETH tokens by minimizing the number of transactions
required and reducing liquidity fragmentation thus improving efficiency and user experience.</p>
<p>This contract accepts WETH deposits on L1, unwraps them to ETH, and sends the ETH to the L2 WETH bridge contract, where
it is wrapped back into WETH and delivered to the L2 recipient.</p>
<p>Thus, the deposit is made in one transaction, and the user receives L2 WETH that can be unwrapped to ETH.</p>
<p>For withdrawals, the contract receives ETH from the L2 WETH bridge contract, wraps it into WETH, and sends the WETH to
the L1 recipient.</p>
<p>The owner of the L1WethBridge contract is the Governance contract.</p>
<h3 id="l2sharedbridge"><a class="header" href="#l2sharedbridge">L2SharedBridge</a></h3>
<p>The L2 counterpart of the L1 Shared bridge.</p>
<ul>
<li><code>withdraw</code> - initiate a withdrawal by burning funds on the contract and sending a corresponding message to L1.</li>
<li><code>finalizeDeposit</code> - finalize the deposit and mint funds on L2. The function is only callable by L1 bridge.</li>
</ul>
<p>The owner of the L2SharedBridge and the contracts related to it is the Governance contract.</p>
<h2 id="governance"><a class="header" href="#governance">Governance</a></h2>
<p>This contract manages calls for all governed zkEVM contracts on L1 and L2. Mostly, it is used for upgradability an
changing critical system parameters. The contract has minimum delay settings for the call execution.</p>
<p>Each upgrade consists of two steps:</p>
<ul>
<li>Scheduling - The owner can schedule upgrades in two different manners:
<ul>
<li>Fully transparent data. All the targets, calldata, and upgrade conditions are known to the community before upgrade
execution.</li>
<li>Shadow upgrade. The owner only shows the commitment to the upgrade. This upgrade type is mostly useful for fixing
critical issues in the production environment.</li>
</ul>
</li>
<li>Upgrade execution - the Owner or Security council can perform the upgrade with previously scheduled parameters.
<ul>
<li>Upgrade with delay. Scheduled operations should elapse the delay period. Both the owner and Security Council can
execute this type of upgrade.</li>
<li>Instant upgrade. Scheduled operations can be executed at any moment. Only the Security Council can perform this type
of upgrade.</li>
</ul>
</li>
</ul>
<p>Please note, that both the Owner and Security council can cancel the upgrade before its execution.</p>
<p>The diagram below outlines the complete journey from the initiation of an operation to its execution.</p>
<p><img src="specs/./img/governance.jpg" alt="governance.png" /></p>
<h2 id="validatortimelock"><a class="header" href="#validatortimelock">ValidatorTimelock</a></h2>
<p>An intermediate smart contract between the validator EOA account and the ZKsync smart contract. Its primary purpose is
to provide a trustless means of delaying batch execution without modifying the main ZKsync contract. ZKsync actively
monitors the chain activity and reacts to any suspicious activity by freezing the chain. This allows time for
investigation and mitigation before resuming normal operations.</p>
<p>It is a temporary solution to prevent any significant impact of the validator hot key leakage, while the network is in
the Alpha stage.</p>
<p>This contract consists of four main functions <code>commitBatches</code>, <code>proveBatches</code>, <code>executeBatches</code>, and <code>revertBatches</code>,
which can be called only by the validator.</p>
<p>When the validator calls <code>commitBatches</code>, the same calldata will be propagated to the ZKsync contract (<code>DiamondProxy</code>
through <code>call</code> where it invokes the <code>ExecutorFacet</code> through <code>delegatecall</code>), and also a timestamp is assigned to these
batches to track the time these batches are committed by the validator to enforce a delay between committing and
execution of batches. Then, the validator can prove the already committed batches regardless of the mentioned timestamp,
and again the same calldata (related to the <code>proveBatches</code> function) will be propagated to the ZKsync contract. After
the <code>delay</code> is elapsed, the validator is allowed to call <code>executeBatches</code> to propagate the same calldata to ZKsync
contract.</p>
<p>The owner of the ValidatorTimelock contract is the same as the owner of the Governance contract - Matter Labs multisig.</p>
<h2 id="allowlist"><a class="header" href="#allowlist">Allowlist</a></h2>
<p>The auxiliary contract controls the permission access list. It is used in bridges and diamond proxies to control which
addresses can interact with them in the Alpha release. Currently, it is supposed to set all permissions to public.</p>
<p>The owner of the Allowlist contract is the Governance contract.</p>
<h2 id="deposit-limitation"><a class="header" href="#deposit-limitation">Deposit Limitation</a></h2>
<p>The amount of deposit can be limited. This limitation is applied on an account level and is not time-based. In other
words, each account cannot deposit more than the cap defined. The tokens and the cap can be set through governance
transactions. Moreover, there is an allow listing mechanism as well (only some allow listed accounts can call some
specific functions). So, the combination of deposit limitation and allow listing leads to limiting the deposit of the
allow listed account to be less than the defined cap.</p>
<pre><code class="language-solidity">struct Deposit {
  bool depositLimitation;
  uint256 depositCap;
}

</code></pre>
<p>Currently, the limit is used only for blocking deposits of the specific token (turning on the limitation and setting the
limit to zero). And on the near future, this functionality will be completely removed.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="zk-stack-contracts-specs"><a class="header" href="#zk-stack-contracts-specs">ZK Stack contracts specs</a></h1>
<p>The order of the files here only roughly represents the order of reading. A lot of topics are intertwined, so it is recommended to read everything first to have a complete picture and then refer to specific documents for more details.</p>
<ul>
<li><a href="specs/contracts/../contracts/overview.html">Overview</a></li>
<li><a href="specs/contracts/../contracts/glossary.html">Glossary</a></li>
<li><a href="specs/contracts/../contracts/chain_management/overview.html">Chain Management</a>
<ul>
<li><a href="specs/contracts/../contracts/chain_management/bridgehub.html">Bridgehub</a></li>
<li><a href="specs/contracts/../contracts/chain_management/chain_type_manager.html">Chain type manager</a></li>
<li><a href="specs/contracts/../contracts/chain_management/admin_role.html">Admin role</a></li>
<li><a href="specs/contracts/../contracts/chain_management/chain_genesis.html">Chain genesis</a></li>
<li><a href="specs/contracts/../contracts/chain_management/upgrade_process.html">Standard Upgrade process</a></li>
</ul>
</li>
<li><a href="specs/contracts/../contracts/bridging/overview.html">Bridging</a>
<ul>
<li><a href="specs/contracts/../contracts/bridging/asset_router_and_ntv/asset_router.html">Asset Router</a></li>
<li><a href="specs/contracts/../contracts/bridging/asset_router_and_ntv/native_token_vault.html">Native token vault</a></li>
</ul>
</li>
<li><a href="specs/contracts/../contracts/settlement_contracts/zkchain_basics.html">Settlement Contracts</a>
<ul>
<li><a href="specs/contracts/../contracts/settlement_contracts/priority_queue/README.html">L1 &lt;&gt; L2 communication</a>
<ul>
<li><a href="specs/contracts/../contracts/settlement_contracts/priority_queue/l1_l2_communication/l1_to_l2.html">Handling L1→L2 operations</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/priority_queue/l1_l2_communication/l2_to_l1.html">L2→L1 communication</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/priority_queue/l1_l2_communication/overview_deposits_withdrawals.html">Overview - Deposits and Withdrawals</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/priority_queue/priority-queue.html">Priority queue</a></li>
</ul>
</li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/README.html">Data availability</a>
<ul>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/pubdata.html">Pubdata</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/compression.html">Compression</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/reconstruction.html">Reconstruction</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/validium_zk_porter.html">Validium and zkPorter</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/custom_da.html">Custom DA support</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/rollup_da.html">Rollup DA support</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/standard_pubdata_format.html">Standard pubdata format</a></li>
<li><a href="specs/contracts/../contracts/settlement_contracts/data_availability/state_diff_compression_v1_spec.html">State diff compression v1 spec</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="specs/contracts/../contracts/consensus/README.html">Consensus</a>
<ul>
<li><a href="specs/contracts/../contracts/consensus/consensus-registry.html">Consensus Registry</a></li>
</ul>
</li>
</ul>
<p><img src="specs/contracts/./img/reading_order.png" alt="Reading order" /></p>
<h2 id="contracts-repo-structure"><a class="header" href="#contracts-repo-structure">Contracts repo structure</a></h2>
<p>The repository contains the following sections:</p>
<ul>
<li>[gas-bound-caller][TODO] that contains <code>GasBoundCaller</code> utility contract implementation. You can read more about it in its README.</li>
<li>[da-contracts][TODO] contracts that should be deployed on L1 only.</li>
<li>[l1-contracts][TODO]. Despite the legacy name, it contains contracts that are deployed both on L1 and on L2. This folder encompasses bridging, ZK chain contracts, the contracts for chain admin, etc. The name is historical due to the fact that these contracts were usually deployed on L1 only. However with Gateway, settlement and bridging-related contracts will be deployed on both EVM and eraVM environment. Also, bridging has been unified between L1 and L2 in many places and so keeping everything in one project allows to avoid code duplication.</li>
<li>[l2-contracts][TODO]. Contains contracts that are deployed only on L2.</li>
<li>[system-contracts][TODO]. Contains system contracts or predeployed L2 contracts.</li>
</ul>
<h2 id="for-auditors-invariantstricky-places-to-look-out-for"><a class="header" href="#for-auditors-invariantstricky-places-to-look-out-for">For auditors: Invariants/tricky places to look out for</a></h2>
<p>This section is for auditors of the codebase. It includes some of the important invariants that the system relies on and which if broken could have bad consequences.</p>
<ul>
<li>Assuming that the accepting CTM is correct &amp; efficient, the L1→GW part of the L1→GW→L3 transaction never fails. It is assumed that the provided max amount for gas is always enough for any transaction that can realistically come from L1.</li>
<li>GW → L1 migration never fails. If it is possible to get into a state where the migration is not possible to finish, then the chain is basically lost. There are some exceptions where for now it is the expected behavior. (check out the “Migration invariants &amp; protocol upgradability” section)</li>
<li>The general consistency of chains when migration between different settlement layers is done. Including the feasibility of emergency upgrades, etc. I.e. whether the whole system is thought-through.</li>
<li>Preimage attacks in the L3→L1 tree, we apply special prefixes to ensure that the tree structure is fixed, i.e. all logs are 88 bytes long (this is for backwards compatibility reasons). For batch leaves and chain id leaves we use special prefixes.</li>
<li>Data availability guarantees. Whether rollup users can always restore all their storage slots, etc. An example of a potential tricky issue can be found in “Security notes for Gateway-based rollups” [in this document][TODO].</li>
</ul>
<p>The desired properties of the system are that funds can not be stolen from the L1 contracts, and that L2 constracts are executed securely.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="contracts-1"><a class="header" href="#contracts-1">Contracts</a></h1>
<p>Contracts are used throughout the codebase. The core or the L2 system is coded in smart contracts, these can be found in [l2_system_contracts][TODO]</p>
<p>The chain uses contracts to <a href="specs/contracts/./settlement_contracts/zkchain_basics.html">settle</a>.</p>
<p>Chain admins use contracts on L1 to <a href="specs/contracts/./chain_management/overview.html">manage</a> the chain.</p>
<p>Contracts are also used to <a href="specs/contracts/./bridging/overview.html">bridge</a> assets between chains.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glossary-1"><a class="header" href="#glossary-1">Glossary</a></h1>
<ul>
<li><strong>Validator/Operator</strong> - a privileged address that can commit/verify/execute L2 batches.</li>
<li><strong>L2 batch (or just batch)</strong> - An aggregation of multiple L2 blocks. Note, that while the API operates on L2 blocks,
the prove system operates on batches, which represent a single proved VM execution, which typically contains multiple
L2 blocks.</li>
<li><strong>Facet</strong> - implementation contract. The word comes from the EIP-2535.</li>
<li><strong>Gas</strong> - a unit that measures the amount of computational effort required to execute specific operations on the
ZKsync Era network.</li>
<li><strong>MessageRoot</strong>, <strong>ChainRoot</strong>, <strong>ChainBatchRoot</strong>, <strong>LocalLogsRoot</strong> , <strong>L2ToL1LogsRoot</strong>- different nodes in the recursive Merkle tree used to aggregate messages. Note, LocalLogsRoot and L2ToL1LogsRoot are the same.</li>
<li><strong>assetId</strong> - unique 32 bytes used to identify different assets in the AssetRouter.</li>
<li><strong>Settlement Layer</strong> - the layer where a chains settles its batches. Can be L1 or Gateway.</li>
</ul>
<p>Some terms are inherited from the wider <a href="https://github.com/ethereum/L2-interop/blob/main/NOMENCLATURE.md">Ethereum ecosystem</a>.</p>
<p>List of contracts and abbreviations:</p>
<ul>
<li>
<p>Chain Manager Contracts</p>
<ul>
<li>
<p>Bridgehub</p>
<p>Main ecosystem contract. Tracks all the chains and their types (i.e. CTMs), based on their chain ID and addresses. This is where new chains register.</p>
</li>
<li>
<p>ChainTypeManager (CTM) Contract:</p>
<p>used to coordinate upgrades for a certain chain classes. We only support a single CTM currently.</p>
</li>
<li>
<p>CTMDeploymentTracker:</p>
<p>Tracks the deployment of a new CTM on different settlement layers, such as Gateway. Needed as the Bridgehub is registered in the AssetRouter as an AssetHandler for chains so that they can be migrated to the Gateway.</p>
</li>
</ul>
</li>
<li>
<p>Chain contracts</p>
<ul>
<li>Diamond Proxy: A type of proxy contract (i.e. like Transparent Upgradable, Universal Upgradable). Currently only the ZK chains use this contract, so it is used sometimes as a synonym for the chain.
<ul>
<li>MailboxFacet: functions on the chain used to send and receive messages to the L1</li>
<li>GetterFacet: functions that are read-only and can be called by anyone</li>
<li>AdminFacet: functions that are used to manage the chain</li>
<li>ExecutorFacet: functions that are used to execute L2 batches</li>
<li>Verifier</li>
<li>Additional contracts:
<ul>
<li>ValidatorTimelock</li>
<li>DiamondInit</li>
<li>PriorityQueue</li>
<li>PriorityTree</li>
<li>MessageVerification</li>
<li>Any upgrade contract, i.e. GatewayUpgrade</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Messaging related contracts:</p>
<ul>
<li>MessageRoot</li>
<li>L1Nullifier</li>
</ul>
</li>
<li>
<p>Asset related contracts:</p>
<ul>
<li>AssetRouter</li>
<li>NativeTokenVault</li>
<li>Additional contracts:
<ul>
<li>BridgeStandardERC29</li>
<li>IAssetHandler</li>
<li>DeploymentTracker</li>
</ul>
</li>
<li>Legacy:
<ul>
<li>L1Erc20Bridge</li>
<li>L2SharedBridge (upgraded from L2Erc20Bridge)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>DA contracts:</p>
<ul>
<li>CalldataDA</li>
<li>CaddlataDAGateway</li>
<li>RelayedSLDAValidator</li>
<li>RollupDAManager</li>
<li>ValidiumL1DAValidator</li>
</ul>
</li>
<li>
<p>Libraries and Primitives:</p>
<ul>
<li>DynamicIncrementalMerkleTree</li>
<li>FullMerkleTree</li>
<li>MessageHashing</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-4"><a class="header" href="#overview-4">Overview</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>Ethereum’s future is rollup-centric. This means breaking with the current paradigm of isolated EVM chains to
infrastructure that is focused on an ecosystem of interconnected zkEVMs, (which we name ZK Chains). This ecosystem will
be grounded on Ethereum, requiring the appropriate L1 smart contracts. Here we outline our ZK Stack approach for these
contracts, their interfaces, the needed changes to the existing architecture, as well as future features to be
implemented.</p>
<p>If you want to know more about ZK Chains, check this
<a href="https://blog.matter-labs.io/introduction-to-hyperchains-fdb33414ead7">blog post</a>, or go through
<a href="https://docs.zksync.io/zk-stack/concepts/zk-chains">our docs</a>.</p>
<h3 id="high-level-design"><a class="header" href="#high-level-design">High-level design</a></h3>
<p>We want to create a system where:</p>
<ul>
<li>ZK Chains should be launched permissionlessly within the ecosystem.</li>
<li>Settlement should be made cheap by proof aggregation.</li>
<li>Interop should enable unified liquidity for assets across the ecosystem.</li>
<li>Multi-chain smart contracts need to be easy to develop, which means easy access to traditional bridges, and other
supporting architecture.</li>
</ul>
<p>ZK Chains have specific trust requirements - they need to satisfy certain common standards so that they can trust each
other. This means a single set of L1 smart contracts has to manage the proof verification for all ZK Chains, and if the
proof system is upgraded, all chains have to be upgraded together. New chains will be able to be launched
permissionlessly in the ecosystem according to this shared standard.</p>
<p>To allow unified liquidity each L1 asset (ETH, ERC20, NFTs) will have a single bridge contract on L1 for the whole
ecosystem. These shared bridges will allow users to deposit, withdraw and transfer from any ZK Chain in the ecosystem.
These shared bridges are also responsible for deploying and maintaining their counterparts on the ZK Chains. The
counterparts will be asset contracts extended with bridging functionality.</p>
<p>To enable the bridging functionality:</p>
<ul>
<li>On the L1 we will add a Bridgehub contract which connects asset bridges to all the ZK Chains.</li>
<li>On the ZK Chain side we will add special system contracts that enable these features.</li>
</ul>
<p>We want to make the ecosystem as modular as possible, giving developers the ability to modify the architecture as
needed; consensus mechanism, staking, and DA requirements.</p>
<p>We also want the system to be forward-compatible, alternative Chain Type Manager (CTM) contracts. Those future features have
to be considered in this initial design, so it can evolve to support them (meaning, chains being launched now will still
be able to leverage them when available).</p>
<hr />
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<h3 id="general-architecture"><a class="header" href="#general-architecture">General Architecture</a></h3>
<p><img src="specs/contracts/chain_management/./img/ecosystem_architecture.png" alt="Contracts" /></p>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<h4 id="bridgehub"><a class="header" href="#bridgehub">Bridgehub</a></h4>
<ul>
<li>
<p>This is the main registry contract. This is where ZK Chains can register, starting in a permissioned manner, but with the goal to be
permissionless in the future. This is where their <code>chainID</code> is determined. Chains on Gateway will also register here.
This <code>Registry</code> is also where Chain Type Manager contracts should register. Each chain has to specify its desired CTM
when registering (Initially, only one will be available).</p>
<pre><code>function newChain(
        uint256 _chainId,
        address _chainTypeManager
    ) external returns (uint256 chainId);

function newChainTypeManager(address _chainTypeManager) external;
</code></pre>
</li>
</ul>
<h4 id="chain-type-manager"><a class="header" href="#chain-type-manager">Chain Type Manager</a></h4>
<ul>
<li><code>ChainTypeManager</code> A chain type manager manages proof verification and standard rollup DA for multiple chains. It also implements the
following functionalities:
<ul>
<li><code>ChainTypeRegistry</code> The chain type is shared for multiple chains, so initialization and upgrades have to be the same for all
chains. Registration is not permissionless but happens based on the registrations in the bridgehub’s <code>Registry</code>. At
registration a <code>DiamondProxy</code> is deployed and initialized with the appropriate <code>Facets</code> for each ZK Chain.</li>
<li><code>Facets</code> and <code>Verifier</code> are shared across chains that relies on the same chain type: <code>Base</code>, <code>Executor</code> , <code>Getters</code>, <code>Admin</code>
, <code>Mailbox.</code>The <code>Verifier</code> is the contract that actually verifies the proof, and is called by the <code>Executor</code>.</li>
<li>Upgrade Mechanism The system requires all chains to be up-to-date with the latest implementation, so whenever an
update is needed, we have to “force” each chain to update, but due to decentralization, we have to give each chain a
time frame. This is done in the update mechanism contract, this is where the bootloader and system contracts are
published, and the <code>ProposedUpgrade</code> is stored. Then each chain can call this upgrade for themselves as needed.
After the deadline is over, the not-updated chains are frozen, that is, cannot post new proofs. Frozen chains can
unfreeze by updating their proof system.</li>
</ul>
</li>
<li>Each chain has a <code>DiamondProxy</code>.
<ul>
<li>The <a href="https://eips.ethereum.org/EIPS/eip-2535">Diamond Proxy</a> is the proxy pattern that is used for the chain
contracts. A diamond proxy points to multiple implementation contracts called facets. Each selector is saved in the
proxy, and the correct facet is selected and called.</li>
<li>In the future the DiamondProxy can be configured by picking alternative facets e.g. Validiums will have their own
<code>Executor</code></li>
</ul>
</li>
</ul>
<h4 id="chain-specific-contracts"><a class="header" href="#chain-specific-contracts">Chain specific contracts</a></h4>
<ul>
<li>A chain might implement its own specific consensus mechanism. This needs its own contracts. Only this contract will be
able to submit proofs to the State Transition contract.</li>
<li>DA contracts.</li>
<li>Currently, the <code>ValidatorTimelock</code> is an example of such a contract.</li>
</ul>
<h3 id="components-interactions"><a class="header" href="#components-interactions">Components interactions</a></h3>
<p>In this section, we will present some diagrams showing the interaction of different components.</p>
<h4 id="new-chain"><a class="header" href="#new-chain">New Chain</a></h4>
<p>A chain registers in the Bridgehub, this is where the chain ID is determined. Read more about it <a href="specs/contracts/chain_management/./chain_genesis.html">here</a>. The chain’s governor specifies the State
Transition that they plan to use. In the first version only a single State Transition contract will be available for
use, our with Boojum proof verification.</p>
<p>At initialization we prepare the <code>DiamondInit</code> contract. We store the genesis batch hash in the chain contract, all
chains start out with the same state. A diamond proxy is deployed and initialised with this initial value, along with
predefined facets which are made available by the chain contract. These facets contain the proof verification and other
features required to process proofs. The chain ID is set in the VM in a special system transaction sent from L1.</p>
<hr />
<h3 id="common-standards-and-upgrades"><a class="header" href="#common-standards-and-upgrades">Common Standards and Upgrades</a></h3>
<p>In this initial phase, ZK Chains have to follow some common standards, so that they can trust each other. Read more about it <a href="specs/contracts/chain_management/./upgrade_process.html">here</a>. This means all
chains start out with the same empty state, they have the same VM implementations and proof systems, asset contracts can
trust each on different chains, and the chains are upgraded together. We elaborate on the shared upgrade mechanism here.</p>
<h4 id="upgrade-mechanism"><a class="header" href="#upgrade-mechanism">Upgrade mechanism</a></h4>
<p>Currently, there are three types of upgrades for zkEVM. Normal upgrades (used for new features) are initiated by the
Governor (a multisig) and are public for a certain timeframe before they can be applied. Shadow upgrades are similar to
normal upgrades, but the data is not known at the moment the upgrade is proposed, but only when executed (they can be
executed with the delay, or instantly if approved by the security council). Instant upgrades (used for security issues),
on the other hand happen quickly and need to be approved by the Security Council in addition to the Governor. For ZK
Chains the difference is that upgrades now happen on multiple chains. This is only a problem for shadow upgrades - in
this case, the chains have to tightly coordinate to make all the upgrades happen in a short time frame, as the content
of the upgrade becomes public once the first chain is upgraded. The actual upgrade process is as follows:</p>
<ol>
<li>Prepare Upgrade for all chains:
<ul>
<li>The new facets and upgrade contracts have to be deployed,</li>
<li>The upgrade’ calldata (diamondCut, initCalldata with ProposedUpgrade) is hashed on L1 and the hash is saved.</li>
</ul>
</li>
<li>Upgrade specific chain
<ul>
<li>The upgrade has to be called on the specific chain. The upgrade calldata is passed in as calldata and verified. The
protocol version is updated.</li>
<li>Ideally, the upgrade will be very similar for all chains. If it is not, a smart contract can calculate the
differences. If this is also not possible, we have to set the <code>diamondCut</code> for each chain by hand.</li>
</ul>
</li>
<li>Freeze not upgraded chains
<ul>
<li>After a certain time the chains that are not upgraded are frozen.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="bridgehub-1"><a class="header" href="#bridgehub-1">Bridgehub</a></h1>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<p>Bridgehub is the main chain registry contract for the ecosystem, that stores:</p>
<ul>
<li>A mapping from chainId to chains address</li>
<li>A mapping from chainId to the CTM it belongs to.</li>
<li>A mapping from chainId to its base token (i.e. the token that is used for paying fees)</li>
<li>Whitelisted settlement layers (i.e. Gateway)</li>
</ul>
<p>Note sure what CTM is? Check our the <a href="specs/contracts/chain_management/./chain_type_manager.html">overview</a>.</p>
<blockquote>
<p>This document will not cover how ZK Gateway works, you can check it out in [a separate doc][../gateway/overview.md].</p>
</blockquote>
<p>The Bridgehub is the contract where new chains can <a href="specs/contracts/chain_management/./chain_genesis.html">register</a>. The Bridgehub also serves as an AssetHandler for chains when migrating chains between settlement layers, read more about it [here][TODO].</p>
<p>Overall, it is the main registry for all the contracts. Note, that a clone of Bridgehub is also deployed on each L2 chain, it is used to start interop txs by checking that the chain is active. It is also used on settlement layers such as Gateway. All the in all, the architecture of the entire ecosystem can be seen below:</p>
<p><img src="specs/contracts/chain_management/./img/ecosystem_architecture.png" alt="Contracts" /></p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="chain-type-manager-ctm"><a class="header" href="#chain-type-manager-ctm">Chain Type Manager (CTM)</a></h1>
<blockquote>
<p>If someone is already familiar with the <a href="https://github.com/code-423n4/2024-03-zksync">previous version</a> of ZKsync architecture, this contract was previously known as “State Transition Manager (STM)”.</p>
</blockquote>
<p>Currently bridging between different zk rollups requires the funds to pass through L1. This is slow &amp; expensive.</p>
<p>The vision of seamless internet of value requires transfers of value to be <em>both</em> seamless and trustless. This means that for instance different chains need to share the same L1 liquidity, i.e. a transfer of funds should never touch L1 in the process. However, it requires some sort of trust between two chains. If a malicious (or broken) rollup becomes a part of the shared liquidity pool it can steal all the funds.</p>
<p>However, can two instances of the same zk rollup trust each other? The answer is yes, because no new additions of rollups introduce new trust assumptions. Assuming there are no bugs in circuits, the system will work as intended.</p>
<p>How can two rollups know that they are two different instances of the same system? We can create a factory of such contracts (and so we would know that each new rollup created by this instance is correct one). But just creating correct contracts is not enough. Ethereum changes, new bugs may be found in the original system &amp; so an instance that does not keep itself up-to-date with the upgrades may exploit some bug from the past and jeopardize the entire system. Just deploying is not enough. We need to constantly make sure that all chains are up to date and maintain whatever other invariants are needed for these chains to trust each other.</p>
<p>Let’s define as <em>Chain Type Manager</em> (CTM) **as a contract that is responsible for the following:</p>
<ul>
<li>It serves as a factory to deploy new ZK chains.</li>
<li>It is responsible for ensuring that all the chains deployed by it are up-to-date.</li>
</ul>
<p>Note, that this means that chains have a “weaker” governance. I.e. governance can only do very limited number of things, such as setting the validator. Chain admin can not set its own upgrades and it can only “execute” the upgrade that has already been prepared by the CTM.</p>
<p>In the long term vision chains deployment will be permissionless, however CTM will always remain the main point of trust and will have to be explicitly whitelisted by the decentralized governance of the entire ecosystem before its chain can get the access to the shared liquidity.</p>
<h2 id="configurability-in-the-current-release"><a class="header" href="#configurability-in-the-current-release">Configurability in the current release</a></h2>
<p>For now, only one CTM will be supported — the one that deploys instances of ZKsync Era, possibly using other DA layers. To read more about different DA layers, check out [this document][TODO].</p>
<p>The exact process of deploying &amp; registering a chain can be <a href="specs/contracts/chain_management/./chain_genesis.html">read here</a>. Overall, each chain in the current release will have the following parameters:</p>
<div class="table-wrapper"><table><thead><tr><th>Chain parameter</th><th>Updatability</th><th>Comment</th></tr></thead><tbody>
<tr><td>chainId</td><td>Permanent</td><td>Permanent identifier of the chain. Due to wallet support reasons, for now chainId has to be small (48 bits). This is one of the reasons why for now we’ll deploy chains manually, to prevent them from having the same chainId as some another popular chain. In the future it will be trustlessly assigned as a random 32-byte value.</td></tr>
<tr><td>baseTokenAssetId</td><td>Permanent</td><td>Each chain can have their own custom base token (i.e. token used for paying the fees). It is set once during creation and can never be changed. Note, that we refer to and “asset id” here instead of an L1 address. To read more about what is assetId and how it works check out the document for <a href="specs/contracts/chain_management/../bridging/asset_router_and_ntv/asset_router.html">asset router</a></td></tr>
<tr><td>chainTypeManager</td><td>Permanent</td><td>The CTM that deployed the chain. In principle, it could be possible to migrate between CTMs (assuming both CTMs support that). However, in practice it may be very hard and as of now such functionality is not supported.</td></tr>
<tr><td>admin</td><td>By admin of chain</td><td>The admin of the chain. It has some limited powers to govern the chain. To read more about which powers are available to a chain admin and which precautions should be taken, check <a href="specs/contracts/chain_management/./admin_role.html">out this document</a></td></tr>
<tr><td>validatorTimelock</td><td>CTM</td><td>For now, we want all the chains to use the same 3h timelock period before their batches are finalized. Only CTM can update the address that can submit state transitions to the rollup (that is, the validatorTimelock).</td></tr>
<tr><td>validatorTimelock.validator</td><td>By admin of chain</td><td>The admin of chain can choose who can submit new batches to the ValidatorTimelock.</td></tr>
<tr><td>priorityTx FeeParams</td><td>By admin of chain</td><td>The admin of a ZK chain can amend the priority transaction fee params.</td></tr>
<tr><td>transactionFilterer</td><td>By admin of chain</td><td>A chain may put an additional filter to the incoming L1-&gt;L2 transactions. This may be needed by a permissioned chain (e.g. a Validium bank-like corporate chain).</td></tr>
<tr><td>DA validation / permanent rollup status</td><td>By admin of chain</td><td>A chain can decide which DA layer to use. You check out more about <a href="specs/contracts/chain_management/./admin_role.html">safe DA management here</a></td></tr>
<tr><td>executing upgrades</td><td>By admin of chain</td><td>While exclusively CTM governance can set the content of the upgrade, chains will typically be able to choose suitable time for them to actually execute it. In the current release, chains will have to follow our upgrades.</td></tr>
<tr><td>settlement layer</td><td>By admin of chain</td><td>The admin of the chain can enact migrations to other settlement layers.</td></tr>
</tbody></table>
</div>
<blockquote>
<p>Note, that if we take a look at the access control for the corresponding functions inside the <a href="https://github.com/matter-labs/era-contracts/tree/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/state-transition/chain-deps/facets/Admin.sol">AdminFacet</a>, the may see that a lot of methods from above that are marked as “By admin of chain” could be in theory amended by the ChainTypeManager. However, this sort of action requires approval from decentralized governance. Also, in case of an urgent high risk situation, the decentralized governance might force upgrade the contract via CTM.</p>
</blockquote>
<h2 id="upgradability-in-the-current-release"><a class="header" href="#upgradability-in-the-current-release">Upgradability in the current release</a></h2>
<p>In the current release, each chain will be an instance of ZKsync Era and so the upgrade process of each individual chain will be similar to that of ZKsync Era.</p>
<ol>
<li>
<p>Firstly, the governance of the CTM will publish the server (including sequencer, prover, etc) that support the new version . This is done offchain. Enough time should be given to various zkStack devs to update their version.</p>
</li>
<li>
<p>The governance of the CTM will publish the upgrade onchain by automatically executing the following three transactions:</p>
<ul>
<li><code>setChainCreationParams</code> ⇒ to ensure that new chains will be created with the version</li>
<li><code>setValidatorTimelock</code> (if needed) ⇒ to ensure that the new chains will use the new validator timelock right-away</li>
<li><code>setNewVersionUpgrade</code> ⇒ to save the upgrade information that each chain will need to follow to conduct the upgrade on their side.</li>
</ul>
</li>
<li>
<p>After that, each ChainAdmin can upgrade to the new version in suitable time for them.</p>
</li>
</ol>
<blockquote>
<p>Note, that while the governance does try to give the maximal possible time for chains to upgrade, the governance will typically put restrictions (aka deadlines) on the time by which the chain has to be upgraded. If the deadline is passed, the chain can not commit new batches until the upgrade is executed.</p>
</blockquote>
<h3 id="emergency-upgrade"><a class="header" href="#emergency-upgrade">Emergency upgrade</a></h3>
<p>In case of an emergency, the <a href="https://blog.zknation.io/introducing-zk-nation/">security council</a> has the ability to freeze the ecosystem and conduct an emergency upgrade.</p>
<p>In case we are aware that some of the committed batches on a chain are dangerous to be executed, the CTM can call <code>revertBatches</code> on that chain. For faster reaction, the admin of the ChainTypeManager has the ability to do so without waiting for govenrnace approval that may take a lot of time. This action does not lead to funds being lost, so it is considered suitable for the partially trusted role of the admin of the ChainTypeManager.</p>
<h3 id="issues--caveats"><a class="header" href="#issues--caveats">Issues &amp; caveats</a></h3>
<ul>
<li>If an ZK chain skips an upgrade (i.e. it has version X, it did not upgrade to <code>X + 1</code> and now the latest protocol version is <code>X + 2</code> there is no built-in way to upgrade). This team will require manual intervention from us to upgrade.</li>
<li>The approach of calling <code>revertBatches</code> for malicious chains is not scalable (O(N) of the number of chains). The situation is very rare, so it is fine in the short term, but not in the long run.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="safe-chainadmin-management"><a class="header" href="#safe-chainadmin-management">Safe ChainAdmin management</a></h1>
<p>While the ecosystem does a <a href="https://blog.zknation.io/introducing-zk-nation/">decentralized trusted governance</a>, each chain has its own Chain Admin. While the upgrade parameters are chosen by the governance, chain admin is still a powerful role and should be managed carefully.</p>
<p>In this document we will explore what are the abilities of the ChainAdmin, how dangerous they are and how to mitigate potential issues.</p>
<h2 id="general-guidelines"><a class="header" href="#general-guidelines">General guidelines</a></h2>
<p>The system does not restrict in any way how the admin of the chain should be implemented. However special caution should be taken to keep it safe.</p>
<p>The general guideline is that an admin of a ZK chain should be <em>at least</em> a well-distributed multisig. Having it as an EOA is definitely a bad idea since having this address stolen can lead to <a href="specs/contracts/chain_management/admin_role.html#setting-da-layer">chain being permanently frozen</a>.</p>
<p>Additional measures may be taken <a href="specs/contracts/chain_management/admin_role.html#proposed-modular-chainadmin-implementation">to self-restrict</a> the ChainAdmin to ensure that some operations can be only done in safe fashion.</p>
<p>Generally all the functionality of chain admin should be treated with maximal security and caution, and having hotkey separate roles in rare circuimstances, e.g. to call <code>setTokenMultiplier</code> in case of an ERC-20 based chain.</p>
<h2 id="chain-admin-functionality"><a class="header" href="#chain-admin-functionality">Chain Admin functionality</a></h2>
<h3 id="setting-validators-for-a-chain"><a class="header" href="#setting-validators-for-a-chain">Setting validators for a chain</a></h3>
<p>The admin of a chain can call <code>ValidatorTimelock</code> on the settlement layer to add or remove validators, i.e. addresses that have the right to <code>commit</code>/<code>verify</code>/<code>execute</code> batches etc.</p>
<p>The system is protected against malicious validators, they can never steal funds from users. However, this role is still relatively powerful: If the DA layer is not reliable, and a batch does get executed, the funds may be frozen. This is why the chains should be <a href="specs/contracts/chain_management/admin_role.html#setting-da-layer">cautious about DA layers that they use</a>. Note, that on L1 the <code>ValidatorTimelock</code> has 3h delay, while on Gateway this timelock will not be present.</p>
<p>In case the malicious block has not been executed yet, it can be reverted.</p>
<h3 id="setting-da-layer"><a class="header" href="#setting-da-layer">Setting DA layer</a></h3>
<p>This is one of the most powerful settings that a chain can have: setting a custom DA layer. The dangers of doing this wrong are obvious: lack of proper data availability solution may lead to funds being frozen. Term “frozen funds” mainly refers to the inability to reconstruct the complete state since externally only the root hash is visible, thus preventing the chain from progressing. (Note: that funds can never be <em>stolen</em> due to ZKP checks of the VM execution).</p>
<p>Sometimes, users may need assurances that a chain will never become frozen even under a malicious chain admin. A general though unstable approach is discussed <a href="specs/contracts/chain_management/admin_role.html#proposed-modular-chainadmin-implementation">here</a>, however this release comes with a solution specially tailored for rollups: the <code>isPermanentRollup</code> setting.</p>
<h4 id="ispermanentrollup-setting"><a class="header" href="#ispermanentrollup-setting"><code>isPermanentRollup</code> setting</a></h4>
<p>Chain also exposes the <code>AdminFacet.makePermanentRollup</code> function. It will turn a chain into a permanent rollup, ensuring that DA validator pairs can be only set to values that are approved by decentralized governance to be used for rollups.</p>
<p>This functionality is obviously dangerous in a sense that it is permanent and revokes the right of the chain to change its DA layer. On the other hand, it ensures perpetual safety for users. This is the option that ZKsync Era is using.</p>
<p>This setting is preserved even when migrating to [gateway][TODO]. If this setting was set while chain is on top of Gateway, and it migrates back to L1, it will keep this status, i.e. it is fully irrevocable.</p>
<h3 id="changefeeparams-method"><a class="header" href="#changefeeparams-method"><code>changeFeeParams</code> method</a></h3>
<p>This method allows to change how the fees are charged for priority operations.</p>
<p>The worst impact of setting this value wrongly is having L1-&gt;L2 transactions underpriced.</p>
<h3 id="settokenmultiplier-method"><a class="header" href="#settokenmultiplier-method"><code>setTokenMultiplier</code> method</a></h3>
<p>This method allows to set the token multiplier, i.e. the ratio between the price of ETH and the price of the token. It will be used for L1-&gt;L2 priority transactions.</p>
<p>Typically, <code>ChainAdmin</code>s of ERC20 chains will have a special hotkey responsible for calling this function to keep the price up to date. An example on how it is implemented in the current system can be seen <a href="https://github.com/matter-labs/era-contracts/blob/aafee035db892689df3f7afe4b89fd6467a39313/l1-contracts/contracts/governance/ChainAdmin.sol#L23">here</a>.</p>
<p>The worst impact of setting this value wrongly is having L1-&gt;L2 transactions underpriced.</p>
<h3 id="setpubdatapricingmode"><a class="header" href="#setpubdatapricingmode"><code>setPubdataPricingMode</code></a></h3>
<p>This method allows to set whether the pubdata price will be taken into account for priority operations.</p>
<p>The worst impact of setting this value wrongly is having L1-&gt;L2 transactions underpriced.</p>
<h3 id="settransactionfilterer"><a class="header" href="#settransactionfilterer"><code>setTransactionFilterer</code></a></h3>
<p>This method allows to set a transaction filterer, i.e. an additional validator for all incoming L1-&gt;L2 transactions. The worst impact is users’ transactions being censored.</p>
<h3 id="migration-to-another-settlement-layer"><a class="header" href="#migration-to-another-settlement-layer">Migration to another settlement layer</a></h3>
<p>The upgrade can start migration of a chain to another settlement layer. Currently all the settlement layers are whitelisted, so generally this operation is harmless (except for the inconvenience in case the migration was unplanned).</p>
<p>However, some caution needs to be applied to migrate properly as described in the section below.</p>
<h2 id="chain-admin-when-migrating-to-gateway"><a class="header" href="#chain-admin-when-migrating-to-gateway">Chain admin when migrating to gateway</a></h2>
<p>When a chain migrates to gateway, it provides the address of the new admin on L2. The following rules apply:</p>
<ul>
<li>If a ZK chain has already been deployed on a settlement layer, its admin stays the same.</li>
<li>If a ZK chain has not been deployed yet, then the new admin is set.</li>
</ul>
<p>The above means that in the current release the admin of the chain on the new settlement layer is “detached” from the admin on L1. It is the responsibility of the chain to set the L2 admin correctly: either it should have the same signers or, even better in the long run, put the aliased L1 admin to have most of the abilities inside the L2 chain admin.</p>
<p>Since most of the Admin’s functionality above are related to L1-&gt;L2 operations, the L1 chain admin will continue playing a crucial role even after the chain migrates to Gateway. However, some of the new functionality are relevant on the chain admin on the settlement layer only:</p>
<ul>
<li>Managing DA</li>
<li>Managing new validators</li>
<li>It is the admin of the settlement layer that do migrations of chains</li>
</ul>
<p>As such, the choice of the L2 Admin is very important. Also, if the chain admin on the new settlement layer is not accessible (e.g. accidentally wrong address was chosen), the chain is lost:</p>
<ul>
<li>No validators will be set</li>
<li>The chain can not migrate back</li>
</ul>
<p>Overall <strong>very special care</strong> needs to be taken when selecting an admin for the migration to a new settlement layer.</p>
<h2 id="proposed-modular-chainadmin-implementation"><a class="header" href="#proposed-modular-chainadmin-implementation">Proposed modular <code>ChainAdmin</code> implementation</a></h2>
<blockquote>
<p><strong>Warning</strong>. The proposed implementation here will likely <strong>not</strong> be used by the Matter Labs team for ZKsync Era due to the issues listed in the issues section. This code, however, is still in scope of the audit and may serve as a future basis of a more long term solution.</p>
</blockquote>
<p>In order to ensure that the architecture here flexible enough for future other chains to use, it uses a modular architecture to ensure that other chains could fit it to their needs. By default, this contract is not even <code>Ownable</code>, and anyone can execute transactions out of the name of it. In order to add new features such as restricting calling dangerous methods and access control, <em>restrictions</em> should be added there. Each restriction is a contract that implements the <code>IRestriction</code> interface. The following restrictions have been implemented so far:</p>
<ul>
<li>
<p><code>AccessControlRestriction</code> that allows to specify which addresses can call which methods. In the case of Era, only the <code>DEFAULT_ADMIN_ROLE</code> will be able to call any methods. Other chains with non-ETH base token may need an account that would periodically call the L1 contract to update the ETH price there. They may create the <code>SET_TOKEN_MULTIPLIER_ROLE</code> role that is required to update the token price and give its rights to some hot private key.</p>
</li>
<li>
<p><code>PermanentRestriction</code> that ensures that:</p>
</li>
</ul>
<p>a) This restriction could not be lifted, i.e. the chain admin of the chain must forever have it. Even if the address of the <code>ChainAdmin</code> changes, it ensures that the new admin has this restriction turned on.
b) It specifies the calldata this which certain methods can be called. For instance, in case a chain wants to keep itself permanently tied to certain DA, it will ensure that the only DA validation method that can be used is rollup. Some sort of decentralized governance could be chosen to select which DA validation pair corresponds to this DA method.</p>
<p>The approach above does not only helps to protect the chain, but also provides correct information for chains that are present in our ecosystem. For instance, if a chain claims to perpetually have a certain property, having the <code>PermanentRestriction</code> as part of the chain admin can ensure all observers of that.</p>
<h3 id="issues-and-limitations"><a class="header" href="#issues-and-limitations">Issues and limitations</a></h3>
<p>Due to specifics of <a href="specs/contracts/chain_management/admin_role.html#migration-to-another-settlement-layer">migration to another settlement layers</a> (i.e. that migrations do not overwrite the admin), maintaining the same <code>PermanentRestriction</code> becomes hard in case a restriction has been added on top of the chain admin inside one chain, but not the other.</p>
<p>While very flexible, this modular approach should still be polished enough before recommending it as a generic solution for everyone. However, the provided new <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/governance/ChainAdmin.sol">ChainAdmin</a> can still be helpful for new chains as with the <code>AccessControlRestriction</code> it provides a ready-to-use framework for role-based managing of the chain. Using <code>PermanentRestriction</code> for now is discouraged however.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-new-chains-with-bridgehub"><a class="header" href="#creating-new-chains-with-bridgehub">Creating new chains with BridgeHub</a></h1>
<p>The main contract of the whole ZK Chains ecosystem is called <em><code>BridgeHub</code></em>. It contains:</p>
<ul>
<li>the registry from chainId to CTMs that is responsible for that chainId</li>
<li>the base token for each chainId.</li>
<li>the whitelist of CTMs</li>
<li>the whitelist of tokens allowed to be <code>baseTokens</code> of chains.</li>
<li>the whitelist of settlement layers</li>
<li>etc</li>
</ul>
<p>BridgeHub is responsible for creating new chains. It is also the main point of entry for L1→L2 transactions for all the chains. Users won’t be able to interact with chains directly, all the actions must be done through the BridgeHub, which will ensure that the fees have been paid and will route the call to the corresponding chain. One of the reasons it was done this way was to have the unified interface for all chains that will ever be included in the ZK Chains ecosystem.</p>
<p>To create a chain, the <code>BridgeHub.createNewChain</code> function needs to be called:</p>
<pre><code class="language-solidity">/// @notice register new chain. New chains can be only registered on Bridgehub deployed on L1. Later they can be moved to any other layer.
/// @notice for Eth the baseToken address is 1
/// @param _chainId the chainId of the chain
/// @param _chainTypeManager the state transition manager address
/// @param _baseTokenAssetId the base token asset id of the chain
/// @param _salt the salt for the chainId, currently not used
/// @param _admin the admin of the chain
/// @param _initData the fixed initialization data for the chain
/// @param _factoryDeps the factory dependencies for the chain's deployment
function createNewChain(
    uint256 _chainId,
    address _chainTypeManager,
    bytes32 _baseTokenAssetId,
    // solhint-disable-next-line no-unused-vars
    uint256 _salt,
    address _admin,
    bytes calldata _initData,
    bytes[] calldata _factoryDeps
) external
</code></pre>
<p>BridgeHub will check that the CTM as well as the base token are whitelisted and route the call to the CTM</p>
<p><img src="specs/contracts/chain_management/./img/create_new_chain.png" alt="newChain (2).png" /></p>
<h3 id="creation-of-a-chain-in-the-current-release"><a class="header" href="#creation-of-a-chain-in-the-current-release">Creation of a chain in the current release</a></h3>
<p>In the future, chain creation will be permissionless. A securely random <code>chainId</code> will be generated for each chain to be registered. However, generating 32-byte chainId is not feasible with the current SDK expectations on EVM and so for now chainId is of type <code>uint48</code>. And so it has to be chosen by the admin of <code>BridgeHub</code>. Also, for the current release we would want to avoid chains being able to choose their own initialization parameter to prevent possible malicious input.</p>
<p>For this reason, there will be an entity called <code>admin</code> which is basically a hot key managed by us and it will be used to deploy new chains.</p>
<p>So the flow for deploying their own chain for users will be the following:</p>
<ol>
<li>Users tell us that they want to deploy a chain with certain governance, CTM (we’ll likely allow only one for now), and baseToken.</li>
<li>Our server will generate a chainId not reserved by any other major chain and the <code>admin</code> will call the <code>BridgeHub.createNewChain</code> . This will call the <code>CTM.createNewChain</code> that will deploy the instance of the rollup as well as initialize the first transaction there — the system upgrade transaction needed to set the chainId on L2.</li>
</ol>
<p>After that, the chain is ready to be used. Note, that the admin of the newly created chain (this will be the organization that will manage this chain from now on) will have to conduct certain configurations before the chain <a href="specs/contracts/chain_management/../chain_management/admin_role.html">can be used securely</a>.</p>
<h2 id="built-in-contracts-and-their-initialization"><a class="header" href="#built-in-contracts-and-their-initialization">Built-in contracts and their initialization</a></h2>
<p>Each single ZK Chain has a set of the following contracts that, while not belong to kernel space, are built-in and provide important functionality:</p>
<ul>
<li>Bridgehub (the source code is identical to the L1 one). The role of bridgehub is to facilitate cross chain transactions. It contains a mapping from chainId to the address of the diamond proxy of the chain. It is really used only on the L1 and Gateway, i.e. layers that can serve as a settlement layer.</li>
<li>L2AssetRouter. The new iteration of the SharedBridge.</li>
<li>L2NativeTokenVault. The Native token vault on L2.</li>
<li>MessageRoot (the source code is identical to the L1 one). Similar to bridgehub, it facilitates cross-chain communication, but is practically unused on all chains except for L1/GW.</li>
</ul>
<p>It should be noted that those contracts are deployed at genesis and have the same address on all ZK Chains.</p>
<p>The exact addresses for those contracts are (with links to ZKsync Era explorer):</p>
<ul>
<li>Bridgehub – <a href="https://explorer.zksync.io/address/0000000000000000000000000000000000010002#contract"><code>0x0000000000000000000000000000000000010002</code></a>.</li>
<li>L2AssetRouter – <a href="https://explorer.zksync.io/address/0000000000000000000000000000000000010003#contract"><code>0x0000000000000000000000000000000000010003</code></a>.</li>
<li>L2NativeTokenVault – <a href="https://explorer.zksync.io/address/0000000000000000000000000000000000010004#contract"><code>0x0000000000000000000000000000000000010004</code></a>.</li>
<li>MessageRoot – <a href="https://explorer.zksync.io/address/0000000000000000000000000000000000010005#contract"><code>0x0000000000000000000000000000000000010005</code></a>.</li>
</ul>
<p>To reuse as much code as possible from L1 and also to allow easier initialization, most of these contracts are not initialized as just part of the genesis storage root. Instead, the data for their initialization is part of the original diamondcut for the chain. In the same initial upgrade transaction when the chainId is initialized, these contracts are force-deployed and initialized also. An important part in it plays the new <code>L2GenesisUpgrade</code> contract, which is pre-deployed in a user-space contract, but it is delegate-called by the <code>ComplexUpgrader</code> system contract (already exists as part of genesis and existed before this upgrade).</p>
<h2 id="additional-limitations-for-the-current-version"><a class="header" href="#additional-limitations-for-the-current-version">Additional limitations for the current version</a></h2>
<p>In the current version creating new chains will not be permissionless. That is needed to ensure that no malicious input can be provided there.</p>
<h3 id="limitations-of-custom-base-tokens-in-the-current-release"><a class="header" href="#limitations-of-custom-base-tokens-in-the-current-release">Limitations of custom base tokens in the current release</a></h3>
<p>ZKsync Era uses ETH as a base token. Upon creation of an ZKChain other chains may want to use their own custom base tokens. Note, that for the current release all the possible base tokens are whitelisted. The other limitation is that all the base tokens must be backed 1-1 on L1 as well as they are solely implemented with <code>L2BaseToken</code> contract. In other words:</p>
<ul>
<li>No custom logic is allowed on L2 for base tokens</li>
<li>Base tokens can not be minted on L2 without being backed by the corresponding L1 amount.</li>
</ul>
<p>If someone wants to build a protocol that mints base tokens on L2, the option for now is to “mint” an infinite amount of those on L1, deposit on L2 and then give those out as a way to “mint”. We will update this in the future.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="upgrade-process-document"><a class="header" href="#upgrade-process-document">Upgrade process document</a></h1>
<h2 id="intro"><a class="header" href="#intro">Intro</a></h2>
<p>This document assumes that you have understanding about <a href="specs/contracts/chain_management/../settlement_contracts/zkchain_basics.html">the structure</a> on individual chains’ L1 contracts.</p>
<p>Upgrading the ecosystem of ZKChains is a complicated process. ZKSync is a complex ecosystem with many chains and contracts and each upgrade is unique, but there are some steps that repeat for most upgrades. These are mostly how we interact with the CTM, the diamond facets, the L1→L2 upgrade, how we update the verification keys.</p>
<p>Where each upgrade consists of two parameters:</p>
<ul>
<li>Facet cuts - change of the internal implementation of the diamond proxy</li>
<li>Diamond Initialization - delegate call to the specified address with specified data</li>
</ul>
<p>The second parameter is very powerful and flexible enough to move majority of upgrade logic there.</p>
<h2 id="preparation-for-the-upgrade"><a class="header" href="#preparation-for-the-upgrade">Preparation for the upgrade</a></h2>
<p>The ZKsync ecosystem has <a href="https://github.com/zksync-association/zk-governance">governance smart contracts</a> that govern the protocol. Only these contracts have the permission to set upgrades in the CTM. This is done via the <code>setNewVersionUpgrade</code> function. This sets the upgrade data, the new protocol version, and the deadline by which chains have to upgrade. Chains can upgrade themselves with the same data. After the deadline is over, each non-upgraded chain is frozen, they cannot post new proofs. Frozen chains can
unfreeze by updating.</p>
<h2 id="upgrade-structure"><a class="header" href="#upgrade-structure">Upgrade structure</a></h2>
<p>Upgrade information is composed in the form of a <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/state-transition/libraries/Diamond.sol#L75">DiamondCutData</a> struct. During the upgrade, the chain’s DiamondProxy will delegateCall the <code>initAddress</code> with the provided <code>initCalldata</code>, while the facets that the <code>DiamondProxy</code> will be changed according to the <code>facetCuts</code>. This scheme is very powerful and it allows to change anything in the contract. However, we typically have a very specific set of changes that we need to do. To facilitate these, two contracts have been created:</p>
<ol>
<li><a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/upgrades/BaseZkSyncUpgrade.sol">BaseZkSyncUpgrade</a> - Generic template with function that can be useful for upgrades</li>
<li><a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/upgrades/DefaultUpgrade.sol">DefaultUpgrade</a> - Default implementation of the <code>BaseZkSyncUpgrade</code>, contract that is most often planned to be used as diamond initialization when doing upgrades.</li>
</ol>
<blockquote>
<p>Note, that the Gateway upgrade (v26) was more complex than the usual ones and so a similar, but separate [process][TODO] was used for it. It also used its own custom implementation of the <code>BaseZkSyncUpgrade</code>: <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/upgrades/GatewayUpgrade.sol">GatewayUpgrade</a>.</p>
</blockquote>
<h3 id="protocol-version"><a class="header" href="#protocol-version">Protocol version</a></h3>
<p>For tracking upgrade versions on different networks (private testnet, public testnet, mainnet) we use protocol version, which is basically just a number denoting the deployed version. The protocol version is different from Diamond Cut <code>proposalId</code>, since <code>protocolId</code> only shows how much upgrade proposal was proposed/executed, but nothing about the content of upgrades, while the protocol version is needed to understand what version is deployed.</p>
<p>In the <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/upgrades/BaseZkSyncUpgrade.sol">BaseZkSyncUpgrade</a> &amp; <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/upgrades/DefaultUpgrade.sol">DefaultUpgrade</a> we allow to arbitrarily increase the proposal version while upgrading a system, but only increase it. We are doing that since we can skip some protocol versions if for example found a bug there (but it was deployed on another network already).</p>
<h2 id="protocol-upgrade-transaction"><a class="header" href="#protocol-upgrade-transaction">Protocol upgrade transaction</a></h2>
<p>During upgrade, we typically need not only update the L1 contracts, but also the L2 ones. This is achieved by creating an upgrade transactions. More details on how those are processed inside the system can be read [here][(../settlement_contracts/priority_queue/l1_l2_communication/l1_to_l2.md)].</p>
<h2 id="whitelisting-and-executing-upgrade"><a class="header" href="#whitelisting-and-executing-upgrade">Whitelisting and executing upgrade</a></h2>
<p>Note, that due to how powerful the upgrades are, if we allowed any <a href="specs/contracts/chain_management/../chain_management/admin_role.html">chain admin</a> to inact any upgrade it wants, it could allow malicious chains to potentially break some of the ecosystem invariants. Because of that, any upgrade should be firstly whitelisted by the decentralized governance through calling the <code>setNewVersionUpgrade</code> function of the <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/state-transition/ChainTypeManager.sol">ChainTypeManager</a>.</p>
<p>In order to execute the upgrade, the chain admin would call the <code>upgradeChainFromVersion</code> function from the <a href="https://github.com/matter-labs/era-contracts/blob/8222265420f362c853da7160769620d9fed7f834/l1-contracts/contracts/state-transition/chain-deps/facets/Admin.sol">Admin</a> facet.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<!--- This document will be extended once interop docs come in place --->
<h1 id="overview-of-bridging"><a class="header" href="#overview-of-bridging">Overview of bridging</a></h1>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<p>ZK Stack chains are launched on L1 into an ecosystem of contracts with the main registry being the <a href="specs/contracts/bridging/../chain_management/bridgehub.html">bridgehub</a>. The Bridgehub creates an
ecosystem of chains, with shared standards, upgrades. This allows chains to trust each other. The bridging of assets is handled by the <a href="specs/contracts/bridging/./asset_router_and_ntv/asset_router.html">AssetRouter</a> and NativeTokenVault contracts.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="assetrouter"><a class="header" href="#assetrouter">AssetRouter</a></h1>
<blockquote>
<p><strong>Overview:</strong></p>
<ul>
<li><strong>On L1:</strong> The <strong>L1 Asset Router</strong> handles cross‑chain asset coordination <em>and</em> is complemented by the <strong>L1 Nullifier</strong> for additional finalization and recovery features.</li>
<li><strong>On L2:</strong> Only the <strong>L2 Asset Router</strong> is pre-deployed at the same address (<code>0x10003</code>) on every L2 chain.</li>
</ul>
</blockquote>
<h3 id="assetrouter-l1l2"><a class="header" href="#assetrouter-l1l2">AssetRouter (L1/L2)</a></h3>
<p>The main job of the asset router is to be the central point of coordination for asset bridging. All crosschain token bridging is done between asset routers only and once the message reaches asset router, it then routes it to the corresponding asset handler.</p>
<p>In order to make this easier, all L2 chains have the asset router located on the same address on every chain. It is <code>0x10003</code> and it is pre-deployed contract. More on how it is deployed can be seen in the <a href="specs/contracts/bridging/asset_router_and_ntv/../../chain_management/chain_genesis.html">Chain Genesis</a> section.</p>
<p>The endgame is to have L1 asset router have the same functionality as the L2 one. This is not the case yet, but some progress has been made: L2AssetRouter can now bridge L2-native assets to L1, from which it could be bridged to other chains in the ecosystem.</p>
<p>Examples of differences in functionality are:</p>
<ol>
<li>Failed‐deposit recovery.
The <code>L1AssetRouter</code> provides on-chain recovery for failed L1→L2 deposits via its <code>bridgeRecoverFailedTransfer</code> and <code>claimFailedDeposit</code> functions. The <code>L2AssetRouter</code> has no equivalent, because L2→L1 withdrawals are atomic on L2: burning the L2 token and sending the L2→L1 withdrawal message occur in the same transaction. If anything goes wrong, the entire transaction reverts and no token is burned and no message is sent.</li>
<li>User-initiated L1→L2 deposits must go through Bridgehub (or the legacy bridge), not directly.
There is no public <code>deposit(...)</code> on <code>L1AssetRouter</code> you can call as a user—only, by contrast, on L2 you have a public <code>withdraw(...)</code> method.
In case of <code>L1AssetRouter</code> user has to start deposit through either <code>Bridgehub</code> or <code>L1ERC20Bridge</code> (legacy bridge).
However in case of <code>L2AssetRouter</code> user is able to simply call <code>withdraw</code> function in <code>L2AssetRouter</code>.</li>
</ol>
<p>The specifics of the L2AssetRouter is the need to interact with the previously deployed L2SharedBridgeLegacy if it was already present. It has less “rights” than the L1AssetRouter: at the moment it is assumed that all asset deployment trackers are from L1, the only way to register an asset handler on L2 is to make an L1→L2 transaction.</p>
<blockquote>
<p>Note, that today registering new asset deployment trackers is permissioned, but the plan is to make it permissionless in the future</p>
</blockquote>
<p>The specifics of the L1AssetRouter come from the need to be backwards compatible with the old L1SharedBridge. Yes, it does not share the same storage, but it inherits the need to be backwards compatible with the current SDK. Also, L1AssetRouter needs to facilitate L1-only operations, such as recovering from failed deposits.</p>
<p>Also, L1AssetRouter is the only base token bridge contract that can participate in initiation of cross chain transactions via the bridgehub. This might change in the future.</p>
<h3 id="l1nullifier"><a class="header" href="#l1nullifier">L1Nullifier</a></h3>
<p>While the endgoal is to unify L1 and L2 asset routers, in reality, it may not be that easy: while L2 asset routers get called by L1→L2 transactions, L1 ones don’t and require manual finalization of transactions, which involves proof verification, etc. To move this logic outside of the L1AssetRouter, it was moved into a separate L1Nullifier contract.</p>
<p><em>This is the contract the previous L1SharedBridge had been upgraded to, so it has the backwards compatible storage.</em></p>
<h3 id="a-separate-l2nullifier-does-not-exist"><a class="header" href="#a-separate-l2nullifier-does-not-exist">A separate L2Nullifier does not exist</a></h3>
<p>The L1Nullifier stores two things:</p>
<ol>
<li>
<p>finalized L2-&gt;L1 withdrawals</p>
</li>
<li>
<p>initiated L1-&gt;L2 priority transactions</p>
</li>
<li>
<p>is needed on L1 as L2-&gt;L1 txs are executed arbitrarily, so we need to record whether they happened or not to stop double spending. However, L1-&gt;L2 priority txs are enforced to be executed only once, so we don’t need a separate L2Nullifier.</p>
</li>
<li>
<p>The initiated L2-&gt;L1 and L2-&gt;L2 transactions are not stored. This is needed for L1-&gt;L2 txs, as priority transactions have to be executed by the system, and cannot be retried if they fail. So failed deposits have to be redeemable on L1. L2-&gt;L2 and L2-&gt;L1 txs might also fail, but if they fail due to gas reasons, they can be retried. If they fail due to contract error, then the receiving contract has to be fixed ( another way of explaining it, there can always be a contract error even for claiming failed deposits. So the only sustainable way of fixing contract errors is to fix the contract).</p>
</li>
</ol>
<p>For this reason, on the L2 claiming failed deposits and bridgehubConfirmL2Transaction are not implemented.</p>
<h3 id="l2sharedbridgelegacy"><a class="header" href="#l2sharedbridgelegacy">L2SharedBridgeLegacy</a></h3>
<p>L2AssetRouter is pre-deployed onto a specific address. The old L2SharedBridge is upgraded to L2SharedBridgeLegacy contract. The main purpose of this contract is to ensure compatibility with the incoming deposits and re-route them to the shared bridge.</p>
<p>This contract is never deployed for new chains.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="native-token-vault"><a class="header" href="#native-token-vault">Native Token Vault</a></h1>
<h3 id="nativetokenvault-l1l2"><a class="header" href="#nativetokenvault-l1l2">NativeTokenVault (L1/L2)</a></h3>
<p>NativeTokenVault is an asset handler that is available on all chains and is also predeployed. It provides the functionality of the most basic bridging: locking funds on one chain and minting the bridged equivalent on the other one. On L2 chains NTV is predeployed at the <code>0x10004</code> address. NativeTokenVault acts as the default AssetHandler, so regular ERC-20 tokens can use it unless custom bridging logic or special features are required.</p>
<p>The L1 and L2 versions of the NativeTokenVault share the same core functionality, but differ in their deployment mechanics and certain L1-specific responsibilities.</p>
<p>On L1, the contract is deployed using standard CREATE2, while on L2 it uses low-level calls to the CONTRACT_DEPLOYER system contract.</p>
<p>Also, the L1NTV has the following specifics:</p>
<ul>
<li>It operates the <code>chainBalance</code> mapping, ensuring that the chains do not go beyond their balances.</li>
<li>It allows recovering from failed L1→L2 transfers.</li>
<li>It needs to both be able to retrieve funds from the former L1SharedBridge (now this contract has L1Nullifier in its place), but also needs to support the old SDK that gives out allowance to the “l1 shared bridge” value returned from the API, i.e. in our case this is will the L1AssetRouter.</li>
</ul>
<h3 id="summary-6"><a class="header" href="#summary-6">Summary</a></h3>
<p><img src="specs/contracts/bridging/asset_router_and_ntv/./img/bridge_contracts.png" alt="image.png" /></p>
<blockquote>
<p>New bridge contracts</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><!--- WIP --->
<h1 id="l1-smart-contract-of-an-individual-chain"><a class="header" href="#l1-smart-contract-of-an-individual-chain">L1 smart contract of an individual chain</a></h1>
<h2 id="diamond-also-mentioned-as-state-transition-contract"><a class="header" href="#diamond-also-mentioned-as-state-transition-contract">Diamond (also mentioned as State Transition contract)</a></h2>
<p>Technically, this L1 smart contract acts as a connector between Ethereum (L1) and ZK Chain (L2). It checks the
validity proof and data availability, handles L2 &lt;-&gt; L1 communication, finalizes L2 state transition, and more.</p>
<p>There are also important contracts deployed on the L2 that can also execute logic called <em>system contracts</em>. Using L2
&lt;-&gt; L1 communication can affect both the L1 and the L2.</p>
<p><img src="specs/contracts/settlement_contracts/./img/Diamond-scheme.png" alt="diamondProxy.png" /></p>
<h3 id="diamondproxy-1"><a class="header" href="#diamondproxy-1">DiamondProxy</a></h3>
<p>The main contract uses <a href="https://eips.ethereum.org/EIPS/eip-2535">EIP-2535</a> diamond proxy pattern. It is an in-house
implementation that is inspired by the <a href="https://github.com/mudgen/Diamond">mudgen reference implementation</a>. It has no
external functions, only the fallback that delegates a call to one of the facets (target/implementation contract). So
even an upgrade system is a separate facet that can be replaced.</p>
<p>One of the differences from the reference implementation is access freezability. Each of the facets has an associated
parameter that indicates if it is possible to freeze access to the facet. Privileged actors can freeze the <strong>diamond</strong>
(not a specific facet!) and all facets with the marker <code>isFreezable</code> should be inaccessible until the governor or admin
unfreezes the diamond. Note that it is a very dangerous thing since the diamond proxy can freeze the upgrade system and then
the diamond will be frozen forever.</p>
<p>The diamond proxy pattern is very flexible and extendable. For now, it allows splitting implementation contracts by their logical meaning, removes the limit of bytecode size per contract and implements security features such as freezing. In the future, it can also be viewed as <a href="https://eips.ethereum.org/EIPS/eip-6900">EIP-6900</a> for <a href="https://blog.matter-labs.io/introducing-the-zk-stack-c24240c2532a">zkStack</a>, where each ZK Chain can implement a sub-set of allowed implementation contracts.</p>
<h3 id="gettersfacet-1"><a class="header" href="#gettersfacet-1">GettersFacet</a></h3>
<p>Separate facet, whose only function is providing <code>view</code> and <code>pure</code> methods. It also implements
<a href="https://eips.ethereum.org/EIPS/eip-2535#diamond-loupe">diamond loupe</a> which makes managing facets easier.
This contract must never be frozen.</p>
<h3 id="adminfacet-1"><a class="header" href="#adminfacet-1">AdminFacet</a></h3>
<p>This facet responsible for the configuration setup and upgradability, handling tasks such as:</p>
<ul>
<li>Privileged Address Management: Updating key roles, including the governor and validators.</li>
<li>System Parameter Configuration: Adjusting critical system settings, such as the L2 bootloader bytecode hash, verifier address, changing DA layer or fee configurations.</li>
<li>Freezability: Executing the freezing/unfreezing of facets within the diamond proxy to safeguard the ecosystem during upgrades or in response to detected vulnerabilities.</li>
</ul>
<p>Control over the AdminFacet is divided between two main entities:</p>
<ul>
<li>Chain Type Manager - Separate smart contract that can perform critical changes to the system as protocol upgrades. For more detailed information on its function and design, refer to <a href="specs/contracts/settlement_contracts/../chain_management/chain_type_manager.html">this document</a>. Although currently only one version of the CTM exists, the architecture allows for future versions to be introduced via subsequent upgrades. The owner of the CTM is the <a href="https://blog.zknation.io/introducing-zk-nation/">decentralized governance</a>, while for non-critical an Admin entity is used (see details below).</li>
<li>Chain Admin - Multisig smart contract managed by each individual chain that can perform non-critical changes to the system such as granting validator permissions.</li>
</ul>
<h3 id="mailboxfacet-1"><a class="header" href="#mailboxfacet-1">MailboxFacet</a></h3>
<p>The facet that handles L2 &lt;-&gt; L1 communication.</p>
<p>The Mailbox performs three functions:</p>
<ul>
<li>L1 ↔ L2 Communication: Enables data and transaction requests to be sent from L1 to L2 and vice versa, supporting the implementation of multi-layer protocols.</li>
<li>Bridging Native Tokens: Allows the bridging of either ether or ERC20 tokens to L2, enabling users to use these assets within the L2 ecosystem.</li>
<li>Censorship Resistance Mechanism: Currently in the research stage.</li>
</ul>
<p>L1 -&gt; L2 communication is implemented as requesting an L2 transaction on L1 and executing it on L2. This means a user
can call the function on the L1 contract to save the data about the transaction in some queue. Later on, a validator can
process it on L2 and mark it as processed on the L1 priority queue. Currently, it is used for sending information from
L1 to L2 or implementing multi-layer protocols. Users pays for the transaction execution in the native token when requests L1 -&gt; L2 transaction.</p>
<p><em>NOTE</em>: While user requests the transaction from L1, the initiated transaction on L2 will have such a <code>msg.sender</code>:</p>
<pre><code class="language-solidity">  address sender = msg.sender;
  if (sender != tx.origin) {
      sender = AddressAliasHelper.applyL1ToL2Alias(msg.sender);
  }
</code></pre>
<p>where</p>
<pre><code class="language-solidity">uint160 constant offset = uint160(0x1111000000000000000000000000000000001111);

function applyL1ToL2Alias(address l1Address) internal pure returns (address l2Address) {
  unchecked {
    l2Address = address(uint160(l1Address) + offset);
  }
}
</code></pre>
<p>For most of the rollups the address aliasing needs to prevent cross-chain exploits that would otherwise be possible if
we simply reused the same L1 addresses as the L2 sender. In ZKsync Era address derivation rule is different from the
Ethereum, so cross-chain exploits are already impossible. However, ZKsync Era may add full EVM support in the future, so
applying address aliasing leaves room for future EVM compatibility.</p>
<p>The L1 -&gt; L2 communication is also used for bridging <strong>base tokens</strong>. If base token is ether (the case for ZKsync Era) - user should include a <code>msg.value</code> when initiating a
transaction request on the L1 contract, if base token is an ERC20 then contract will spend users allowance. Before executing a transaction on L2, the specified address will be credited
with the funds. To withdraw funds user should call <code>withdraw</code> function on the <code>L2BaseToken</code> system contracts. This will
burn the funds on L2, allowing the user to reclaim them through the <code>finalizeWithdrawal</code> function on the
<code>SharedBridge</code> (more in ZK Chain section).</p>
<p>More about L1-&gt;L2 operations can be found <a href="specs/contracts/settlement_contracts/./priority_queue/l1_l2_communication/l1_to_l2.html">here</a>.</p>
<p>L2 -&gt; L1 communication, in contrast to L1 -&gt; L2 communication, is based only on transferring the information, and not on
the transaction execution on L1. The full description of the mechanism for sending information from L2 to L1 can be found <a href="specs/contracts/settlement_contracts/./data_availability/standard_pubdata_format.html">here</a>.</p>
<p>The Mailbox facet also facilitates L1&lt;&gt;L2 communications for those chains that settle on top of Gateway. The user interfaces for those are identical to the L1&lt;&gt;L2 communication described above. To learn more about L1&lt;&gt;L2 communication works, check out <a href="specs/contracts/settlement_contracts/../settlement_contracts/data_availability/custom_da.html">this document</a> and [this one][TODO].</p>
<h3 id="executorfacet-1"><a class="header" href="#executorfacet-1">ExecutorFacet</a></h3>
<p>A contract that accepts L2 batches, enforces data availability via DA validators and checks the validity of zk-proofs. You can read more about DA validators <a href="specs/contracts/settlement_contracts/../settlement_contracts/data_availability/custom_da.html">in this document</a>.</p>
<p>The state transition is divided into three stages:</p>
<ul>
<li><code>commitBatches</code> - check L2 batch timestamp, process the L2 logs, save data for a batch, and prepare data for zk-proof.</li>
<li><code>proveBatches</code> - validate zk-proof.</li>
<li><code>executeBatches</code> - finalize the state, marking L1 -&gt; L2 communication processing, and saving Merkle tree with L2 logs.</li>
</ul>
<p>Each L2 -&gt; L1 system log will have a key that is part of the following:</p>
<pre><code class="language-solidity">enum SystemLogKey {
  L2_TO_L1_LOGS_TREE_ROOT_KEY,
  PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY,
  CHAINED_PRIORITY_TXN_HASH_KEY,
  NUMBER_OF_LAYER_1_TXS_KEY,
  PREV_BATCH_HASH_KEY,
  L2_DA_VALIDATOR_OUTPUT_HASH_KEY,
  USED_L2_DA_VALIDATOR_ADDRESS_KEY,
  EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY
}
</code></pre>
<p>When a batch is committed, we process L2 -&gt; L1 system logs. Here are the invariants that are expected there:</p>
<ul>
<li>In a given batch there will be either 7 or 8 system logs. The 8th log is only required for a protocol upgrade.</li>
<li>There will be a single log for each key that is contained within <code>SystemLogKey</code></li>
<li>Three logs from the <code>L2_TO_L1_MESSENGER</code> with keys:</li>
<li><code>L2_TO_L1_LOGS_TREE_ROOT_KEY</code></li>
<li><code>L2_DA_VALIDATOR_OUTPUT_HASH_KEY</code></li>
<li><code>USED_L2_DA_VALIDATOR_ADDRESS_KEY</code></li>
<li>Two logs from <code>L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR</code> with keys:
<ul>
<li><code>PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY</code></li>
<li><code>PREV_BATCH_HASH_KEY</code></li>
</ul>
</li>
<li>Two or three logs from <code>L2_BOOTLOADER_ADDRESS</code> with keys:
<ul>
<li><code>CHAINED_PRIORITY_TXN_HASH_KEY</code></li>
<li><code>NUMBER_OF_LAYER_1_TXS_KEY</code></li>
<li><code>EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY</code></li>
</ul>
</li>
<li>None logs from other addresses (may be changed in the future).</li>
</ul>
<h3 id="diamondinit-1"><a class="header" href="#diamondinit-1">DiamondInit</a></h3>
<p>It is a one-function contract that implements the logic of initializing a diamond proxy. It is called only once on the
diamond constructor and is not saved in the diamond as a facet.</p>
<p>Implementation detail - function returns a magic value just like it is designed in
<a href="https://eips.ethereum.org/EIPS/eip-1271">EIP-1271</a>, but the magic value is 32 bytes in size.</p>
<h2 id="validatortimelock-1"><a class="header" href="#validatortimelock-1">ValidatorTimelock</a></h2>
<p>An intermediate smart contract between the validator EOA account and the ZK chain diamond contract. Its primary purpose is
to provide a trustless means of delaying batch execution without modifying the main ZKsync contract. ZKsync actively
monitors the chain activity and reacts to any suspicious activity by freezing the chain. This allows time for
investigation and mitigation before resuming normal operations.</p>
<p>It is a temporary solution to prevent any significant impact of the validator hot key leakage, while the network is in
the Alpha stage.</p>
<p>This contract consists of four main functions <code>commitBatches</code>, <code>proveBatches</code>, <code>executeBatches</code>, and <code>revertBatches</code>, which can be called only by the validator.</p>
<p>When the validator calls <code>commitBatches</code>, the same calldata will be propagated to the ZKsync contract (<code>DiamondProxy</code> through
<code>call</code> where it invokes the <code>ExecutorFacet</code> through <code>delegatecall</code>), and also a timestamp is assigned to these batches to track
the time these batches are committed by the validator to enforce a delay between committing and execution of batches. Then, the
validator can prove the already committed batches regardless of the mentioned timestamp, and again the same calldata (related
to the <code>proveBatches</code> function) will be propagated to the ZKsync contract. After the <code>delay</code> is elapsed, the validator
is allowed to call <code>executeBatches</code> to propagate the same calldata to ZKsync contract.</p>
<p>The owner of the ValidatorTimelock contract is the decentralized governance. Note, that all the chains share the same ValidatorTimelock for simplicity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l1--l2-communication"><a class="header" href="#l1--l2-communication">L1 &lt;&gt; L2 communication</a></h1>
<ul>
<li><a href="specs/contracts/settlement_contracts/priority_queue/./l1_l2_communication/l1_to_l2.html">Handling L1→L2 operations</a></li>
<li><a href="specs/contracts/settlement_contracts/priority_queue/./l1_l2_communication/l2_to_l1.html">L2→L1 communication</a></li>
<li><a href="specs/contracts/settlement_contracts/priority_queue/./l1_l2_communication/overview_deposits_withdrawals.html">Overview - Deposits and Withdrawals</a></li>
<li><a href="specs/contracts/settlement_contracts/priority_queue/./priority-queue.html">Priority queue</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-l1l2-ops"><a class="header" href="#handling-l1l2-ops">Handling L1→L2 ops</a></h1>
<p>The transactions on ZKsync can be initiated not only on L2, but also on L1. There are two types of transactions that can
be initiated on L1:</p>
<ul>
<li>Priority operations. These are the kind of operations that any user can create.</li>
<li>Upgrade transactions. These can be created only during upgrades.</li>
</ul>
<h3 id="prerequisites-4"><a class="header" href="#prerequisites-4">Prerequisites</a></h3>
<p>Please read the full
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">article</a>
on the general system contracts / bootloader structure as well as the pubdata structure with Boojum system to understand
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum.md">the difference</a>
between system and user logs.</p>
<h2 id="priority-operations"><a class="header" href="#priority-operations">Priority operations</a></h2>
<h3 id="initiation"><a class="header" href="#initiation">Initiation</a></h3>
<p>A new priority operation can be appended by calling the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Mailbox.sol#L236">requestL2Transaction</a>
method on L1. This method will perform several checks for the transaction, making sure that it is processable and
provides enough fee to compensate the operator for this transaction. Then, this transaction will be
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Mailbox.sol#L369C1-L369C1">appended</a>
to the priority queue.</p>
<h3 id="bootloader"><a class="header" href="#bootloader">Bootloader</a></h3>
<p>Whenever an operator sees a priority operation, it can include the transaction into the batch. While for normal L2
transaction the account abstraction protocol will ensure that the <code>msg.sender</code> has indeed agreed to start a transaction
out of this name, for L1→L2 transactions there is no signature verification. In order to verify that the operator
includes only transactions that were indeed requested on L1, the bootloader
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L970">maintains</a>
two variables:</p>
<ul>
<li><code>numberOfPriorityTransactions</code> (maintained at <code>PRIORITY_TXS_L1_DATA_BEGIN_BYTE</code> of bootloader memory)</li>
<li><code>priorityOperationsRollingHash</code> (maintained at <code>PRIORITY_TXS_L1_DATA_BEGIN_BYTE + 32</code> of the bootloader memory)</li>
</ul>
<p>Whenever a priority transaction is processed, the <code>numberOfPriorityTransactions</code> gets incremented by 1, while
<code>priorityOperationsRollingHash</code> is assigned to <code>keccak256(priorityOperationsRollingHash, processedPriorityOpHash)</code>,
where <code>processedPriorityOpHash</code> is the hash of the priority operations that has been just processed.</p>
<p>Also, for each priority transaction, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L966">emit</a>
a user L2→L1 log with its hash and result, which basically means that it will get Merklized and users will be able to
prove on L1 that a certain priority transaction has succeeded or failed (which can be helpful to reclaim your funds from
bridges if the L2 part of the deposit has failed).</p>
<p>Then, at the end of the batch, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L3819">submit</a>
and 2 L2→L1 log system log with these values.</p>
<h3 id="batch-commit"><a class="header" href="#batch-commit">Batch commit</a></h3>
<p>During block commit, the contract will remember those values, but not validate them in any way.</p>
<h3 id="batch-execution"><a class="header" href="#batch-execution">Batch execution</a></h3>
<p>During batch execution, we would pop <code>numberOfPriorityTransactions</code> from the top of priority queue and
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L282">verify</a>
that their rolling hash does indeed equal to <code>priorityOperationsRollingHash</code>.</p>
<h2 id="upgrade-transactions"><a class="header" href="#upgrade-transactions">Upgrade transactions</a></h2>
<h3 id="initiation-1"><a class="header" href="#initiation-1">Initiation</a></h3>
<p>Upgrade transactions can only be created during a system upgrade. It is done if the <code>DiamondProxy</code> delegatecalls to the
implementation that manually puts this transaction into the storage of the DiamondProxy. Note, that since it happens
during the upgrade, there is no “real” checks on the structure of this transaction. We do have
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/upgrades/BaseZkSyncUpgrade.sol#L175">some validation</a>,
but it is purely on the side of the implementation which the <code>DiamondProxy</code> delegatecalls to and so may be lifted if the
implementation is changed.</p>
<p>The hash of the currently required upgrade transaction is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L138">stored</a>
under <code>l2SystemContractsUpgradeTxHash</code>.</p>
<p>We will also track the batch where the upgrade has been committed in the <code>l2SystemContractsUpgradeBatchNumber</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/Storage.sol#L141">variable</a>.</p>
<p>We can not support multiple upgrades in parallel, i.e. the next upgrade should start only after the previous one has
been complete.</p>
<h3 id="bootloader-1"><a class="header" href="#bootloader-1">Bootloader</a></h3>
<p>The upgrade transactions are processed just like with priority transactions, with only the following differences:</p>
<ul>
<li>We can have only one upgrade transaction per batch &amp; this transaction must be the first transaction in the batch.</li>
<li>The system contracts upgrade transaction is not appended to <code>priorityOperationsRollingHash</code> and doesn’t increment
<code>numberOfPriorityTransactions</code>. Instead, its hash is calculated via a system L2→L1 log <em>before</em> it gets executed.
Note, that it is an important property. More on it <a href="specs/contracts/settlement_contracts/priority_queue/l1_l2_communication/l1_to_l2.html#security-considerations">below</a>.</li>
</ul>
<h3 id="commit"><a class="header" href="#commit">Commit</a></h3>
<p>After an upgrade has been initiated, it will be required that the next commit batches operation already contains the
system upgrade transaction. It is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L157">checked</a>
by verifying the corresponding L2→L1 log.</p>
<p>We also remember that the upgrade transaction has been processed in this batch (by amending the
<code>l2SystemContractsUpgradeBatchNumber</code> variable).</p>
<h3 id="revert"><a class="header" href="#revert">Revert</a></h3>
<p>In a very rare event when the team needs to revert the batch with the upgrade on ZKsync, the
<code>l2SystemContractsUpgradeBatchNumber</code> is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L412">reset</a>.</p>
<p>Note, however, that we do not “remember” that certain batches had a version before the upgrade, i.e. if the reverted
batches will have to be re-executed, the upgrade transaction must still be present there, even if some of the deleted
batches were committed before the upgrade and thus didn’t contain the transaction.</p>
<h3 id="execute"><a class="header" href="#execute">Execute</a></h3>
<p>Once batch with the upgrade transaction has been executed, we
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L304">delete</a>
them from storage for efficiency to signify that the upgrade has been fully processed and that a new upgrade can be
initiated.</p>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security considerations</a></h2>
<p>Since the operator can put any data into the bootloader memory and for L1→L2 transactions the bootloader has to blindly
trust it and rely on L1 contracts to validate it, it may be a very powerful tool for a malicious operator. Note, that
while the governance mechanism is generally trusted, we try to limit our trust for the operator as much as possible,
since in the future anyone would be able to become an operator.</p>
<p>Some time ago, we <em>used to</em> have a system where the upgrades could be done via L1→L2 transactions, i.e. the
implementation of the <code>DiamondProxy</code> upgrade would
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/upgrade-initializers/DIamondUpgradeInit2.sol#L27">include</a>
a priority transaction (with <code>from</code> equal to for instance <code>FORCE_DEPLOYER</code>) with all the upgrade params.</p>
<p>In the Boojum though having such logic would be dangerous and would allow for the following attack:</p>
<ul>
<li>Let’s say that we have at least 1 priority operations in the priority queue. This can be any operation, initiated by
anyone.</li>
<li>The operator puts a malicious priority operation with an upgrade into the bootloader memory. This operation was never
included in the priority operations queue / and it is not an upgrade transaction. However, as already mentioned above
the bootloader has no idea what priority / upgrade transactions are correct and so this transaction will be processed.</li>
</ul>
<p>The most important caveat of this malicious upgrade is that it may change implementation of the <code>Keccak256</code> precompile
to return any values that the operator needs.</p>
<ul>
<li>When the<code>priorityOperationsRollingHash</code> will be updated, instead of the “correct” rolling hash of the priority
transactions, the one which would appear with the correct topmost priority operation is returned. The operator can’t
amend the behaviour of <code>numberOfPriorityTransactions</code>, but it won’t help much, since the
the<code>priorityOperationsRollingHash</code> will match on L1 on the execution step.</li>
</ul>
<p>That’s why the concept of the upgrade transaction is needed: this is the only transaction that can initiate transactions
out of the kernel space and thus change bytecodes of system contracts. That’s why it must be the first one and that’s
why
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L587">emit</a>
its hash via a system L2→L1 log before actually processing it.</p>
<h3 id="why-it-doesnt-break-on-the-previous-version-of-the-system"><a class="header" href="#why-it-doesnt-break-on-the-previous-version-of-the-system">Why it doesn’t break on the previous version of the system</a></h3>
<p>This section is not required for Boojum understanding but for those willing to analyze the production system that is
deployed at the time of this writing.</p>
<p>Note that the hash of the transaction is calculated before the transaction is executed:
<a href="https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1055">https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1055</a></p>
<p>And then we publish its hash on L1 via a <em>system</em> L2→L1 log:
<a href="https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1133">https://github.com/matter-labs/era-system-contracts/blob/3e954a629ad8e01616174bde2218241b360fda0a/bootloader/bootloader.yul#L1133</a></p>
<p>In the new upgrade system, the <code>priorityOperationsRollingHash</code> is calculated on L2 and so if something in the middle
changes the implementation of <code>Keccak256</code>, it may lead to the full <code>priorityOperationsRollingHash</code> be maliciously
crafted. In the pre-Boojum system, we publish all the hashes of the priority transactions via system L2→L1 and then the
rolling hash is calculated on L1. This means that if at least one of the hash is incorrect, then the entire rolling hash
will not match also.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l2l1-communication"><a class="header" href="#l2l1-communication">L2→L1 communication</a></h1>
<p>The L2→L1 communication is more fundamental than the L1→L2 communication, as the second relies on the first. L2→L1
communication happens by the L1 smart contract verifying messages alongside the proofs. The only “provable” part of the
communication from L2 to L1 are native L2→L1 logs emitted by VM. These can be emitted by the <code>to_l1</code>
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">opcode</a>.
Each log consists of the following fields:</p>
<pre><code class="language-solidity">struct L2Log {
  uint8 l2ShardId;
  bool isService;
  uint16 txNumberInBatch;
  address sender;
  bytes32 key;
  bytes32 value;
}

</code></pre>
<p>Where:</p>
<ul>
<li><code>l2ShardId</code> is the id of the shard the opcode was called (it is currently always 0).</li>
<li><code>isService</code> a boolean flag that is not used right now</li>
<li><code>txNumberInBatch</code> the number of the transaction in the batch where the log has happened. This number is taken from the
internal counter which is incremented each time the <code>increment_tx_counter</code> is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/System%20contracts%20bootloader%20description.md">called</a>.</li>
<li><code>sender</code> is the value of <code>this</code> in the frame where the L2→L1 log was emitted.</li>
<li><code>key</code> and <code>value</code> are just two 32-byte values that could be used to carry some data with the log.</li>
</ul>
<p>The hashed array of these opcodes is then included into the
<a href="https://github.com/matter-labs/era-contracts/blob/f06a58360a2b8e7129f64413998767ac169d1efd/ethereum/contracts/zksync/facets/Executor.sol#L493">batch commitment</a>.
Because of that we know that if the proof verifies, then the L2→L1 logs provided by the operator were correct, so we can
use that fact to produce more complex structures. Before Boojum such logs were also Merklized within the circuits and so
the Merkle tree’s root hash was included into the batch commitment also.</p>
<h2 id="important-system-values"><a class="header" href="#important-system-values">Important system values</a></h2>
<p>Two <code>key</code> and <code>value</code> fields are enough for a lot of system-related use-cases, such as sending timestamp of the batch,
previous batch hash, etc. They were and are used
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L438">used</a>
to verify the correctness of the batch’s timestamps and hashes. You can read more about block processing
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Batches%20&amp;%20L2%20blocks%20on%20zkSync.md">here</a>.</p>
<h2 id="long-l2l1-messages--bytecodes"><a class="header" href="#long-l2l1-messages--bytecodes">Long L2→L1 messages &amp; bytecodes</a></h2>
<p>However, sometimes users want to send long messages beyond 64 bytes which <code>key</code> and <code>value</code> allow us. But as already
said, these L2→L1 logs are the only ways that the L2 can communicate with the outside world. How do we provide long
messages?</p>
<p>Let’s add an <code>sendToL1</code> method in L1Messenger, where the main idea is the following:</p>
<ul>
<li>Let’s submit an L2→L1 log with <code>key = msg.sender</code> (the actual sender of the long message) and
<code>value = keccak256(message)</code>.</li>
<li>Now, during batch commitment the operator will have to provide an array of such long L2→L1 messages and it will be
checked on L1 that indeed for each such log the correct preimage was provided.</li>
</ul>
<p>A very similar idea is used to publish uncompressed bytecodes on L1 (the compressed bytecodes were sent via the long
L1→L2 messages mechanism as explained above).</p>
<p>Note, however, that whenever someone wants to prove that a certain message was present, they need to compose the L2→L1
log and prove its presence.</p>
<h2 id="priority-operations-1"><a class="header" href="#priority-operations-1">Priority operations</a></h2>
<p>Also, for each priority operation, we would send its hash and it status via an L2→L1 log. On L1 we would then
reconstruct the rolling hash of the processed priority transactions, allowing to correctly verify during the
<code>executeBatches</code> method that indeed the batch contained the correct priority operations.</p>
<p>Importantly, the fact that both hash and status were sent, it made it possible to
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/bridge/L1ERC20Bridge.sol#L255">prove</a>
that the L2 part of a deposit has failed and ask the bridge to release funds.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview---deposits-and-withdrawals"><a class="header" href="#overview---deposits-and-withdrawals">Overview - Deposits and Withdrawals</a></h1>
<p>The zkEVM supports general message passing for L1&lt;-&gt;L2 communication. Proofs are settled on L1, so core of this process
is the L2-&gt;L1 message passing process. L1-&gt;L2 messages are recorded on L1 inside a priority queue, the sequencer picks
it up from here and executes it in the zkEVM. The zkEVM sends an L2-&gt;L1 message of the L1 transactions that it
processed, and the rollup’s proof is only valid if the processed transactions were exactly right.</p>
<p>There is an asymmetry in the two directions however, in the L1-&gt;L2 direction we support starting message calls by having
a special transaction type called L1 transactions. In the L2-&gt;L1 direction we only support message passing.</p>
<p>In particular, deposits and withdrawals of ether also use the above methods. For deposits the L1-&gt;L2 transaction is sent
with empty calldata, the recipients address and the deposited value. When withdrawing, an L2-&gt;L1 message is sent. This
is then processed by the smart contract holding the ether on L1, which releases the funds.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="priority-queue-to-merkle-tree"><a class="header" href="#priority-queue-to-merkle-tree">Priority Queue to Merkle Tree</a></h1>
<h2 id="overview-of-the-current-implementation"><a class="header" href="#overview-of-the-current-implementation">Overview of the current implementation</a></h2>
<p>Priority queue is a data structure in Era contracts that is used to handle L1-&gt;L2 priority operations. It supports the following:</p>
<ul>
<li>inserting a new operation into the end of the queue</li>
<li>checking that an newly executed batch executed some n first priority operations from the queue (and not some other ones) in correct order</li>
</ul>
<p>The queue itself only stores the following:</p>
<pre><code class="language-solidity">struct PriorityOperation {
  bytes32 canonicalTxHash;
  uint64 expirationTimestamp;
  uint192 layer2Tip;
}
</code></pre>
<p>of which we only care about the canonical hash.</p>
<h3 id="inserting-new-operations"><a class="header" href="#inserting-new-operations">Inserting new operations</a></h3>
<p>The queue is implemented as a <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/l1-contracts/contracts/state-transition/libraries/PriorityQueue.sol#L22">library</a>.
For each incoming priority operation, we simply <code>pushBack</code> its hash, expiration and layer2Tip.</p>
<h3 id="checking-validity"><a class="header" href="#checking-validity">Checking validity</a></h3>
<p>When a new batch is executed, we need to check that operations that were executed there match the operations in the priority queue. The batch header contains <code>numberOfLayer1Txs</code> and <code>priorityOperationsHash</code> which is a rolling hash of all priority operations that were executed in the batch. Bootloader check that this hash indeed corresponds to all priority operations that have been executed in that batch. The contract only checks that this hash matches the operations stored in the queue:</p>
<pre><code class="language-solidity">/// @dev Pops the priority operations from the priority queue and returns a rolling hash of operations
function _collectOperationsFromPriorityQueue(uint256 _nPriorityOps) internal returns (bytes32 concatHash) {
    concatHash = EMPTY_STRING_KECCAK;

    for (uint256 i = 0; i &lt; _nPriorityOps; i = i.uncheckedInc()) {
        PriorityOperation memory priorityOp = s.priorityQueue.popFront();
        concatHash = keccak256(abi.encode(concatHash, priorityOp.canonicalTxHash));
    }
}

bytes32 priorityOperationsHash = _collectOperationsFromPriorityQueue(_storedBatch.numberOfLayer1Txs);
require(priorityOperationsHash == _storedBatch.priorityOperationsHash); // priority operations hash does not match to expected
</code></pre>
<p>As can be seen, this is done in <code>O(n)</code> compute, where <code>n</code> is the number of priority operations in the batch.</p>
<h2 id="motivation-for-migration-to-merkle-tree"><a class="header" href="#motivation-for-migration-to-merkle-tree">Motivation for migration to Merkle Tree</a></h2>
<p>Since we will be introducing Gateway, we will need to support one more operation:</p>
<ul>
<li>migrating priority queue from L1 to Gateway (and back)</li>
</ul>
<p>Current implementation takes <code>O(n)</code> space and is vulnerable to spam attacks during migration
(e.g. an attacker can insert a lot of priority operations and we won’t be able to migrate all of them due to gas limits).</p>
<p>Hence, we need an implementation with a small (constant- or log-size) space imprint that we can migrate to Gateway and back that would still allow us to perform the other 2 operations.</p>
<p>Merkle tree of priority operations is perfect for this since we can simply migrate the latest root hash to Gateway and back.</p>
<ul>
<li>It can still efficiently (in <code>O(height)</code>) insert new operations.</li>
<li>It can also still efficiently (in <code>O(n)</code> compute and <code>O(n + height)</code> calldata) check that the batch’s <code>priorityOperationsHash</code> corresponds to the operations from the queue.</li>
</ul>
<p>Note that <code>n</code> here is the number of priority operations in the batch, not <code>2^height</code>.</p>
<p>The implementation details are described below.</p>
<h3 id="faq"><a class="header" href="#faq">FAQ</a></h3>
<ul>
<li>Q: Why can’t we just migrate the rolling hash of the operations in the existing priority queue?</li>
<li>A: The rolling hash is not enough to check that the operations from the executed batch are indeed from the priority queue. We would need to store all historical rolling hashes, which would be <code>O(n)</code> space and would not solve the spam attack problem.</li>
</ul>
<h2 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h2>
<p>The implementation will consist of two parts:</p>
<ul>
<li>Merkle tree on L1 contracts, to replace the existing priority queue (while still supporting the existing operations)</li>
<li>Merkle tree off-chain on the server, to generate the merkle proofs for the executed priority operations.</li>
</ul>
<h3 id="contracts-2"><a class="header" href="#contracts-2">Contracts</a></h3>
<p>On the contracts, the Merkle tree will be implemented as an Incremental (append-only) Merkle Tree (<a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/MerkleTreeWithHistory.sol">example implementation</a>), meaning that it can efficiently (in <code>O(height)</code> compute) append new elements to the right, while only storing <code>O(height)</code> nodes at all times.</p>
<p>It will also be dynamically sized, meaning that it will double in size when the current size is not enough to store the new element.</p>
<h3 id="server-1"><a class="header" href="#server-1">Server</a></h3>
<p>On the server, the Merkle tree will be implemented as an extension of <code>MiniMerkleTree</code> currently used for L2-&gt;L1 logs.</p>
<p>It will have the following properties:</p>
<ul>
<li>in-memory: the tree will be stored in memory and will be rebuilt on each restart (details below).</li>
<li>dynamically sized (to match the contracts implementation)</li>
<li>append-only (to match the contracts implementation)</li>
</ul>
<p>The tree does not need to be super efficient, since we process on average 7 operations per batch.</p>
<h3 id="why-in-memory"><a class="header" href="#why-in-memory">Why in-memory?</a></h3>
<p>Having the tree in-memory means rebuilding the tree on each restart. This is fine because on mainnet after &gt;1 year since release we have only 3.2M priority operations. We only have to fully rebuild the tree <em>once</em> and then simply cache the already executed operations (which are the majority). Having the tree in-memory has an added benefit of not having to have additional infrastructure to store it on disk and not having to be bothered to rollback its state manually if we ever have to (as we do for e.g. for the storage logs tree).</p>
<p>Note: If even rebuilding it once becomes a problem, it can be easily mitigated by only persisting the cache nodes.</p>
<h3 id="caching"><a class="header" href="#caching">Caching</a></h3>
<p><strong>Why do we need caching?</strong> After a batch is successfully executed, we will no longer need to have the ability to generate merkle paths for those operations. This means that we can save space and compute by only fully storing the operations that are not yet executed, and caching the leaves
corresponding to the already executed operations.</p>
<p>We will only cache some prefix of the tree, meaning nodes in the interval [0; N) where N is the number of executed priority operations. The cache will store the rightmost cached left-child node on each level of the tree (see diagrams).</p>
<p><img src="specs/contracts/settlement_contracts/priority_queue/./img/PQ1.png" alt="Untitled" /></p>
<p><img src="specs/contracts/settlement_contracts/priority_queue/./img/PQ2.png" alt="Untitled" /></p>
<p><img src="specs/contracts/settlement_contracts/priority_queue/./img/PQ3.png" alt="Untitled" /></p>
<p>This means that we will not be able to generate merkle proofs for the cached nodes (and since they are already executed, we don’t need to). This structure allows us to save a lot of space, since it only takes up <code>O(height)</code> space instead of linear space for all executed operations. This is a big optimization since there are currently 3.2M total operations but &lt;10 non-executed operations in the mainnet priority queue, which means most of the tree will be cached.</p>
<p>This also means we don’t really have to store non-leaf nodes other than cache, since we can calculate merkle root / merkle paths in <code>O(n)</code> where <code>n</code> is the number of non-executed operations (and not total number of operations), and since <code>n</code> is so small, it is really fast.</p>
<h3 id="adding-new-operations"><a class="header" href="#adding-new-operations">Adding new operations</a></h3>
<p>On the contracts, appending a new operation to the tree is done by simply calling <code>append</code> on the Incremental Merkle Tree, which will update at most <code>height</code> slots. Actually, it works almost exactly like the cache described above. Once again: <a href="https://github.com/tornadocash/tornado-core/blob/1ef6a263ac6a0e476d063fcb269a9df65a1bd56a/contracts/MerkleTreeWithHistory.sol#L68">tornado-cash implementation</a>.</p>
<p>On the server, <code>eth_watch</code> will listen for <code>NewPriorityOperation</code> events as it does now, and will append the new operation to the tree on the server.</p>
<h3 id="checking-validity-1"><a class="header" href="#checking-validity-1">Checking validity</a></h3>
<p>To check that the executed batch indeed took its priority operations from the queue, we have to make sure that if we take first <code>numberOfL1Txs</code> non-executed operations from the tree, their rolling hash will match <code>priorityOperationsHash</code> . Since will not be storing the hashes of these operations onchain anymore, we will have to provide them as calldata. Additionally in calldata, we should provide merkle proofs for the <strong>first and last</strong> operations in that batch (hence <code>O(n + height)</code> calldata). This will make it possible to prove onchain that that contiguous interval of hashes indeed exists in the merkle tree.</p>
<p>This can be done simply by constructing the part of the tree above this interval using the provided paths to first and last elements of the interval checking that computed merkle root matches with stored one (in <code>O(n)</code> where <code>n</code> is number of priority operations in a batch). We will also need to track the <code>index</code> of the first unexecuted operation onchain to properly calculate the merkle root and ensure that batches don’t execute some operations out of order or multiple times.</p>
<p>We will also need to prove that the rolling hash of provided hashes matches with <code>priorityOperationsHash</code> which is also <code>O(n)</code></p>
<p>It is important to note that we should store some number of historical root hashes, since the Merkle tree on the server might lag behind the contracts a bit, and hence merkle paths generated on the server-side might become invalid if we compare them to the latest root hash on the contracts. These historical root hashes are not necessary to migrate to and from Gateway though.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-availability"><a class="header" href="#data-availability">Data availability</a></h1>
<ul>
<li><a href="specs/contracts/settlement_contracts/data_availability/./pubdata.html">Pubdata</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./compression.html">Compression</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./reconstruction.html">Reconstruction</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./validium_zk_porter.html">Validium and zkPorter</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./custom_da.html">Custom DA support</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./rollup_da.html">Rollup DA support</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./standard_pubdata_format.html">Standard pubdata format</a></li>
<li><a href="specs/contracts/settlement_contracts/data_availability/./state_diff_compression_v1_spec.html">State diff compression v1 spec</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-pubdata-in-boojum"><a class="header" href="#handling-pubdata-in-boojum">Handling pubdata in Boojum</a></h1>
<p>Pubdata in ZKsync can be divided up into 4 different categories:</p>
<ol>
<li>L2 to L1 Logs</li>
<li>L2 to L1 Messages</li>
<li>Smart Contract Bytecodes</li>
<li>Storage writes</li>
</ol>
<p>Using data corresponding to these 4 facets, across all executed batches, we’re able to reconstruct the full state of L2.
With the upgrade to our new proof system, Boojum, the way this data is represented will change. At a high level, in the
pre-Boojum system these are represented as separate fields while for boojum they will be packed into a single bytes
array. Once 4844 gets integrated this bytes array will move from being part of the calldata to blob data.</p>
<p>While the structure of the pubdata changes, the way in which one can go about pulling the information will remain the
same. Basically, we just need to filter all of the transactions to the L1 ZKsync contract for only the <code>commitBatches</code>
transactions where the proposed block has been referenced by a corresponding <code>executeBatches</code> call (the reason for this
is that a committed or even proven block can be reverted but an executed one cannot). Once we have all the committed
batches that have been executed, we then will pull the transaction input and the relevant fields, applying them in order
to reconstruct the current state of L2.</p>
<h2 id="l2l1-communication-1"><a class="header" href="#l2l1-communication-1">L2→L1 communication</a></h2>
<h3 id="l2l1-communication-before-boojum"><a class="header" href="#l2l1-communication-before-boojum">L2→L1 communication before Boojum</a></h3>
<p>While there were quite some changes during Boojum upgrade, most of the scheme remains the same and so explaining how it
worked before gives some background on why certain decisions are made and kept for backward compatibility.</p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum/L2%E2%86%92L1%20communication%20before%20Boojum.md">L2→L1 communication before Boojum</a></p>
<p>The most important feature that we’ll need to maintain in Boojum for backward compatibility is to provide a similar
Merkle tree of L2→L1 logs with the long L2→L1 messages and priority operations’ status.</p>
<p>Before Boojum, whenever we sent an L2→L1 long message, a <em>log</em> was appended to the Merkle tree of L2→L1 messages on L1
due to necessity. In Boojum we’ll have to maintain this fact. Having the priority operations’ statuses is important to
enable
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/bridge/L1ERC20Bridge.sol#L255">proving</a>
failed deposits for bridges.</p>
<h3 id="changes-with-boojum"><a class="header" href="#changes-with-boojum">Changes with Boojum</a></h3>
<h4 id="problems-with-the-previous-approach"><a class="header" href="#problems-with-the-previous-approach">Problems with the previous approach</a></h4>
<ul>
<li>There was a limit of 512 L2→L1 logs per batch, which is very limiting. It causes our block to be forcefully closed
based on the number of these messages instead of having the pubdata as the only limit.</li>
<li>In the ideal world, we would like to have the tree adapt to the requirements of the batch, with any number of leaves
possible (in practice, a maximum of 2048 would likely be enough for the foreseeable future).</li>
<li>Extending the tree in the circuits will be hard to do and hard to maintain.</li>
<li>The hash of the contents of the L2→L1 messages needs to be rehashed to support the danksharding blobs, so we want to
keep only the essential logs as parts of calldata and the rest should be separated so that they could be moved the
EIP4844 blob in the future.</li>
</ul>
<h4 id="solution-2"><a class="header" href="#solution-2">Solution</a></h4>
<p>We will implement the calculation of the Merkle root of the L2→L1 messages via a system contract as part of the
<code>L1Messenger</code>. Basically, whenever a new log emitted by users that needs to be Merklized is created, the <code>L1Messenger</code>
contract will append it to its rolling hash and then at the end of the batch, during the formation of the blob it will
receive the original preimages from the operator, verify, and include the logs to the blob.</p>
<p>We will now call the logs that are created by users and are Merklized <em>user</em> logs and the logs that are emitted by
natively by VM <em>system</em> logs. Here is a short comparison table for better understanding:</p>
<div class="table-wrapper"><table><thead><tr><th>System logs</th><th>User logs</th></tr></thead><tbody>
<tr><td>Emitted by VM via an opcode.</td><td>VM knows nothing about them.</td></tr>
<tr><td>Consistency and correctness is enforced by the verifier on L1 (i.e. their hash is part of the block commitment.</td><td>Consistency and correctness is enforced by the L1Messenger system contract. The correctness of the behavior of the L1Messenger is enforced implicitly by prover in a sense that it proves the correctness of the execution overall.</td></tr>
<tr><td>We don’t calculate their Merkle root.</td><td>We calculate their Merkle root on the L1Messenger system contract.</td></tr>
<tr><td>We have constant small number of those.</td><td>We can have as much as possible as long as the commitBatches function on L1 remains executable (it is the job of the operator to ensure that only such transactions are selected)</td></tr>
<tr><td>In EIP4844 they will remain part of the calldata.</td><td>In EIP4844 they will become part of the blobs.</td></tr>
</tbody></table>
</div>
<h4 id="backwards-compatibility"><a class="header" href="#backwards-compatibility">Backwards-compatibility</a></h4>
<p>Note, that to maintain a unified interface with the previous version of the protocol, the leaves of the Merkle tree will
have to maintain the following structure:</p>
<pre><code class="language-solidity">struct L2Log {
  uint8 l2ShardId;
  bool isService;
  uint16 txNumberInBlock;
  address sender;
  bytes32 key;
  bytes32 value;
}

</code></pre>
<p>While the leaf will look the following way:</p>
<pre><code class="language-solidity">bytes32 hashedLog = keccak256(
    abi.encodePacked(_log.l2ShardId, _log.isService, _log.txNumberInBlock, _log.sender, _log.key, _log.value)
);
</code></pre>
<p><code>keccak256</code> will continue being the function for the merkle tree.</p>
<p>To put it shortly, the proofs for L2→L1 log inclusion will continue having exactly the same format as they did in the
pre-Boojum system, which avoids breaking changes for SDKs and bridges alike.</p>
<h4 id="implementation-of-l1messenger"><a class="header" href="#implementation-of-l1messenger">Implementation of <code>L1Messenger</code></a></h4>
<p>The L1Messenger contract will maintain a rolling hash of all the L2ToL1 logs <code>chainedLogsHash</code> as well as the rolling
hashes of messages <code>chainedMessagesHash</code>. Whenever a contract wants to send an L2→L1 log, the following operation will
be
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/L1Messenger.sol#L110">applied</a>:</p>
<p><code>chainedLogsHash = keccak256(chainedLogsHash, hashedLog)</code>. L2→L1 logs have the same 88-byte format as in the current
version of ZKsync.</p>
<p>Note, that the user is charged for necessary future the computation that will be needed to calculate the final merkle
root. It is roughly 4x higher than the cost to calculate the hash of the leaf, since the eventual tree might have be 4x
times the number nodes. In any case, this will likely be a relatively negligible part compared to the cost of the
pubdata.</p>
<p>At the end of the execution, the bootloader will
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2470">provide</a>
a list of all the L2ToL1 logs as well as the messages in this block to the L1Messenger (this will be provided by the
operator in the memory of the bootloader). The L1Messenger checks that the rolling hash from the provided logs is the
same as in the <code>chainedLogsHash</code> and calculate the merkle tree of the provided messages. Right now, we always build the
Merkle tree of size <code>2048</code>, but we charge the user as if the tree was built dynamically based on the number of leaves in
there. The implementation of the dynamic tree has been postponed until the later upgrades.</p>
<h4 id="long-l2l1-messages--bytecodes-1"><a class="header" href="#long-l2l1-messages--bytecodes-1">Long L2→L1 messages &amp; bytecodes</a></h4>
<p>Before, the fact that the correct preimages for L2→L1 messages as bytecodes were provided was checked on the L1 side.
Now, it will be done on L2.</p>
<p>If the user wants to send an L2→L1 message, its preimage is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/L1Messenger.sol#L125">appended</a>
to the message’s rolling hash too <code>chainedMessagesHash = keccak256(chainedMessagesHash, keccak256(message))</code>.</p>
<p>A very similar approach for bytecodes is used, where their rolling hash is calculated and then the preimages are
provided at the end of the batch to form the full pubdata for the batch.</p>
<p>Note, that in for backward compatibility, just like before any long message or bytecode is accompanied by the
corresponding user L2→L1 log.</p>
<h4 id="using-system-l2l1-logs-vs-the-user-logs"><a class="header" href="#using-system-l2l1-logs-vs-the-user-logs">Using system L2→L1 logs vs the user logs</a></h4>
<p>The content of the L2→L1 logs by the L1Messenger will go to the blob of EIP4844. Meaning, that all the data that belongs
to the tree by L1Messenger’s L2→L1 logs should not be needed during block commitment. Also, note that in the future we
will remove the calculation of the Merkle root of the built-in L2→L1 messages.</p>
<p>The only places where the built-in L2→L1 messaging should continue to be used:</p>
<ul>
<li>Logs by SystemContext (they are needed on commit to check the previous block hash).</li>
<li>Logs by L1Messenger for the merkle root of the L2→L1 tree as well as the hash of the <code>totalPubdata</code>.</li>
<li><code>chainedPriorityTxsHash</code> and <code>numberOfLayer1Txs</code> from the bootloader (read more about it below).</li>
</ul>
<h4 id="obtaining-txnumberinblock"><a class="header" href="#obtaining-txnumberinblock">Obtaining <code>txNumberInBlock</code></a></h4>
<p>To have the same log format, the <code>txNumberInBlock</code> must be obtained. While it is internally counted in the VM, there is
currently no opcode to retrieve this number. We will have a public variable <code>txNumberInBlock</code> in the <code>SystemContext</code>,
which will be incremented with each new transaction and retrieve this variable from there. It is
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/SystemContext.sol#L458">zeroed out</a>
at the end of the batch.</p>
<h3 id="bootloader-implementation"><a class="header" href="#bootloader-implementation">Bootloader implementation</a></h3>
<p>The bootloader has a memory segment dedicated to the ABI-encoded data of the L1ToL2Messenger to perform the
<code>publishPubdataAndClearState</code> call.</p>
<p>At the end of the execution of the batch, the operator should provide the corresponding data into the bootloader memory,
i.e user L2→L1 logs, long messages, bytecodes, etc. After that, the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/bootloader/bootloader.yul#L2484">call</a>
is performed to the <code>L1Messenger</code> system contract, that should validate the adherence of the pubdata to the required
format</p>
<h2 id="bytecode-publishing"><a class="header" href="#bytecode-publishing">Bytecode Publishing</a></h2>
<p>Within pubdata, bytecodes are published in 1 of 2 ways: (1) uncompressed via <code>factoryDeps</code> (pre-boojum this is within
its own field, and post-boojum as part of the <code>totalPubdata</code>) and (2) compressed via long l2 → l1 messages.</p>
<h3 id="uncompressed-bytecode-publishing"><a class="header" href="#uncompressed-bytecode-publishing">Uncompressed Bytecode Publishing</a></h3>
<p>With Boojum, <code>factoryDeps</code> are included within the <code>totalPubdata</code> bytes and have the following format:
<code>number of bytecodes || forEachBytecode (length of bytecode(n) || bytecode(n))</code> .</p>
<h3 id="compressed-bytecode-publishing"><a class="header" href="#compressed-bytecode-publishing">Compressed Bytecode Publishing</a></h3>
<p>This part stays the same in a pre and post boojum ZKsync. Unlike uncompressed bytecode which are published as part of
<code>factoryDeps</code>, compressed bytecodes are published as long l2 → l1 messages which can be seen
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/system-contracts/contracts/Compressor.sol#L80">here</a>.</p>
<h4 id="bytecode-compression-algorithm--server-side"><a class="header" href="#bytecode-compression-algorithm--server-side">Bytecode Compression Algorithm — Server Side</a></h4>
<p>This is the part that is responsible for taking bytecode, that has already been chunked into 8 byte words, performing
validation, and compressing it.</p>
<p>Each 8 byte word from the chunked bytecode is assigned a 2 byte index (constraint on size of dictionary of chunk → index
is 2^16 - 1 elements). The length of the dictionary, dictionary entries (index assumed through order), and indexes are
all concatenated together to yield the final compressed version.</p>
<p>For bytecode to be considered valid it must satisfy the following:</p>
<ol>
<li>Bytecode length must be less than 2097120 ((2^16 - 1) * 32) bytes.</li>
<li>Bytecode length must be a multiple of 32.</li>
<li>Number of 32-byte words cannot be even.</li>
</ol>
<p>The following is a simplified version of the algorithm:</p>
<pre><code class="language-python">statistic: Map[chunk, (count, first_pos)]
dictionary: Map[chunk, index]
encoded_data: List[index]

for position, chunk in chunked_bytecode:
 if chunk is in statistic:
  statistic[chunk].count += 1
 else:
  statistic[chunk] = (count=1, first_pos=pos)

# We want the more frequently used bytes to have smaller ids to save on calldata (zero bytes cost less)
statistic.sort(primary=count, secondary=first_pos, order=desc)

for index, chunk in enumerated(sorted_statistics):
  dictionary[chunk] = index

for chunk in chunked_bytecode:
 encoded_data.append(dictionary[chunk])

return [len(dictionary), dictionary.keys(order=index asc), encoded_data]
</code></pre>
<h4 id="verification-and-publishing--l2-contract"><a class="header" href="#verification-and-publishing--l2-contract">Verification And Publishing — L2 Contract</a></h4>
<p>The function <code>publishCompressBytecode</code> takes in both the original <code>_bytecode</code> and the <code>_rawCompressedData</code> , the latter
of which comes from the output of the server’s compression algorithm. Looping over the encoded data, derived from
<code>_rawCompressedData</code> , the corresponding chunks are pulled from the dictionary and compared to the original byte code,
reverting if there is a mismatch. After the encoded data has been verified, it is published to L1 and marked accordingly
within the <code>KnownCodesStorage</code> contract.</p>
<p>Pseudo-code implementation:</p>
<pre><code class="language-python">length_of_dict = _rawCompressedData[:2]
dictionary = _rawCompressedData[2:2 + length_of_dict * 8] # need to offset by bytes used to store length (2) and multiply by 8 for chunk size
encoded_data = _rawCompressedData[2 + length_of_dict * 8:]

assert(len(dictionary) % 8 == 0) # each element should be 8 bytes
assert(num_entries(dictionary) &lt;= 2^16)
assert(len(encoded_data) * 4 == len(_bytecode)) # given that each chunk is 8 bytes and each index is 2 bytes they should differ by a factor of 4

for (index, dict_index) in list(enumerate(encoded_data)):
 encoded_chunk = dictionary[dict_index]
 real_chunk = _bytecode.readUint64(index * 8) # need to pull from index * 8 to account for difference in element size
 verify(encoded_chunk == real_chunk)

# Sending the compressed bytecode to L1 for data availability
sendToL1(_rawCompressedBytecode)
markAsPublished(hash(_bytecode))
</code></pre>
<h2 id="storage-diff-publishing"><a class="header" href="#storage-diff-publishing">Storage diff publishing</a></h2>
<p>ZKsync is a statediff-based rollup and so publishing the correct state diffs plays an integral role in ensuring data
availability.</p>
<h3 id="how-publishing-of-storage-diffs-worked-before-boojum"><a class="header" href="#how-publishing-of-storage-diffs-worked-before-boojum">How publishing of storage diffs worked before Boojum</a></h3>
<p>As always in order to understand the new system better, some information about the previous one is important.</p>
<p>Before, the system contracts had no clue about storage diffs. It was the job of the operator to provide the
<code>initialStorageChanges</code> and <code>reapeatedStorageWrites</code> (more on the differences will be explained below). The information
to commit the block looked the following way:</p>
<pre><code class="language-solidity">struct CommitBlockInfo {
  uint64 blockNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 l2LogsTreeRoot;
  bytes32 priorityOperationsHash;
  bytes initialStorageChanges;
  bytes repeatedStorageChanges;
  bytes l2Logs;
  bytes[] l2ArbitraryLengthMessages;
  bytes[] factoryDeps;
}

</code></pre>
<p>These two fields would be then included into the block commitment and checked by the verifier.</p>
<h3 id="difference-between-initial-and-repeated-writes"><a class="header" href="#difference-between-initial-and-repeated-writes">Difference between initial and repeated writes</a></h3>
<p>ZKsync publishes state changes that happened within the batch instead of transactions themselves. Meaning, that for
instance some storage slot <code>S</code> under account <code>A</code> has changed to value <code>V</code>, we could publish a triple of <code>A,S,V</code>. Users
by observing all the triples could restore the state of ZKsync. However, note that our tree unlike Ethereum’s one is not
account based (i.e. there is no first layer of depth 160 of the merkle tree corresponding to accounts and second layer
of depth 256 of the merkle tree corresponding to users). Our tree is “flat”, i.e. a slot <code>S</code> under account <code>A</code> is just
stored in the leaf number <code>H(S,A)</code>. Our tree is of depth 256 + 8 (the 256 is for these hashed account/key pairs and 8 is
for potential shards in the future, we currently have only one shard and it is irrelevant for the rest of the document).</p>
<p>We call this <code>H(S,A)</code> <em>derived key</em>, because it is derived from the address and the actual key in the storage of the
account. Since our tree is flat, whenever a change happens, we can publish a pair <code>DK, V</code>, where <code>DK=H(S,A)</code>.</p>
<p>However, these is an optimization that could be done:</p>
<ul>
<li>Whenever a change to a key is used for the first time, we publish a pair of <code>DK,V</code> and we assign some sequential id to
this derived key. This is called an <em>initial write</em>. It happens for the first time and that’s why we must publish the
full key.</li>
<li>If this storage slot is published in some of the subsequent batches, instead of publishing the whole <code>DK</code>, we can use
the sequential id instead. This is called a <em>repeated write</em>.</li>
</ul>
<p>For instance, if the slots <code>A</code>,<code>B</code> (I’ll use latin letters instead of 32-byte hashes for readability) changed their
values to <code>12</code>,<code>13</code> accordingly, in the batch it happened they will be published in the following format:</p>
<ul>
<li><code>(A, 12), (B, 13)</code>. Let’s say that the last sequential id ever used is 6. Then, <code>A</code> will receive the id of <code>7</code> and B
will receive the id of <code>8</code>.</li>
</ul>
<p>Let’s say that in the next block, they changes their values to <code>13</code>,<code>14</code>. Then, their diff will be published in the
following format:</p>
<ul>
<li><code>(7, 13), (8,14)</code>.</li>
</ul>
<p>The id is permanently assigned to each storage key that was ever published. While in the description above it may not
seem like a huge boost, however, each <code>DK</code> is 32 bytes long and id is at most 8 bytes long.</p>
<p>We call this id <em>enumeration_index</em>.</p>
<p>Note, that the enumeration indexes are assigned in the order of sorted array of (address, key), i.e. they are internally
sorted. The enumeration indexes are part of the state merkle tree, it is <strong>crucial</strong> that the initial writes are
published in the correct order, so that anyone could restore the correct enum indexes for the storage slots. In
addition, an enumeration index of <code>0</code> indicates that the storage write is an initial write.</p>
<h3 id="state-diffs-after-boojum-upgrade"><a class="header" href="#state-diffs-after-boojum-upgrade">State diffs after Boojum upgrade</a></h3>
<p>Firstly, let’s define what we’ll call the <code>stateDiffs</code>. A <em>state diff</em> is an element of the following structure.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/encodings/state_diff_record.rs#L8">https://github.com/matter-labs/era-zkevm_test_harness/blob/3cd647aa57fc2e1180bab53f7a3b61ec47502a46/circuit_definitions/src/encodings/state_diff_record.rs#L8</a>.</p>
<p>Basically, it contains all the values which might interest us about the state diff:</p>
<ul>
<li><code>address</code> where the storage has been changed.</li>
<li><code>key</code> (the original key inside the address)</li>
<li><code>derived_key</code> — <code>H(key, address)</code> as described in the previous section.
<ul>
<li>Note, the hashing algorithm currently used here is <code>Blake2s</code></li>
</ul>
</li>
<li><code>enumeration_index</code> — Enumeration index as explained above. It is equal to 0 if the write is initial and contains the
non-zero enumeration index if it is the repeated write (indexes are numerated starting from 1).</li>
<li><code>initial_value</code> — The value that was present in the key at the start of the batch</li>
<li><code>final_value</code> — The value that the key has changed to by the end of the batch.</li>
</ul>
<p>We will consider <code>stateDiffs</code> an array of such objects, sorted by (address, key).</p>
<p>This is the internal structure that is used by the circuits to represent the state diffs. The most basic “compression”
algorithm is the one described above:</p>
<ul>
<li>For initial writes, write the pair of (<code>derived_key</code>, <code>final_value</code>)</li>
<li>For repeated writes write the pair of (<code>enumeration_index</code>, <code>final_value</code>).</li>
</ul>
<p>Note, that values like <code>initial_value</code>, <code>address</code> and <code>key</code> are not used in the “simplified” algorithm above, but they
will be helpful for the more advanced compression algorithms in the future. The
<a href="specs/contracts/settlement_contracts/data_availability/pubdata.html#state-diff-compression-format">algorithm</a> for Boojum will already utilize the difference between the <code>initial_value</code>
and <code>final_value</code> for saving up on pubdata.</p>
<h3 id="how-the-new-pubdata-verification-would-work"><a class="header" href="#how-the-new-pubdata-verification-would-work">How the new pubdata verification would work</a></h3>
<h4 id="l2"><a class="header" href="#l2">L2</a></h4>
<ol>
<li>The operator provides both full <code>stateDiffs</code> (i.e. the array of the structs above) and the compressed state diffs
(i.e. the array which contains the state diffs, compressed by the algorithm explained
<a href="specs/contracts/settlement_contracts/data_availability/pubdata.html#state-diff-compression-format">below</a>).</li>
<li>The L1Messenger must verify that the compressed version is consistent with the original stateDiffs.</li>
<li>Once verified, the L1Messenger will publish the <em>hash</em> of the original state diff via a system log. It will also
include the compressed state diffs into the totalPubdata to be published onto L1.</li>
</ol>
<h4 id="l1"><a class="header" href="#l1">L1</a></h4>
<ol>
<li>During committing the block, the L1
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L139">verifies</a>
that the operator has provided the full preimage for the totalPubdata (which includes L2→L1 logs, L2→L1 messages,
bytecodes as well as the compressed state diffs).</li>
<li>The block commitment
<a href="https://github.com/code-423n4/2023-10-zksync/blob/ef99273a8fdb19f5912ca38ba46d6bd02071363d/code/contracts/ethereum/contracts/zksync/facets/Executor.sol#L462">includes</a>
*the hash of the <code>stateDiffs</code>. Thus, during ZKP verification will fail if the provided stateDiff hash is not
correct.</li>
</ol>
<p>It is a secure construction because the proof can be verified only if both the execution was correct and the hash of the
provided hash of the <code>stateDiffs</code> is correct. This means that the L1Messenger indeed received the array of correct
<code>stateDiffs</code> and, assuming the L1Messenger is working correctly, double-checked that the compression is of the correct
format, while L1 contracts on the commit stage double checked that the operator provided the preimage for the compressed
state diffs.</p>
<h3 id="state-diff-compression-format"><a class="header" href="#state-diff-compression-format">State diff compression format</a></h3>
<p>The following algorithm is used for the state diff compression:</p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Smart%20contract%20Section/Handling%20pubdata%20in%20Boojum/State%20diff%20compression%20v1%20spec.md">State diff compression v1 spec</a></p>
<h2 id="general-pubdata-format"><a class="header" href="#general-pubdata-format">General pubdata format</a></h2>
<p>At the end of the execution of the batch, the bootloader provides the <code>L1Messenger</code> with the preimages for the user
L2→L1 logs, L2→L1 long messages as well as uncompressed bytecodes. It also provides with compressed state diffs as well
as the original expanded state diff entries.</p>
<p>It will check that the preimages are correct as well as the fact that the compression is correct. It will output the
following three values via system logs:</p>
<ul>
<li>The root of the L2→L1 log Merkle tree. It will be stored and used for proving withdrawals.</li>
<li>The hash of the <code>totalPubdata</code> (i.e. the pubdata that contains the preimages above as well as packed state diffs).</li>
<li>The hash of the state diffs provided by the operator (it later on be included in the block commitment and its will be
enforced by the circuits).</li>
</ul>
<p>The <code>totalPubdata</code> has the following structure:</p>
<ol>
<li>First 4 bytes — the number of user L2→L1 logs in the batch</li>
<li>Then, the concatenation of packed L2→L1 user logs.</li>
<li>Next, 4 bytes — the number of long L2→L1 messages in the batch.</li>
<li>Then, the concatenation of L2→L1 messages, each in the format of <code>&lt;4 byte length || actual_message&gt;</code>.</li>
<li>Next, 4 bytes — the number of uncompressed bytecodes in the batch.</li>
<li>Then, the concatenation of uncompressed bytecodes, each in the format of <code>&lt;4 byte length || actual_bytecode&gt;</code>.</li>
<li>Next, 4 bytes — the length of the compressed state diffs.</li>
<li>Then, state diffs are compressed by the spec <a href="specs/contracts/settlement_contracts/data_availability/pubdata.html#state-diff-compression-format">above</a>.</li>
</ol>
<p>With Boojum, the interface for committing batches is the following one:</p>
<pre><code class="language-solidity">/// @notice Data needed to commit new batch
/// @param batchNumber Number of the committed batch
/// @param timestamp Unix timestamp denoting the start of the batch execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param priorityOperationsHash Hash of all priority operations from this batch
/// @param bootloaderHeapInitialContentsHash Hash of the initial contents of the bootloader heap. In practice it serves as the commitment to the transactions in the batch.
/// @param eventsQueueStateHash Hash of the events queue state. In practice it serves as the commitment to the events in the batch.
/// @param systemLogs concatenation of all L2 -&gt; L1 system logs in the batch
/// @param totalL2ToL1Pubdata Total pubdata committed to as part of bootloader run. Contents are: l2Tol1Logs &lt;&gt; l2Tol1Messages &lt;&gt; publishedBytecodes &lt;&gt; stateDiffs
struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes totalL2ToL1Pubdata;
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="state-diff-compression"><a class="header" href="#state-diff-compression">State diff Compression</a></h1>
<p>The most basic strategy to publish state diffs is to publish those in either of the following two forms:</p>
<ul>
<li>When a key is updated for the first time — <code>&lt;key, value&gt;</code>, where key is 32-byte derived key and the value is new
32-byte value of the slot.</li>
<li>When a key is updated for the second time and more — <code>&lt;enumeration_index, value&gt;</code>, where the <code>enumeration_index</code> is an
8-byte id of the slot and the value is the new 32-byte value of the slot.</li>
</ul>
<p>This compression strategy will utilize a similar idea for treating keys and values separately and it will be focused on
the efficient compression of keys and values separately.</p>
<h2 id="keys-2"><a class="header" href="#keys-2">Keys</a></h2>
<p>Keys will be packed in the same way as they were before. The only change is that we’ll avoid using the 8-byte
enumeration index and will pack it to the minimal necessary number of bytes. This number will be part of the pubdata.
Once a key has been used, it can already use the 4 or 5 byte enumeration index and it is very hard to have something
cheaper for keys that has been used already. The opportunity comes when remembering the ids for accounts to spare some
bytes on nonce/balance key, but ultimately the complexity may not be worth it.</p>
<p>There is some room for optimization of the keys that are being written for the first time, however, optimizing those is
more complex and achieves only a one-time effect (when the key is published for the first time), so they may be in scope
of the future upgrades.</p>
<h2 id="values-1"><a class="header" href="#values-1">Values</a></h2>
<p>Values are much easier to compress since they usually contain only zeroes. Also, we can leverage the nature of how those
values are changed. For instance, if nonce has been increased only by 1, we do not need to write the entire 32-byte new
value, we can just tell that the slot has been <em>increased</em> and then supply only the 1-byte value by which it was
increased. This way instead of 32 bytes we need to publish only 2 bytes: first byte to denote which operation has been
applied and the second by to denote the number by which the addition has been made.</p>
<p>We have the following 4 types of changes: <code>Add</code>, <code>Sub,</code> <code>Transform</code>, <code>NoCompression</code> where:</p>
<ul>
<li><code>NoCompression</code> denotes that the whole 32 byte will be provided.</li>
<li><code>Add</code> denotes that the value has been increased. (modulo 2^256)</li>
<li><code>Sub</code> denotes that the value has been decreased. (modulo 2^256)</li>
<li><code>Transform</code> denotes the value just has been changed (i.e. we disregard any potential relation between the previous and
the new value, though the new value might be small enough to save up on the number of bytes).</li>
</ul>
<p>Where the byte size of the output can be anywhere from 0 to 31 (also 0 makes sense for <code>Transform</code>, since it denotes
that it has been zeroed out). For <code>NoCompression</code> the whole 32 byte value is used.</p>
<p>So the format of the pubdata is the following:</p>
<p><strong>Part 1. Header.</strong></p>
<ul>
<li><code>&lt;version = 1 byte&gt;</code> — this will enable easier automated unpacking in the future. Currently, it will be only equal to
<code>1</code>.</li>
<li><code>&lt;total_logs_len = 3 bytes&gt;</code> — we need only 3 bytes to describe the total length of the L2→L1 logs.</li>
<li><code>&lt;the number of bytes used for derived keys = 1 byte&gt;</code>. It should be equal to the minimal required bytes to represent
the enum indexes for repeated writes.</li>
</ul>
<p><strong>Part 2. Initial writes.</strong></p>
<ul>
<li><code>&lt;num_of_initial_writes = 2 bytes&gt;</code> - the number of initial writes. Since each initial write publishes at least 32
bytes for key, then <code>2^16 * 32 = 2097152</code> will be enough for a lot of time (right now with the limit of 120kb it will
take more than 15 L1 txs to use up all the space there).</li>
<li>Then for each <code>&lt;key, value&gt;</code> pair for each initial write:
<ul>
<li>print key as 32-byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<p><strong>Part 3. Repeated writes.</strong></p>
<p>Note, that there is no need to write the number of repeated writes, since we know that until the end of the pubdata, all
the writes will be repeated ones.</p>
<ul>
<li>For each <code>&lt;key, value&gt;</code> pair for each repeated write:
<ul>
<li>print key as derived key by using the number of bytes provided in the header.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote
the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<h2 id="impact"><a class="header" href="#impact">Impact</a></h2>
<p>This setup allows us to achieve nearly 75% packing for values, and 50% gains overall in terms of the storage logs based
on historical data.</p>
<h2 id="encoding-of-packing-type"><a class="header" href="#encoding-of-packing-type">Encoding of packing type</a></h2>
<p>Since we have <code>32 * 3 + 1</code> ways to pack a state diff, we need at least 7 bits to present the packing type. To make
parsing easier, we will use 8 bits, i.e. 1 byte.</p>
<p>We will use the first 5 bits to represent the length of the bytes (from 0 to 31 inclusive) to be used. The other 3 bits
will be used to represent the type of the packing: <code>Add</code>, <code>Sub</code> , <code>Transform</code>, <code>NoCompression</code>.</p>
<h2 id="worst-case-scenario"><a class="header" href="#worst-case-scenario">Worst case scenario</a></h2>
<p>The worst case scenario for such packing is when we have to pack a completely random new value, i.e. it will take us 32
bytes to pack + 1 byte to denote which type it is. However, for such a write the user will anyway pay at least for 32
bytes. Adding an additional byte is roughly 3% increase, which will likely be barely felt by users, most of which use
storage slots for balances, etc, which will consume only 7-9 bytes for packed value.</p>
<h2 id="why-do-we-need-to-repeat-the-same-packing-method-id"><a class="header" href="#why-do-we-need-to-repeat-the-same-packing-method-id">Why do we need to repeat the same packing method id</a></h2>
<p>You might have noticed that for each pair <code>&lt;key, value&gt;</code> to describe value we always first write the packing type and
then write the packed value. However, the reader might ask, it is more efficient to just supply the packing id once and
then list all the pairs <code>&lt;key, value&gt;</code> which use such packing.</p>
<p>I.e. instead of listing</p>
<p>(key = 0, type = 1, value = 1), (key = 1, type = 1, value = 3), (key = 2, type = 1, value = 4), …</p>
<p>Just write:</p>
<p>type = 1, (key = 0, value = 1), (key = 1, value = 3), (key = 2, value = 4), …</p>
<p>There are two reasons for it:</p>
<ul>
<li>A minor reason: sometimes it is less efficient in case the packing is used for very few slots (since for correct
unpacking we need to provide the number of slots for each packing type).</li>
<li>A fundamental reason: currently enum indices are stored directly in the merkle tree &amp; have very strict order of
incrementing enforced by the circuits and (they are given in order by pairs <code>(address, key)</code>), which are generally not
accessible from pubdata.</li>
</ul>
<p>All this means that we are not allowed to change the order of “first writes” above, so indexes for them are directly
recoverable from their order, and so we can not permute them. If we were to reorder keys without supplying the new
enumeration indices for them, the state would be unrecoverable. Always supplying the new enum index may add additional 5
bytes for each key, which might negate the compression benefits in a lot of cases. Even if the compression will still be
beneficial, the added complexity may not be worth it.</p>
<p>That being said, we <em>could</em> rearrange those for <em>repeated</em> writes, but for now we stick to the same value compression
format for simplicity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="l2-state-reconstruction-tool"><a class="header" href="#l2-state-reconstruction-tool">L2 State Reconstruction Tool</a></h1>
<p>Given that we post all data to L1, there is a tool, created by the <a href="https://equilibrium.co/">Equilibrium Team</a> that
solely uses L1 pubdata for reconstructing the state and verifying that the state root on L1 can be created using
pubdata. A link to the repo can be found <a href="https://github.com/eqlabs/zksync-state-reconstruct">here</a>. The way the tool
works is by parsing out all the L1 pubdata for an executed batch, comparing the state roots after each batch is
processed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validium-and-zkporter"><a class="header" href="#validium-and-zkporter">Validium and zkPorter</a></h1>
<p>The may choose not to post their data to L1, in which case they become a validium. This makes transactions there much
cheaper, but less secure. Because the ZK Stack uses state diffs to post data, it can combine the rollup and validium
features, by separating storage slots that need to post data from the ones that don’t. This construction combines the
benefits of rollups and validiums, and it is called a
<a href="https://blog.matter-labs.io/zkporter-composable-scalability-in-l2-beyond-zkrollup-2a30c4d69a75">zkPorter</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-da-support"><a class="header" href="#custom-da-support">Custom DA Support</a></h1>
<h2 id="intro-1"><a class="header" href="#intro-1">Intro</a></h2>
<p>We introduced modularity into our contracts to support multiple DA layers, to support Validium and Rollup modes more easily, and to enable settlement via the Gateway</p>
<p><img src="specs/contracts/settlement_contracts/data_availability/./img/custom_da.png" alt="The contracts for the rollup case" />
<img src="specs/contracts/settlement_contracts/data_availability/./img/custom_da_external.png" alt="The general architecture" /></p>
<h3 id="background"><a class="header" href="#background">Background</a></h3>
<p><strong>Pubdata</strong> – information published by the ZK Chain that can be used to reconstruct its state; it consists of L2→L1 logs, L2→L1 messages, contract bytecodes, and compressed state diffs.</p>
<pre><code class="language-solidity">struct PubdataInput {
    pub(crate) user_logs: Vec&lt;L1MessengerL2ToL1Log&gt;,
    pub(crate) l2_to_l1_messages: Vec&lt;Vec&lt;u8&gt;&gt;,
    pub(crate) published_bytecodes: Vec&lt;Vec&lt;u8&gt;&gt;,
    pub(crate) state_diffs: Vec&lt;StateDiffRecord&gt;,
}
</code></pre>
<p>The current version of ZK Chains supports the following Data Availability commitment schemes:</p>
<ul>
<li>
<p>EMPTY_NO_DA (Validium)<br />
No on-chain DA enforcement: all commitments are <code>bytes32(0)</code>.</p>
</li>
<li>
<p>PUBDATA_KECCAK256<br />
Commitment =</p>
<pre><code class="language-solidity">keccak256(uncompressedStateDiffHash || keccak256(pubdata))
</code></pre>
<p>Suitable for calldata or external DA layers that can verify a simple keccak-based proof.</p>
</li>
<li>
<p>BLOBS_AND_PUBDATA_KECCAK256
Includes EIP-4844 blobs. Commitment =</p>
<pre><code class="language-solidity">keccak256(
  uncompressedStateDiffHash ||
  keccak256(pubdata) ||
  uint8(blobLinearHashes.length) ||
  blobLinearHashes
)
</code></pre>
</li>
<li>
<p>NONE
A placeholder for “no scheme” (should never be chosen).</p>
</li>
</ul>
<p>This means that a separate solution like AvailDA, EigenDA, Celestia, etc. could be used to store the pubdata. Each DA layer provides an inclusion proof of our pubdata in its storage, and that proof can later be verified on Ethereum. This gives stronger guarantees than <code>No DA Validium</code>, but lower fees than <code>Blobs</code> when Ethereum usage grows.</p>
<p>This allows for a general-purpose solution that ensures DA consistency and verifiability, on top of which we build what partners need—on-chain games, DEXes, and more: <strong>Validium with Abstract DA</strong>.</p>
<h2 id="implementation-overview"><a class="header" href="#implementation-overview">Implementation overview</a></h2>
<ol>
<li>
<p>Configuration
In Admin facet of ZK Chain, the desired pair of <code>L2DACommitmentScheme</code> and the address of the L1 DA validator can be set by admin.</p>
</li>
<li>
<p>L2 commitment generation<br />
All commitment logic lives in the <code>L2DAValidator</code> library, which computes the <code>pubdataDACommitment</code> based on the chosen scheme.</p>
</li>
<li>
<p>Emission
Bootloader performs L1 Messenger pubdata “publishing” call to <code>L1Messenger</code> after executing the batch.
<code>L1Messenger.publishPubdataAndClearState()</code> emits two DA-related system logs per batch:</p>
<ul>
<li><code>USED_L2_DA_VALIDATION_COMMITMENT_SCHEME_KEY</code> (the scheme index)</li>
<li><code>L2_DA_VALIDATOR_OUTPUT_HASH_KEY</code> (the commitment)</li>
</ul>
</li>
<li>
<p>L1 verification<br />
On L1 the Executor facet will call L1 DA validator, providing it with chain ID and batch number of the chain for which the DA must be verified, along with <code>l2DAValidatorOutputHash</code>, <code>operatorDAInput</code> and <code>_maxBlobsSupported</code> value. It’s then L1 DA Validator job to verify the correctness of the DA on L1.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rollup-da"><a class="header" href="#rollup-da">Rollup DA</a></h1>
<h2 id="prerequisites-5"><a class="header" href="#prerequisites-5">Prerequisites</a></h2>
<p>Before reading this document, it is recommended to understand how the <a href="specs/contracts/settlement_contracts/data_availability/./custom_da.html">custom DA</a> works in general.</p>
<h2 id="eip-4844-support"><a class="header" href="#eip-4844-support">EIP-4844 Support</a></h2>
<p>EIP-4844, commonly known as Proto-Danksharding, is an upgrade to the Ethereum protocol that introduces a new data availability solution embedded in layer 1. More information about it can be found <a href="https://ethereum.org/en/roadmap/danksharding/">here</a>.</p>
<p>To facilitate EIP-4844 blob support, our circuits allow providing two arrays in our public input to the circuit:</p>
<ul>
<li><code>blobCommitments</code> – the commitment that helps to check the correctness of the blob content. The formula on how it is computed will be explained below in the document.</li>
<li><code>blobHash</code> – the <code>keccak256</code> hash of the inner contents of the blob.</li>
</ul>
<p>Note that our circuits require that each blob contains exactly <code>4096 * 31</code> bytes. The maximal number of blobs that are supported by our proving system is 16, but the system contracts support only 6 blobs at most for now.</p>
<p>When committing a batch, the <code>L1DAValidator</code> is called with the data provided by the operator and it should return the two arrays described above. These arrays are put inside the batch commitment and then the correctness of the commitments will be verified at the proving stage.</p>
<p>Note that the <code>Executor.sol</code> (and the contract itself) is not responsible for checking that the provided <code>blobHash</code> and <code>blobCommitments</code> in any way correspond to the pubdata inside the batch as it is the job of the DA Validation in corresponding contracts on L1 (<code>L1DAValidator</code>) and L2 (<code>L2DAValidator</code> library).</p>
<h2 id="publishing-pubdata-to-l1"><a class="header" href="#publishing-pubdata-to-l1">Publishing pubdata to L1</a></h2>
<p>Let’s see an example of how the approach above works in rollup DA validators.</p>
<h3 id="rollup-use-case-of-l2davalidator-library"><a class="header" href="#rollup-use-case-of-l2davalidator-library">Rollup use case of L2DAValidator library</a></h3>
<p><img src="specs/contracts/settlement_contracts/data_availability/./img/custom_da.png" alt="RollupUsecaseL2DAValidator.png" /></p>
<p>Let’s consider <code>BLOBS_AND_PUBDATA_KECCAK256</code> commitment scheme. This is the one that’s being used in default <code>RollupL1DAValidator</code>. In this case, the following will happen:</p>
<p><code>L2DAValidator</code> library accepts the preimages for the data to publish as well as their compressed format. After verifying the compression, it forms the <code>_totalPubdata</code> bytes array, which represents the entire blob of data that should be published to L1.</p>
<p>It calls the <code>PubdataChunkPublisher</code> system contract to split this pubdata into multiple “chunks” of size <code>4096 * 31</code> bytes and return the <code>keccak256</code> hash of those, These will be the <code>blobHash</code> of from the section before.</p>
<p>To give the flexibility of checking different DA, we send the following data to L1:</p>
<ul>
<li><code>l2DAValidatorOutputHash</code> that was returned by <code>L2DAValidator</code> library. This hash includes <code>uncompressedStateDiffHash</code>, <code>pubdata</code>, the number of blobs, and <code>blobLinearHashes</code>. This hash, alongside the <code>_operatorDAInput</code> will be provided to <code>L1DAValidator</code> (<code>RollupL1DAValidator.sol</code> is the one that accepts <code>BLOBS_AND_PUBDATA_KECCAK256</code>, for example).</li>
<li><code>_l2DACommitmentScheme</code> that denotes the commitment scheme that was used. In the case we’re looking into, it’s the <code>BLOBS_AND_PUBDATA_KECCAK256</code>.</li>
</ul>
<h3 id="rollupl1davalidator"><a class="header" href="#rollupl1davalidator">RollupL1DAValidator</a></h3>
<p>When committing the batch, the operator will provide the <code>_operatorDAInput</code> and <code>_l2DAValidatorOutputHash</code>. Using these values, <code>RollupL1DAValidator</code> parses the input that the L2 DA validator has provided to it into <code>stateDiffHash</code>, <code>fullPubdataHash</code>, <code>blobsLinearHashes</code>, <code>blobsProvided</code>, <code>l1DaInput</code>. This <code>l1DaInput</code> will be used to prove that the pubdata was indeed provided in this batch.</p>
<p>The first byte of the <code>l1DaInput</code> denotes which way of pubdata publishing was used: calldata or blobs.</p>
<p>In case it is calldata it will be just checked that the provided calldata matches the hash of the <code>fullPubdataHash</code> that was sent by the L2 counterpart. Note that calldata may still contain the blob information as we typically start generating proofs before we know which way of calldata will be used. Note that in case the calldata is used for DA, we do not verify the <code>blobCommitments</code> as the presence of the correct pubdata has been verified already.</p>
<p>In case it is blobs, we need to construct the <code>blobCommitment</code>s correctly for each of the blob of data.</p>
<p>For each of the <code>blob</code>s the operator provides so called <code>_commitment</code> that consists of the following packed structure: <code>opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)</code>.</p>
<p>The verification of the <code>_commitment</code> can be summarized in the following snippet:</p>
<pre><code class="language-solidity">// The opening point is passed as 16 bytes as that is what our circuits expect and use when verifying the new batch commitment
// PUBDATA_COMMITMENT_SIZE = 144 bytes
pubdata_commitments &lt;- [opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)] from calldata
opening_point = bytes32(pubdata_commitments[:16])
versioned_hash &lt;- from BLOBHASH opcode

// Given that we needed to pad the opening point for the precompile, append the data after.
point_eval_input = versioned_hash || opening_point || pubdata_commitments[16: PUBDATA_COMMITMENT_SIZE]

// this part handles the following:
// verify versioned_hash == hash(commitment)
// verify P(z) = y
res &lt;- point_valuation_precompile(point_eval_input)

assert uint256(res[32:]) == BLS_MODULUS
</code></pre>
<p>The final <code>blobCommitment</code> is calculated as the hash between the <code>blobVersionedHash</code>, <code>opening point</code> and the <code>claimed value</code>. The zero knowledge circuits will verify that the opening point and the claimed value were calculated correctly and correspond to the data that was hashed under the <code>blobHash</code>.</p>
<h2 id="structure-of-the-pubdata"><a class="header" href="#structure-of-the-pubdata">Structure of the pubdata</a></h2>
<p>Rollups maintain the same structure of pubdata and apply the same rules for compression as those that were used in the previous versions of the system. These can be read <a href="specs/contracts/settlement_contracts/data_availability/./state_diff_compression_v1_spec.html">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="standard-pubdata-format"><a class="header" href="#standard-pubdata-format">Standard pubdata format</a></h1>
<p>With the introduction of <a href="specs/contracts/settlement_contracts/data_availability/./custom_da.html">custom DA validation</a>, different pubdata logic can be applied for each chain (including calldata-based pubdata); however, ZK chains are generally optimized for a state-diffs–based rollup model.</p>
<p>This document describes how the standard pubdata format looks. This is the format that is enforced for <a href="specs/contracts/settlement_contracts/data_availability/../../chain_management/admin_role.html#ispermanentrollup-setting">permanent rollup chains</a>.</p>
<p>Pubdata in ZKsync can be divided up into 4 categories:</p>
<ol>
<li>L2 to L1 Logs</li>
<li>L2 to L1 Messages</li>
<li>Smart Contract Bytecodes</li>
<li>Storage Writes</li>
</ol>
<p>Using data corresponding to these 4 facets across all executed batches, we’re able to reconstruct the full state of L2. To restore the state we just need to filter all of the transactions to the L1 ZKsync contract for only the <code>commitBatches</code> transactions where the proposed block has been referenced by a corresponding <code>executeBatches</code> call (the reason for this is that a committed or even proven block can be reverted but an executed one cannot). Once we have all the committed batches that have been executed, we will then pull the transaction input and the relevant fields, applying them in order to reconstruct the current state of L2.</p>
<h2 id="l2l1-communication-2"><a class="header" href="#l2l1-communication-2">L2→L1 communication</a></h2>
<p>We will implement the calculation of the Merkle root of the L2→L1 messages via a system contract as part of the <code>L1Messenger</code>. Basically, whenever a new user-emitted log that needs to be Merklized is created, the <code>L1Messenger</code> contract will append it to its rolling hash and then, at the end of the batch during the formation of the blob, it will receive the original preimages from the operator, verify their consistency, and send those to the <code>L2DAValidator</code> library to facilitate the DA protocol.</p>
<p>We will now refer to the logs that are created by users and Merklized as <em>user</em> logs, and the logs that are emitted natively by the VM as <em>system</em> logs. Here is a short comparison table for better understanding:</p>
<div class="table-wrapper"><table><thead><tr><th>System logs</th><th>User logs</th></tr></thead><tbody>
<tr><td>Emitted by VM via an opcode.</td><td>VM knows nothing about them.</td></tr>
<tr><td>Consistency and correctness are enforced by the verifier on L1 (i.e. their hash is part of the block commitment).</td><td>Consistency and correctness is enforced by the L1Messenger system contract. The correctness of the behavior of the L1Messenger is enforced implicitly by the prover in the sense that it proves the correctness of the execution overall.</td></tr>
<tr><td>We don’t calculate their Merkle root.</td><td>We calculate their Merkle root on the L1Messenger system contract.</td></tr>
<tr><td>There is a constant small number of these logs.</td><td>We can have as many as possible as long as the commitBatches function on L1 remains executable (it is the job of the operator to ensure that only such transactions are selected).</td></tr>
<tr><td>In EIP-4844 they will remain part of the calldata.</td><td>In EIP-4844 they will become part of the blobs.</td></tr>
</tbody></table>
</div>
<h3 id="backwards-compatibility-1"><a class="header" href="#backwards-compatibility-1">Backwards-compatibility</a></h3>
<p>Note that to maintain a unified interface with the previous version of the protocol, the leaves of the Merkle tree will have to maintain the following structure:</p>
<pre><code class="language-solidity">struct L2Log {
  uint8 l2ShardId;
  bool isService;
  uint16 txNumberInBlock;
  address sender;
  bytes32 key;
  bytes32 value;
}
</code></pre>
<p>The leaf will look as follows:</p>
<pre><code class="language-solidity">bytes32 hashedLog = keccak256(
    abi.encodePacked(_log.l2ShardId, _log.isService, _log.txNumberInBlock, _log.sender, _log.key, _log.value)
);
</code></pre>
<p><code>keccak256</code> will continue being the function for the Merkle tree.</p>
<p>To put it shortly, the proofs for L2→L1 log inclusion will continue to have exactly the same format as they did in the pre-Boojum system, which avoids breaking changes for SDKs and bridges alike.</p>
<h3 id="implementation-of-l1messenger-1"><a class="header" href="#implementation-of-l1messenger-1">Implementation of <code>L1Messenger</code></a></h3>
<p>The L1Messenger contract will maintain a rolling hash of all the L2ToL1 logs <code>chainedLogsHash</code> as well as the rolling hash of messages <code>chainedMessagesHash</code>. Whenever a contract wants to send an L2→L1 log, the following operation will be <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/contracts/L1Messenger.sol#L73">applied</a>:</p>
<pre><code>chainedLogsHash = keccak256(chainedLogsHash, hashedLog)
</code></pre>
<p>L2→L1 logs have the same 88-byte format as in the current version of ZKsync.</p>
<p>Note that the user is charged for the future computation needed to calculate the final Merkle root. It is roughly 4× higher than the cost to calculate the hash of the leaf, since the eventual tree might be 4× the number of nodes. In any case, this will likely be a relatively negligible part compared to the cost of the pubdata.</p>
<p>At the end of the execution, the bootloader will <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/bootloader/bootloader.yul#L2676">provide</a> a list of all the L2ToL1 logs (this will be provided by the operator in the memory of the bootloader). The L1Messenger checks that the rolling hash from the provided logs is the same as in the <code>chainedLogsHash</code> and calculates the Merkle tree of the provided messages. Right now, we always build the Merkle tree of size <code>16384</code>, but we charge the user as if the tree were built dynamically based on the number of leaves. The implementation of the dynamic tree has been postponed until later upgrades.</p>
<blockquote>
<p>Note that, unlike most other parts of pubdata, the user L2→L1 must always be validated by the trusted <code>L1Messenger</code> system contract. If we moved this responsibility to <code>L2DAValidator</code> library it would be possible for a malicious operator to provide incorrect data and forge transactions out of names of certain users.</p>
</blockquote>
<h3 id="long-l2l1-messages--bytecodes-2"><a class="header" href="#long-l2l1-messages--bytecodes-2">Long L2→L1 messages &amp; bytecodes</a></h3>
<p>If the user wants to send an L2→L1 message, its preimage is <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/contracts/L1Messenger.sol#L126">appended</a> to the message’s rolling hash to:</p>
<pre><code>chainedMessagesHash = keccak256(abi.encode(chainedMessagesHash, hash));
</code></pre>
<p>A very similar approach for bytecodes is used, where their rolling hash is calculated and then the preimages are provided at the end of the batch to form the full pubdata for the batch.</p>
<p>Note that for backward compatibility, just like before, any long message or bytecode is accompanied by the corresponding user L2→L1 log.</p>
<h3 id="using-system-l2l1-logs-vs-the-user-logs-1"><a class="header" href="#using-system-l2l1-logs-vs-the-user-logs-1">Using system L2→L1 logs vs the user logs</a></h3>
<p>The content of the L2→L1 logs by the L1Messenger will go to the blob of EIP-4844. Meaning that all the data that belongs to the tree by L1Messenger’s L2→L1 logs should not be needed during block commitment. Also, note that in the future we will remove the calculation of the Merkle root of the built-in L2→L1 messages.</p>
<p>The only places where the built-in L2→L1 messaging should continue to be used:</p>
<ul>
<li>Logs by SystemContext (they are needed on commit to check the previous block hash).</li>
<li>Logs by L1Messenger for the Merkle root of the L2→L1 tree as well as the data needed for <code>L1DAValidator</code>.</li>
<li><code>chainedPriorityTxsHash</code> and <code>numberOfLayer1Txs</code> from the bootloader (read more about it below).</li>
</ul>
<h3 id="obtaining-txnumberinblock-1"><a class="header" href="#obtaining-txnumberinblock-1">Obtaining <code>txNumberInBlock</code></a></h3>
<p>To have the same log format, the <code>txNumberInBlock</code> must be obtained. While it is internally counted in the VM, there is currently no opcode to retrieve this number. We will have a public variable <code>txNumberInBlock</code> in the <code>SystemContext</code>, which will be incremented with each new transaction and from which it can be retrieved. It is <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/contracts/SystemContext.sol#L515">zeroed out</a> at the end of the batch.</p>
<h3 id="bootloader-implementation-1"><a class="header" href="#bootloader-implementation-1">Bootloader implementation</a></h3>
<p>The bootloader has a memory segment dedicated to the ABI-encoded data of the L1ToL2Messenger to perform the <code>publishPubdataAndClearState</code> call.</p>
<p>At the end of the execution of the batch, the operator should provide the corresponding data in the bootloader memory, i.e., user L2→L1 logs, long messages, bytecodes, etc. After that, the <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/bootloader/bootloader.yul#L2676">call</a> is performed to the <code>L1Messenger</code> system contract, which would call the <code>L2DAValidator</code> library to check the adherence of the pubdata to the specified format.</p>
<h2 id="bytecode-publishing-1"><a class="header" href="#bytecode-publishing-1">Bytecode Publishing</a></h2>
<p>Within pubdata, bytecodes are published in one of two ways:<br />
(1) uncompressed as part of the bytecodes array and<br />
(2) compressed via long L2→L1 messages.</p>
<h3 id="uncompressed-bytecode-publishing-1"><a class="header" href="#uncompressed-bytecode-publishing-1">Uncompressed Bytecode Publishing</a></h3>
<p>Uncompressed bytecodes are included within the <code>totalPubdata</code> bytes and have the following format:</p>
<pre><code>number of bytecodes || forEachBytecode (length of bytecode(n) || bytecode(n))
</code></pre>
<h3 id="compressed-bytecode-publishing-1"><a class="header" href="#compressed-bytecode-publishing-1">Compressed Bytecode Publishing</a></h3>
<p>Unlike uncompressed bytecode, which is published as part of <code>factoryDeps</code>, compressed bytecodes are published as long L2→L1 messages, which can be seen <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/system-contracts/contracts/Compressor.sol#L78">here</a>.</p>
<h4 id="bytecode-compression-algorithm--server-side-1"><a class="header" href="#bytecode-compression-algorithm--server-side-1">Bytecode Compression Algorithm — Server Side</a></h4>
<p>This is the part that is responsible for taking bytecode that has already been chunked into 8-byte words, performing validation, and compressing it.</p>
<p>Each 8-byte word from the chunked bytecode is assigned a 2-byte index (constraint on size of dictionary of chunk → index is 2^16 – 1 elements). The length of the dictionary, dictionary entries (index assumed through order), and indexes are all concatenated together to yield the final compressed version.</p>
<p>For bytecode to be considered valid, it must satisfy the following:</p>
<ol>
<li>Bytecode length must be less than 2097120 ((2^16 – 1) * 32) bytes.</li>
<li>Bytecode length must be a multiple of 32.</li>
<li>Number of 32-byte words cannot be even.</li>
</ol>
<p>The following is a simplified version of the algorithm:</p>
<pre><code class="language-python">statistic: Map[chunk, (count, first_pos)]
dictionary: Map[chunk, index]
encoded_data: List[index]

for position, chunk in chunked_bytecode:
 if chunk is in statistic:
  statistic[chunk].count += 1
 else:
  statistic[chunk] = (count=1, first_pos=pos)

# We want the more frequently used bytes to have smaller ids to save on calldata (zero bytes cost less)
statistic.sort(primary=count, secondary=first_pos, order=desc)

for index, chunk in enumerated(sorted_statistics):
  dictionary[chunk] = index

for chunk in chunked_bytecode:
 encoded_data.append(dictionary[chunk])

return [len(dictionary), dictionary.keys(order=index asc), encoded_data]
</code></pre>
<h4 id="verification-and-publishing--l2-contract-1"><a class="header" href="#verification-and-publishing--l2-contract-1">Verification And Publishing — L2 Contract</a></h4>
<p>The function <code>publishCompressBytecode</code> takes in both the original <code>_bytecode</code> and the <code>_rawCompressedData</code>, the latter of which comes from the output of the server’s compression algorithm. Looping over the encoded data derived from <code>_rawCompressedData</code>, the corresponding chunks are pulled from the dictionary and compared to the original bytecode, reverting if there is a mismatch. After the encoded data has been verified, it is published to L1 and marked accordingly within the <code>KnownCodesStorage</code> contract.</p>
<p>Pseudo-code implementation:</p>
<pre><code class="language-python">length_of_dict = _rawCompressedData[:2]
dictionary = _rawCompressedData[2:2 + length_of_dict * 8] # need to offset by bytes used to store length (2) and multiply by 8 for chunk size
encoded_data = _rawCompressedData[2 + length_of_dict * 8:]

assert(len(dictionary) % 8 == 0) # each element should be 8 bytes
assert(num_entries(dictionary) &lt;= 2^16)
assert(len(encoded_data) * 4 == len(_bytecode)) # given that each chunk is 8 bytes and each index is 2 bytes they should differ by a factor of 4

for (index, dict_index) in list(enumerate(encoded_data)):
 encoded_chunk = dictionary[dict_index]
 real_chunk = _bytecode.readUint64(index * 8) # need to pull from index * 8 to account for difference in element size
 verify(encoded_chunk == real_chunk)

# Sending the compressed bytecode to L1 for data availability
sendToL1(_rawCompressedBytecode)
markAsPublished(hash(_bytecode))
</code></pre>
<h2 id="storage-diff-publishing-1"><a class="header" href="#storage-diff-publishing-1">Storage diff publishing</a></h2>
<p>ZKsync is a state-diff–based rollup and so publishing the correct state diffs plays an integral role in ensuring data availability.</p>
<h3 id="difference-between-initial-and-repeated-writes-1"><a class="header" href="#difference-between-initial-and-repeated-writes-1">Difference between initial and repeated writes</a></h3>
<p>ZKsync publishes state changes that happened within the batch instead of transactions themselves. Meaning that, for instance, some storage slot <code>S</code> under account <code>A</code> has changed to value <code>V</code>, we could publish a triple of <code>A,S,V</code>. Users, by observing all the triples, could restore the state of ZKsync. However, note that our tree, unlike Ethereum’s one, is not account based (i.e. there is no first layer of depth 160 of the Merkle tree corresponding to accounts and a second layer of depth 256 of the Merkle tree corresponding to users). Our tree is “flat,” i.e. a slot <code>S</code> under account <code>A</code> is just stored in the leaf number <code>H(S,A)</code>. Our tree is of depth 256 + 8 (the 256 is for these hashed account/key pairs and 8 is for potential shards in the future; we currently have only one shard and it is irrelevant for the rest of the document).</p>
<p>We call this <code>H(S,A)</code> a <em>derived key</em>, because it is derived from the address and the actual key in the storage of the account. Since our tree is flat, whenever a change happens, we can publish a pair <code>DK, V</code>, where <code>DK=H(S,A)</code>.</p>
<p>However, there is an optimization that can be done:</p>
<ul>
<li>Whenever a change to a key is used for the first time, we publish a pair of <code>DK,V</code> and we assign some sequential id to this derived key. This is called an <em>initial write</em>. It happens for the first time and that’s why we must publish the full key.</li>
<li>If this storage slot is published in some of the subsequent batches, instead of publishing the whole <code>DK</code>, we can use the sequential id instead. This is called a <em>repeated write</em>.</li>
</ul>
<p>For instance, if the slots <code>A</code>, <code>B</code> (I’ll use Latin letters instead of 32-byte hashes for readability) changed their values to <code>12</code>, <code>13</code> accordingly in the batch, they will be published in the following format:</p>
<ul>
<li><code>(A, 12), (B, 13)</code>. Let’s say that the last sequential id ever used is 6. Then, <code>A</code> will receive the id of 7 and <code>B</code> will receive the id of 8.</li>
</ul>
<p>Let’s say that in the next block, they change their values to <code>13</code>, <code>14</code>. Then, their diff will be published in the following format:</p>
<ul>
<li><code>(7, 13), (8, 14)</code>.</li>
</ul>
<p>The id is permanently assigned to each storage key that was ever published. While in the description above it may not seem like a huge boost, each <code>DK</code> is 32 bytes long and the id is at most 8 bytes long.</p>
<p>We call this id <em>enumeration_index</em>.</p>
<p>Note that the enumeration indexes are assigned in the order of a sorted array of (address, key) pairs, i.e. they are internally sorted. The enumeration indexes are part of the state Merkle tree; it is <strong>crucial</strong> that the initial writes are published in the correct order, so that anyone could restore the correct enumeration indexes for the storage slots. In addition, an enumeration index of <code>0</code> indicates that the storage write is an initial write.</p>
<h3 id="state-diffs-structure"><a class="header" href="#state-diffs-structure">State diffs structure</a></h3>
<p>Firstly, let’s define what we mean by <em>state diffs</em>. A <em>state diff</em> is an element of the following structure.</p>
<p><a href="https://github.com/matter-labs/zksync-protocol/blob/main/crates/circuit_encodings/src/state_diff_record.rs#L8">State diff structure</a>.</p>
<p>Basically, it contains all the values which might interest us about the state diff:</p>
<ul>
<li><code>address</code> where the storage has been changed.</li>
<li><code>key</code> (the original key inside the address).</li>
<li><code>derived_key</code> — <code>H(key, address)</code> as described in the previous section.
<ul>
<li>Note: the hashing algorithm currently used here is <code>Blake2s</code>.</li>
</ul>
</li>
<li><code>enumeration_index</code> — enumeration index as explained above. It is equal to 0 if the write is initial and contains the non-zero enumeration index if it is a repeated write (indexes start from 1).</li>
<li><code>initial_value</code> — the value that was present in the key at the start of the batch.</li>
<li><code>final_value</code> — the value that the key has changed to by the end of the batch.</li>
</ul>
<p>We will consider <code>stateDiffs</code> an array of such objects, sorted by (address, key).</p>
<p>This is the internal structure that is used by the circuits to represent the state diffs. The most basic “compression” algorithm is the one described above:</p>
<ul>
<li>For initial writes, write the pair (<code>derived_key</code>, <code>final_value</code>).</li>
<li>For repeated writes, write the pair (<code>enumeration_index</code>, <code>final_value</code>).</li>
</ul>
<p>Note that values like <code>initial_value</code>, <code>address</code>, and <code>key</code> are not used in the “simplified” algorithm above, but they will be helpful for more advanced compression algorithms in the future. The <a href="specs/contracts/settlement_contracts/data_availability/standard_pubdata_format.html#state-diff-compression-format">algorithm</a> for Boojum already utilizes the difference between the <code>initial_value</code> and <code>final_value</code> to save on pubdata.</p>
<h3 id="how-the-new-pubdata-verification-works"><a class="header" href="#how-the-new-pubdata-verification-works">How the new pubdata verification works</a></h3>
<h4 id="l2-1"><a class="header" href="#l2-1"><strong>L2</strong></a></h4>
<ol>
<li>The operator provides both full <code>stateDiffs</code> (i.e. the array of the structs above) and the compressed state diffs (i.e. the array containing the state diffs, compressed by the algorithm explained <a href="specs/contracts/settlement_contracts/data_availability/standard_pubdata_format.html#state-diff-compression-format">below</a>).</li>
<li>The <code>L2DAValidator</code> library must verify that the compressed version is consistent with the original stateDiffs and send the <em>hash</em> of the <code>stateDiffs</code> to its L1 counterpart. It will also include the compressed state diffs in the totalPubdata to be published onto L1.</li>
</ol>
<h4 id="l1-1"><a class="header" href="#l1-1"><strong>L1</strong></a></h4>
<ol>
<li>During block commitment, the standard DA protocol follows and the <code>L1DAValidator</code> is responsible for checking that the operator has provided the preimage for the <code>_totalPubdata</code>. More on how this is checked can be seen <a href="specs/contracts/settlement_contracts/data_availability/./rollup_da.html">here</a>.</li>
<li>The block commitment <a href="https://github.com/matter-labs/era-contracts/blob/b43cf6b3b069c85aec3cd61d33dd3ae2c462c896/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol#L550">includes</a> <em>the hash of the <code>stateDiffs</code></em>. Thus, ZKP verification will fail if the provided stateDiffs hash is not correct.</li>
</ol>
<p>It is a secure construction because the proof can be verified only if both the execution was correct and the hash of the <code>stateDiffs</code> is correct. This means that the <code>L2DAValidator</code> library indeed received the array of correct <code>stateDiffs</code> and, assuming the <code>L2DAValidator</code> is working correctly, double-checked that the compression is in the correct format, while L1 contracts at the commit stage double-checked that the operator provided the preimage for the compressed state diffs.</p>
<h3 id="state-diff-compression-format-1"><a class="header" href="#state-diff-compression-format-1">State diff compression format</a></h3>
<p>The following algorithm is used for the state diff compression:</p>
<p><a href="specs/contracts/settlement_contracts/data_availability/./state_diff_compression_v1_spec.html">State diff compression v1 spec</a></p>
<h2 id="general-pubdata-format-1"><a class="header" href="#general-pubdata-format-1">General pubdata format</a></h2>
<p>The <code>totalPubdata</code> has the following structure:</p>
<ol>
<li>First 4 bytes — the number of user L2→L1 logs in the batch.</li>
<li>Then, the concatenation of packed L2→L1 user logs.</li>
<li>Next, 4 bytes — the number of long L2→L1 messages in the batch.</li>
<li>Then, the concatenation of L2→L1 messages, each in the format <code>&lt;4 byte length || actual_message&gt;</code>.</li>
<li>Next, 4 bytes — the number of uncompressed bytecodes in the batch.</li>
<li>Then, the concatenation of uncompressed bytecodes, each in the format <code>&lt;4 byte length || actual_bytecode&gt;</code>.</li>
<li>Next, 4 bytes — the length of the compressed state diffs.</li>
<li>Then, state diffs are compressed by the spec <a href="specs/contracts/settlement_contracts/data_availability/standard_pubdata_format.html#state-diff-compression-format">above</a>.</li>
</ol>
<p>The interface for committing batches is the following:</p>
<pre><code class="language-solidity">/// @notice Data needed to commit new batch
/// @param batchNumber Number of the committed batch
/// @param timestamp Unix timestamp denoting the start of the batch execution
/// @param indexRepeatedStorageChanges The serial number of the shortcut index that's used as a unique identifier for storage keys that were used twice or more
/// @param newStateRoot The state root of the full state tree
/// @param numberOfLayer1Txs Number of priority operations to be processed
/// @param priorityOperationsHash Hash of all priority operations from this batch
/// @param bootloaderHeapInitialContentsHash Hash of the initial contents of the bootloader heap. In practice it serves as the commitment to the transactions in the batch.
/// @param eventsQueueStateHash Hash of the events queue state. In practice it serves as the commitment to the events in the batch.
/// @param systemLogs Concatenation of all L2→L1 system logs in the batch
/// @param totalL2ToL1Pubdata Total pubdata committed to as part of bootloader run. Contents are: l2Tol1Logs &lt;&gt; l2Tol1Messages &lt;&gt; publishedBytecodes &lt;&gt; stateDiffs
struct CommitBatchInfo {
  uint64 batchNumber;
  uint64 timestamp;
  uint64 indexRepeatedStorageChanges;
  bytes32 newStateRoot;
  uint256 numberOfLayer1Txs;
  bytes32 priorityOperationsHash;
  bytes32 bootloaderHeapInitialContentsHash;
  bytes32 eventsQueueStateHash;
  bytes systemLogs;
  bytes totalL2ToL1Pubdata;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="state-diff-compression-v1-spec"><a class="header" href="#state-diff-compression-v1-spec">State diff compression v1 spec</a></h1>
<p>The most basic strategy to publish state diffs is to publish those in either of the following two forms:</p>
<ul>
<li>When a key is updated for the first time — <code>&lt;key, value&gt;</code>, where key is 32-byte derived key and the value is new 32-byte value of the slot.</li>
<li>When a key is updated for the second and subsequent updates — <code>&lt;enumeration_index, value&gt;</code>, where the <code>enumeration_index</code> is an 8-byte id of the slot and the value is the new 32-byte value of the slot.</li>
</ul>
<p>This compression strategy will utilize a similar idea for treating keys and values separately and it will be focused on the efficient compression of keys and values separately.</p>
<h2 id="keys-3"><a class="header" href="#keys-3">Keys</a></h2>
<p>Keys will be packed in the same way as they were before. The only change is that we’ll avoid using the 8-byte enumeration index and will pack it to the minimal necessary number of bytes. That byte-length is encoded in the pubdata. Once a key has been used, it can already use the 4 or 5 byte enumeration index and it is very hard to have something cheaper for keys that have been used already. The opportunity comes when remembering the IDs for accounts to spare some bytes on nonce/balance key, but ultimately the complexity may not be worth it.</p>
<p>There is some room for optimization of the keys that are being written for the first time, however, optimizing those is more complex and achieves only a one-time effect (when the key is published for the first time), so they may be in scope of the future upgrades.</p>
<h2 id="values-2"><a class="header" href="#values-2">Values</a></h2>
<p>Values are much easier to compress since they usually contain only zeroes. Also, we can leverage the nature of how those values are changed. For instance, if nonce has been increased only by 1, we do not need to write the entire 32-byte new value, we can just tell that the slot has been <em>increased</em> and then supply only the 1-byte value by which it was increased. This way instead of 32 bytes we need to publish only 2 bytes: first byte to denote which operation has been applied and the second byte to denote the number by which the addition has been made.</p>
<p>We have the following 4 types of changes: <code>Add</code>, <code>Sub</code>, <code>Transform</code>, <code>NoCompression</code> where:</p>
<ul>
<li><code>NoCompression</code> denotes that the whole 32 byte will be provided.</li>
<li><code>Add</code> denotes that the value has been increased. (modulo 2^256)</li>
<li><code>Sub</code> denotes that the value has been decreased. (modulo 2^256)</li>
<li><code>Transform</code> denotes the value just has been changed (i.e. we disregard any potential relation between the previous and the new value, though the new value might be small enough to save up on the number of bytes).</li>
</ul>
<p>Where the byte size of the output can be anywhere from 0 to 31 (also 0 makes sense for <code>Transform</code>, since it denotes that it has been zeroed out). For <code>NoCompression</code> the whole 32 byte value is used.</p>
<p>So the format of the pubdata is the following:</p>
<p><strong>Part 1. Header.</strong></p>
<ul>
<li><code>&lt;version = 1 byte&gt;</code> — this will enable easier automated unpacking in the future. Currently, it will be only equal to <code>1</code>.</li>
<li><code>&lt;total_logs_len = 3 bytes&gt;</code> — we need only 3 bytes to describe the total length of the L2→L1 logs.</li>
<li><code>&lt;the number of bytes used for derived keys = 1 byte&gt;</code>. It should be equal to the minimal required bytes to represent the enum indexes for repeated writes.</li>
</ul>
<p><strong>Part 2. Initial writes.</strong></p>
<ul>
<li><code>&lt;num_of_initial_writes = 2 bytes&gt;</code> - the number of initial writes. Since each initial write publishes at least 32 bytes for key, then <code>2^16 * 32 = 2097152</code> will be enough for a lot of time (right now with the limit of 120kb it will take more than 15 L1 txs to use up all the space there).</li>
<li>Then for each <code>&lt;key, value&gt;</code> pair for each initial write:
<ul>
<li>print key as 32-byte derived key.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<p><strong>Part 3. Repeated writes.</strong></p>
<p>Note that there is no need to write the number of repeated writes, since we know that until the end of the pubdata, all the writes will be repeated ones.</p>
<ul>
<li>For each <code>&lt;key, value&gt;</code> pair for each repeated write:
<ul>
<li>print key as derived key by using the number of bytes provided in the header.</li>
<li>packing type as a 1 byte value, which consists of 5 bits to denote the length of the packing and 3 bits to denote the type of the packing (either <code>Add</code>, <code>Sub</code>, <code>Transform</code> or <code>NoCompression</code>).</li>
<li>The packed value itself.</li>
</ul>
</li>
</ul>
<h2 id="impact-1"><a class="header" href="#impact-1">Impact</a></h2>
<p>This setup allows us to achieve nearly 75% packing for values, and 50% gains overall in terms of the storage logs based on historical data.</p>
<h2 id="encoding-of-packing-type-1"><a class="header" href="#encoding-of-packing-type-1">Encoding of packing type</a></h2>
<p>Since we have <code>32 * 3 + 1</code> ways to pack a state diff, we need at least 7 bits to present the packing type. To make parsing easier, we will use 8 bits, i.e. 1 byte.</p>
<p>We will use the first 5 bits to represent the length of the bytes (from 0 to 31 inclusive) to be used. The other 3 bits will be used to represent the type of the packing: <code>Add</code>, <code>Sub</code> , <code>Transform</code>, <code>NoCompression</code>.</p>
<h2 id="worst-case-scenario-1"><a class="header" href="#worst-case-scenario-1">Worst case scenario</a></h2>
<p>The worst case scenario for such packing is when we have to pack a completely random new value, i.e. it will take us 32 bytes to pack + 1 byte to denote which type it is. However, for such a write the user will anyway pay at least for 32 bytes. Adding an additional byte is roughly 3% increase, which will likely be barely felt by users, most of which use storage slots for balances, etc, which will consume only 7-9 bytes for packed value.</p>
<h2 id="why-do-we-need-to-repeat-the-same-packing-method-id-1"><a class="header" href="#why-do-we-need-to-repeat-the-same-packing-method-id-1">Why do we need to repeat the same packing method id</a></h2>
<p>You might have noticed that for each pair <code>&lt;key, value&gt;</code> to describe value we always first write the packing type and then write the packed value. However, the reader might ask, if it is more efficient to just supply the packing id once and then list all the pairs <code>&lt;key, value&gt;</code> which use such packing.</p>
<p>I.e. instead of listing</p>
<p>(key = 0, type = 1, value = 1), (key = 1, type = 1, value = 3), (key = 2, type = 1, value = 4), …</p>
<p>Just write:</p>
<p>type = 1, (key = 0, value = 1), (key = 1, value = 3), (key = 2, value = 4), …</p>
<p>There are two reasons for it:</p>
<ul>
<li>A minor reason: sometimes it is less efficient in case the packing is used for very few slots (since for correct unpacking we need to provide the number of slots for each packing type).</li>
<li>A fundamental reason: currently enum indices are stored directly in the Merkle tree &amp; have very strict order of incrementing enforced by the circuits and (they are given in order by pairs <code>(address, key)</code>), which are generally not accessible from pubdata.</li>
</ul>
<p>All this means that we are not allowed to change the order of “first writes” above, so indexes for them are directly recoverable from their order, and so we can not permute them. If we were to reorder keys without supplying the new enumeration indices for them, the state would be unrecoverable. Always supplying the new enum index may add additional 5 bytes for each key, which might negate the compression benefits in a lot of cases. Even if the compression will still be beneficial, the added complexity may not be worth it.</p>
<p>That being said, we <em>could</em> rearrange those for <em>repeated</em> writes, but for now we stick to the same value compression format for simplicity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="consensus"><a class="header" href="#consensus">Consensus</a></h1>
<ul>
<li><a href="specs/contracts/consensus/./consensus-registry.html">Consensus Registry</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="consensus-registry"><a class="header" href="#consensus-registry">Consensus Registry</a></h1>
<p>As part of the decentralization effort we plan to introduce two new roles into the system:</p>
<ul>
<li>Validators, which are nodes that are meant to receive L2 blocks from the sequencer, execute them locally and broadcast their signature over the block if it’s valid. If the sequencer receives enough of these signatures, the L2 block is considered finalized. Nodes that are following the chain or syncing will only accept blocks that are finalized.</li>
<li>Attesters, which basically do the same thing as validators but for L1 batches instead of L2 blocks. Just like with L2 blocks, if a L1 batch is accompanied by enough attester signatures then it’s considered finalized. How these signatures are used is different from validators’ signatures though. These signatures are meant to be submitted to L1 together with the L1 batch when it’s committed. And the L1 contracts are meant to only accept L1 batches that come with enough signatures from the correct attesters. But that functionality is not implemented yet.</li>
</ul>
<p>The <code>ConsensusRegistry</code> contract implements a small part of that entire flow. In order to verify the L2 block and L1 batch signatures we need to know the public keys of the validators and attesters that signed them. And we also want that set of validators and attesters to be dynamic. The <code>ConsensusRegistry</code> contract is going to store and manage the current set of validators and attesters and expose methods to add, remove and modify validators/attesters.</p>
<h2 id="users"><a class="header" href="#users">Users</a></h2>
<p>There are basically three types of users that will call this contract:</p>
<ul>
<li>The contract owner. This is generally meant to be some multisig or governance contract. In this case, it will initially be Matter Labs multisig and later it will be changed to be ZKsync’s governance. It can call any method in the contract and basically can modify the validator and attester sets at will. There are methods that are exclusive to it though. Namely add nodes, remove nodes, change validator/attester weights (the relative voting power of each validator/attester) and commit validator/attester committees (this creates a snapshot of the current nodes and updates the committees).</li>
<li>The node owners. The entities that will run the validators and attesters. They change over time as nodes get added/removed. They can only activate/deactivate their nodes (deactivated nodes do not get selected to be part of committees) and change their validator/attester public keys.</li>
<li>The sequencer plus anyone running an external node. They need to verify L1 batch and L2 block signatures so they need to get the attester and validator committees for each batch. There are getter methods for this.</li>
</ul>
<h2 id="future-integration"><a class="header" href="#future-integration">Future integration</a></h2>
<p>Currently the <code>ConsensusRegistry</code> contract is not directly connected to the protocol. The plan is to read the validator committee from the consensus registry contract on each new batch. And, with upcoming protocol upgrades, start verifying the validator signatures onchain in each submitted batch.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intro-to-zksyncs-zk"><a class="header" href="#intro-to-zksyncs-zk">Intro to ZKsync’s ZK</a></h1>
<p>This page is specific to our cryptography. For a general introduction, please read:
<a href="https://docs.zksync.io/build/developer-reference/rollups.html">https://docs.zksync.io/build/developer-reference/rollups.html</a></p>
<p>As a ZK rollup, we want everything to be verified by cryptography and secured by Ethereum. The power of ZK allows for
transaction compression, reducing fees for users while inheriting the same security.</p>
<p>ZK Proofs allow a verifier to easily check whether a prover has done a computation correctly. For ZKsync, the prover
will prove the correct execution of ZKsync’s EVM, and a smart contract on Ethereum will verify the proof is correct.</p>
<p>In more detail, there are several steps.</p>
<ul>
<li>Witness generation: witness generation can be perceived as part of the process where the user (prover) generates proof
of transaction validity. For instance, when a user initiates a transaction, a corresponding witness is generated,
which serves as proof that the transaction is valid and adheres to the network’s consensus rules. The zero-knowledge
aspect ensures that the witness reveals no information about the transaction’s specifics, maintaining user privacy and
data security. New transactions are proved in batches. These batches will be processed and sent to the circuits.</li>
<li>Circuits: Our virtual machine needs to prove that the execution was completed correctly to generate proofs correctly.
This is accomplished using circuits. In order for proofs to work, normal code logic must be transformed into a format
readable by the proof system. The virtual machine reads the code that will be executed and sorts the parts into
various circuits. These circuits then break down the parts of code, which can then be sent to the proof system.</li>
<li>Proof system: We need a proof system to process the ZK circuit. Our proving system is called Boojum.</li>
</ul>
<p>Here are the different repositories we use:</p>
<ul>
<li><strong>Boojum</strong>: Think of this as the toolbox. It holds essential tools and parts like the prover (which helps confirm the
circuit’s functionality), verifier (which double-checks everything), and various other backend components. These are
the technical bits and pieces, like defining Booleans, Nums, and Variables that will be used in the circuits.</li>
<li><strong>zkevm_circuits</strong>: This is where we build and store the actual circuits. The circuits are built from Boojum and
designed to replicate the behavior of the EVM.</li>
<li><strong>zkevm_test_harness</strong>: It’s like our testing ground. Here, we have different tests to ensure our circuits work
correctly. Additionally, it has the necessary code that helps kickstart and run these circuits smoothly.</li>
</ul>
<h3 id="what-is-a-circuit"><a class="header" href="#what-is-a-circuit">What is a circuit</a></h3>
<p>ZK circuits get their name from Arithmetic Circuits, which look like this (see picture). You can read the circuit by
starting at the bottom with the inputs, and following the arrows, computing each operation as you go.</p>
<p><img src="specs/prover/./img/intro_to_zkSync%E2%80%99s_ZK/circuit.png" alt="Untitled" /></p>
<p>The prover will prove that the circuit is “satisfied” by the inputs, meaning every step is computed correctly, leading
to a correct output.</p>
<p>It is very important that every step is actually “constrained”. The prover must be forced to compute the correct values.
If the circuit is missing a constraint, then a malicious prover can create proofs that will pass verification but not be
valid. The ZK terminology for this is that an underconstrained circuit could lead to a soundness error.</p>
<h3 id="what-do-zksyncs-circuits-prove"><a class="header" href="#what-do-zksyncs-circuits-prove">What do ZKsync’s circuits prove</a></h3>
<p>The main goal of our circuits is to prove correct execution of our VM. This includes proving each opcode run within the
VM, as well as other components such as precompiles, storage, and circuits that connect everything else together. This
is described in more detail in
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits.md">Circuits</a></p>
<h3 id="more-details"><a class="header" href="#more-details">More details</a></h3>
<p>The process of turning code into constraints is called arithmetization. Our arithmetization is based on a variation of
“Plonk”. The details are abstracted away from the circuits, but if you’d like to learn more, read about Plonk in
<a href="https://vitalik.eth.limo/general/2019/09/22/plonk.html">Vitalik’s blog</a> or the
<a href="https://github.com/mir-protocol/plonky2/blob/main/plonky2/plonky2.pdf">Plonky2 paper</a>.</p>
<p>More details of our proving system can be found in the <a href="https://eprint.iacr.org/2019/1400.pdf">Redshift Paper</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h1>
<p>Our ZK code is spread across three repositories:</p>
<p><a href="https://github.com/matter-labs/era-boojum/tree/main">Boojum</a> contains the low level ZK details.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_circuits/tree/main">zkevm_circuits</a> contains the code for the circuits.</p>
<p><a href="https://github.com/matter-labs/era-zkevm_test_harness/tree/v1.4.0">zkevm_test_harness</a> contains the tests for the
circuits.</p>
<p>To get started, run the basic_test from the era-zkevm_test_harness:</p>
<pre><code class="language-bash">rustup default nightly-2023-08-23
cargo update
cargo test basic_test  --release -- --nocapture

</code></pre>
<p>This test may take several minutes to run, but you will see lot’s of information along the way!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zk-terminology"><a class="header" href="#zk-terminology">ZK Terminology</a></h1>
<h3 id="arithmetization"><a class="header" href="#arithmetization">Arithmetization</a></h3>
<p>Arithmetization refers to a technique used in zero-knowledge proof systems, where computations are represented in such a
way that they can be efficiently verified by a prover and a verifier. In simpler terms, it is a way to convert
computations into polynomial equations so that they can be used in cryptographic proofs.</p>
<h3 id="builder"><a class="header" href="#builder">Builder</a></h3>
<p>The builder helps set up the constraint system. The builder knows the placement strategy for each gate, as well as the
geometry and other information needed for building the constraint system.</p>
<h3 id="circuit"><a class="header" href="#circuit">Circuit</a></h3>
<p>An arithmetic circuit is a mathematical construct used in cryptography to encode a computational problem. It is
comprised of gates, with each gate performing an arithmetic operation, for example, such as addition or multiplication.
These circuits are essential for encoding statements or computations that a prover wants to prove knowledge of without
revealing the actual information.</p>
<h3 id="constraint"><a class="header" href="#constraint">Constraint</a></h3>
<p>A constraint is a rule or restriction that a specific operation or set of operations must follow. ZKsync uses
constraints to verify the validity of certain operations, and in the generation of proofs. Constraints can be missing,
causing bugs, or there could be too many constraints, leading to restricted operations.</p>
<h3 id="constraint-degree"><a class="header" href="#constraint-degree">Constraint degree</a></h3>
<p>The “constraint degree” of a constraint system refers to the maximum degree of the polynomial gates in the system. In
simpler terms, it’s the highest power of polynomial equations of the constraint system. At ZKsync, we allow gates with
degree 8 or lower.</p>
<h3 id="constraint-system-1"><a class="header" href="#constraint-system-1">Constraint system</a></h3>
<p>Constraint system is a mathematical representation consisting of a set of equations or polynomial constraints that are
constructed to represent the statements being proved. The system is deemed satisfied if there exist specific assignments
to the variables that make all the equations or constraints true. Imagine it as a list of “placeholders” called
Variables. Then we add gates to the variables which enforce a specific constraint. The Witness represents a specific
assignment of values to these Variables, ensuring that the rules still hold true.</p>
<h3 id="geometry"><a class="header" href="#geometry">Geometry</a></h3>
<p>The geometry defines the number of rows and columns in the constraint system. As part of PLONK arithmetization, the
witness data is arranged into a grid, where each row defines a gate (or a few gates), and the columns are as long as
needed to hold all of the witness data. At ZKsync, we have ~164 base witness columns.</p>
<h3 id="log"><a class="header" href="#log">Log</a></h3>
<p>We use the word “log” in the sense of a database log: a log stores a list of changes.</p>
<h3 id="lookup-table"><a class="header" href="#lookup-table">Lookup table</a></h3>
<p>Lookup table is a predefined table used to map input values to corresponding output values efficiently, assisting in
validating certain relations or computations without revealing any extra information about the inputs or the internal
computations. ****Lookup ****tables are particularly useful in ZK systems to optimize and reduce the complexity of
computations and validations, enabling the prover to construct proofs more efficiently and the verifier to validate
relationships or properties with minimal computational effort. For example, if you want to prove a certain number is
between 0 and 2^8, it is common to use a lookup table.</p>
<h3 id="proof"><a class="header" href="#proof">Proof</a></h3>
<p>A proof can refer generally to the entire proving process, or a proof can refer specifically to the data sent from the
prover to the verifier.</p>
<h3 id="prover"><a class="header" href="#prover">Prover</a></h3>
<p>In our ZKsync zk-rollup context, the prover is used to process a set of transactions executing smart contracts in a
succinct and efficient manner. It computes proofs that all the transactions are correct and ensures a valid transition
from one state to another. The proof will be sent to a Verifier smart contract on Ethereum. At ZKsync, we prove state
diffs of a block of transactions, in order to prove the new state root state is valid.</p>
<h3 id="satisfiable"><a class="header" href="#satisfiable">Satisfiable</a></h3>
<p>In the context of ZK, satisfiability refers to whether the witness passes - or “satisfies” - all of the constraints in a
circuit.</p>
<h3 id="state-diffs"><a class="header" href="#state-diffs">State Diffs</a></h3>
<p>State Diffs, or State Differentials, are the differences in accounts before and after processing transactions contained
in a block. For example, if my ETH Balance changes from 5 ETH to 6 ETH, then the state diff for my account is +1 ETH.</p>
<h3 id="variables"><a class="header" href="#variables">Variables</a></h3>
<p>Variables are placeholders in the constraint system until we know the specific witness data. The reason we would want
placeholders is because we might want to fill in which constraints we need, before knowing the actual input data. For
example, we might know we need to add two numbers and constrain the sum, before we know exactly which two numbers will
be in the witness.</p>
<h3 id="verifier"><a class="header" href="#verifier">Verifier</a></h3>
<p>The Verifier is a smart contract on Ethereum. It will receive a proof, check to make sure the proof is valid, and then
update the state root.</p>
<h3 id="witness"><a class="header" href="#witness">Witness</a></h3>
<p>Witness refers to the private, secret information or set of values that the prover possesses and aims to demonstrate
knowledge of, without revealing the actual information to the verifier. The witness is the input to the circuit. When we
have a circuit, the valid “witness” is the input that meets all the constraints and satisfies everything.</p>
<h3 id="worker"><a class="header" href="#worker">Worker</a></h3>
<p>A worker refers to our multi-threaded proving system. Proving may be “worked” in parallel, meaning that we can execute
some operations, like polynomial addition, in parallel threads.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boojum-function-check_if_satisfied"><a class="header" href="#boojum-function-check_if_satisfied">Boojum function: check_if_satisfied</a></h1>
<p>Note: Please read our other documentation and tests first before reading this page.</p>
<p>Our circuits (and tests) depend on a function from Boojum called
<a href="https://github.com/matter-labs/era-boojum/blob/main/src/cs/implementations/satisfiability_test.rs#L11">check_if_satisfied</a>.
You don’t need to understand it to run circuit tests, but it can be informative to learn more about Boojum and our proof
system.</p>
<p>First we prepare the constants, variables, and witness. As a reminder, the constants are just constant numbers, the
variables circuit columns that are under PLONK copy-permutation constraints (so they are close in semantics to variables
in programming languages), and the witness ephemeral values that can be used to prove certain constraints, for example
by providing an inverse if the variable must be non-zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied.png" alt="Check_if_satisfied.png" /></p>
<p>Next we prepare a view. Instead of working with all of the columns at once, it can be helpful to work with only a
subset.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(1).png" alt="Check_if_satisfied(1).png" /></p>
<p>Next we create the paths_mappings. For each gate in the circuit, we create a vector of booleans in the correct shape.
Later, when we traverse the gates with actual inputs, we’ll be able to remember which gates should be satisfied at
particular rows by computing the corresponding selector using constant columns and the paths_mappings.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(2).png" alt="Check_if_satisfied(2).png" /></p>
<p>Now, we have to actually check everything. The checks for the rows depend on whether they are under general purpose
columns, or under special purpose columns.</p>
<p><strong>General purpose rows:</strong></p>
<p>For each row and gate, we need several things.</p>
<ul>
<li>Evaluator for the gate, to compute the result of the gate</li>
<li>Path for the gate from the paths_mappings, to locate the gate</li>
<li>Constants_placement_offset, to find the constants</li>
<li>Num_terms in the evaluator
<ul>
<li>If this is zero, we can skip the row since there is nothing to do</li>
</ul>
</li>
<li>Gate_debug_name</li>
<li>num_constants_used</li>
<li>this_view</li>
<li>placement (described below)</li>
<li>evaluation function</li>
</ul>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(3).png" alt="Check_if_satisfied(3).png" /></p>
<p>Placement is either UniqueOnRow or MultipleOnRow. UniqueOnRow means there is only one gate on the row (typically because
the gate is larger / more complicated). MultipleOnRow means there are multiple gates within the same row (typically
because the gate is smaller). For example, if a gate only needs 30 columns, but we have 150 columns, we could include
five copies fo that gate in the same row.</p>
<p>Next, if the placement is UniqueOnRow, we call evaluate_over_general_purpose_columns. All of the evaluations should be
equal to zero, or we panic.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(4).png" alt="Check_if_satisfied(4).png" /></p>
<p>If the placement is MultipleOnRow, we again call evaluate_over_general_purpose_columns. If any of the evaluations are
non-zero, we log some extra debug information, and then panic.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(7).png" alt="Check_if_satisfied(7).png" /></p>
<p>This concludes evaluating and checking the generalized rows. Now we will check the specialized rows.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(8).png" alt="Check_if_satisfied(8).png" /></p>
<p>We start by initializing vectors for specialized_placement_data, evaluation_functions, views, and evaluator_names. Then,
we iterate over each gate_type_id and evaluator.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(9).png" alt="Check_if_satisfied(9).png" /></p>
<p>If gate_type_id is a LookupFormalGate, we don’t need to do anything in this loop because it is handled by the lookup
table. For all other cases, we need to check the evaluator’s total_quotient_terms_over_all_repetitions is non-zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(11).png" alt="Check_if_satisfied(11).png" /></p>
<p>Next, we get num_terms, num_repetitions, and share_constants, total_terms, initial_offset, per_repetition_offset, and
total_constants_available. All of these together form our placement data.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(12).png" alt="Check_if_satisfied(12).png" /></p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(13).png" alt="Check_if_satisfied(13).png" /></p>
<p>Once we know the placement_data, we can keep it for later, as well as the evaluator for this gate.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(14).png" alt="Check_if_satisfied(14).png" /></p>
<p>We also will keep the view and evaluator name. This is all the data we need from our specialized columns.</p>
<p>To complete the satisfiability test on the special columns, we just need to loop through and check that each of the
evaluations are zero.</p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(16).png" alt="Check_if_satisfied(16).png" /></p>
<p><img src="specs/prover/./img/boojum_function_check_if_satisfied/Check_if_satisfied(17).png" alt="Check_if_satisfied(17).png" /></p>
<p>Now we have checked every value on every row, so the satisfiability test is passed, and we can return true.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boojum-gadgets"><a class="header" href="#boojum-gadgets">Boojum gadgets</a></h1>
<p>Boojum gadgets are low-level implementations of tools for constraint systems. They consist of various types: curves,
hash functions, lookup tables, and different circuit types. These gadgets are mostly a reference from
<a href="https://github.com/matter-labs/franklin-crypto">franklin-crypto</a>, with additional hash functions added. These gadgets
have been changed to use the Goldilocks field (order 2^64 - 2^32 + 1), which is much smaller than bn256. This allows us
to reduce the proof system.</p>
<h2 id="circuits-types"><a class="header" href="#circuits-types">Circuits types</a></h2>
<p>We have next types with we use for circuits:</p>
<p><strong>Num (Number):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Num&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Boolean:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Boolean&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U8:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt8&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U16:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt16&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U32:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt32&lt;F: SmallField&gt; {
    pub(crate) variable: Variable,
    pub(crate) _marker: std::marker::PhantomData&lt;F&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U160:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt160&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 5],
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U256:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt256&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 8],
}
<span class="boring">}</span></code></pre></pre>
<p><strong>U512:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct UInt512&lt;F: SmallField&gt; {
    pub inner: [UInt32&lt;F&gt;; 16],
}
<span class="boring">}</span></code></pre></pre>
<p>Every type consists of a Variable (the number inside Variable is just the index):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Variable(pub(crate) u64);
<span class="boring">}</span></code></pre></pre>
<p>which is represented in the current Field. Variable is quite diverse, and to have “good” alignment and size we manually
do encoding management to be able to represent it as both copyable variable or witness.</p>
<p>The implementation of this circuit type itself is similar. We can also divide them into classes as main and dependent:
Such type like U8-U512 decoding inside functions to Num<F> for using them in logical operations. As mentioned above, the
property of these types is to perform logical operations and allocate witnesses.</p>
<p>Let’s demonstrate this in a Boolean example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;F: SmallField&gt; CSAllocatable&lt;F&gt; for Boolean&lt;F&gt; {
    type Witness = bool;
    fn placeholder_witness() -&gt; Self::Witness {
        false
    }

    #[inline(always)]
    fn allocate_without_value&lt;CS: ConstraintSystem&lt;F&gt;&gt;(cs: &amp;mut CS) -&gt; Self {
        let var = cs.alloc_variable_without_value();

        Self::from_variable_checked(cs, var)
    }

    fn allocate&lt;CS: ConstraintSystem&lt;F&gt;&gt;(cs: &amp;mut CS, witness: Self::Witness) -&gt; Self {
        let var = cs.alloc_single_variable_from_witness(F::from_u64_unchecked(witness as u64));

        Self::from_variable_checked(cs, var)
    }
}
<span class="boring">}</span></code></pre></pre>
<p>As you see, you can allocate both with and without witnesses.</p>
<h2 id="hash-function"><a class="header" href="#hash-function">Hash function</a></h2>
<p>In gadgets we have a lot of hash implementation:</p>
<ul>
<li>blake2s</li>
<li>keccak256</li>
<li>poseidon/poseidon2</li>
<li>sha256</li>
</ul>
<p>Each of them perform different functions in our proof system.</p>
<h2 id="queues"><a class="header" href="#queues">Queues</a></h2>
<p>One of the most important gadgets in our system is queue. It helps us to send data between circuits. Here is the quick
explanation how it works:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Struct CircuitQueue{
 head: HashState,
 tail: HashState,
 length: UInt32,
 witness: VecDeque&lt;Witness&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>The structure consists of <code>head</code> and <code>tail</code> commitments that basically are rolling hashes. Also, it has a <code>length</code> of
the queue. These three fields are allocated inside the constraint system. Also, there is a <code>witness</code>, that keeps actual
values that are now stored in the queue.</p>
<p>And here is the main functions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn push(&amp;mut self, value: Element) {
 // increment length
 // head - hash(head, value)
 // witness.push_back(value.witness)
}

fn pop(&amp;mut self) -&gt; Element {
 // check length &gt; 0
 // decrement length
 // value = witness.pop_front()
 // tail = hash(tail, value)
 // return value
}

fn final_check(&amp;self) -&gt; Element {
 // check that length == 0
 // check that head == tail
}
<span class="boring">}</span></code></pre></pre>
<p>So the key point, of how the queue proofs that popped elements are the same as pushed ones, is equality of rolling
hashes that stored in fields <code>head</code> and <code>tail</code>.</p>
<p>Also, we check that we can’t pop an element before it was pushed. This is done by checking that <code>length &gt;= 0</code>.</p>
<p>Very important is making the <code>final_check</code> that basically checks the equality of two hashes. So if the queue is never
empty, and we haven’t checked the equality of <code>head</code> and <code>tail</code> in the end, we also haven’t proven that the elements we
popped are correct.</p>
<p>For now, we use poseidon2 hash. Here is the link to queue implementations:</p>
<ul>
<li><a href="https://github.com/matter-labs/era-boojum/blob/main/src/gadgets/queue/mod.rs#L29">CircuitQueue</a></li>
<li><a href="https://github.com/matter-labs/era-boojum/blob/main/src/gadgets/queue/full_state_queue.rs#L20C12-L20C33">FullStateCircuitQueue</a></li>
</ul>
<p>The difference is that we actually compute and store a hash inside CircuitQueue during <code>push</code> and <code>pop</code> operations. But
in FullStateCircuitQueue our <code>head</code> and <code>tail</code> are just states of sponges. So instead of computing a full hash, we just
absorb a pushed (popped) element.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="circuit-testing"><a class="header" href="#circuit-testing">Circuit testing</a></h1>
<p>This page explains unit tests for circuits. Specifically, it goes through a unit test of
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/main/src/ecrecover/mod.rs#L796">ecrecover</a>. The tests for other
circuits are very similar.</p>
<p>Many of the tests for different circuits are nearly identical, for example:</p>
<ul>
<li>test_signature_for_address_verification (ecrecover)</li>
<li>test_code_unpacker_inner</li>
<li>test_demultiplex_storage_logs_inner</li>
<li>and several others.</li>
</ul>
<p>If you understand one, you will quickly be able to understand them all.</p>
<p>Let’s focus on ecrecover. Ecrecover is a precompile that, given your signature, can compute your address. If our circuit
works correctly, we should be able to recover the proper address, and be able to prove the computation was done
correctly.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(4).png" alt="Contest(4).png" /></p>
<p>The test begins by defining the geometry, max_variables, and max_trace_len. This data will be used to create the
constraint system. Next, we define a helper function:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(5).png" alt="Contest(5).png" /></p>
<p>To help run the test, we have a helper function called configure that returns a builder. The builder knows all of the
gates and gate placement strategy, which will be useful for setting up the constraint system.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(6).png" alt="Contest(6).png" /></p>
<p>The constraint system is almost ready! We still need to add the lookup tables for common boolean functions:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(7).png" alt="Contest(7).png" /></p>
<p>Now the constraint system is ready! We can start the main part of the test!</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(8).png" alt="Contest(8).png" /></p>
<p>Here we have hard coded a secret key with its associated public key, and generate a signature. We will test our circuit
on these inputs! Next we “allocate” these inputs as witnessess:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(9).png" alt="Contest(9).png" /></p>
<p>We have to use special integer types because we are working in a finite field.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(10).png" alt="Contest(10).png" /></p>
<p>The constants here are specific to the curve used, and are described in detail by code comments in the
ecrecover_precompile_inner_routine.</p>
<p>Finally we can call the ecrecover_precompile_inner_routine:</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(11).png" alt="Contest(11).png" /></p>
<p>Lastly, we need to check to make sure that 1) we recovered the correct address, and 2) the constraint system can be
satisfied, meaning the proof works.</p>
<p><img src="specs/prover/./img/circuit_testing/Contest(12).png" alt="Contest(12).png" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="circuits-2"><a class="header" href="#circuits-2">Circuits</a></h1>
<h2 id="general-description"><a class="header" href="#general-description">General description</a></h2>
<p>The main circuit is called <code>MainVM</code>. It is the one where all the main logic happens.</p>
<p>It consists of multiple cycles, where on each iteration we take a next opcode and try to execute it the following way:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if opcode == Add {
 // do addition
}
if opcode == SRead {
 // do storage read
}
...
<span class="boring">}</span></code></pre></pre>
<p>You may notice that <code>Add</code> instruction is much simpler than the <code>SRead</code> one. When you work with circuits you still need
to execute every opcode.</p>
<p>That’s why we can use the following approach:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if opcode == Add {
 // do addition
}
if opcode == SRead {
 storage_queue.push((address, value));
 // proof storage read in other circuit
}
...
<span class="boring">}</span></code></pre></pre>
<p>So instead of proving <code>SRead</code> we just push a proving request, that will be sent to another circuit, that will prove it.
That’s how we can make our prover structure more optimized and flexible.</p>
<p>For now, we have 13 base layer circuits:</p>
<ul>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Main%20Vm.md">MainVM</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/SortDecommitments.md">CodeDecommitmentsSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/CodeDecommitter.md">CodeDecommitter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/DemuxLogQueue.md">LogDemuxer</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/KeccakRoundFunction.md">KeccakRoundFunction</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sha256RoundFunction.md">Sha256RoundFunction</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Ecrecover.md">ECRecover</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/RAMPermutation.md">RAMPermutation</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageSorter.md">StorageSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageApplication.md">StorageApplication</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">EventsSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">L1MessagesSorter</a></p>
</li>
<li>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/L1MessagesHasher.md">L1MessagesHasher</a></p>
</li>
<li></li>
</ul>
<p>They mostly communicate by queues (the diagram of communication is below).</p>
<h2 id="public-input-structure"><a class="header" href="#public-input-structure">Public Input structure</a></h2>
<p>Public Input (PI) is some piece of data, that is revealed to the verifier. Usually, it consists of some inputs and
outputs.</p>
<p>The main challenge for base layer circuits is the ability to prove unlimited amount of execution. For example, our
<code>MainVm</code> circuit can handle execution of $x$ opcodes. Then, if some transaction causes execution of more than $x$
opcodes, we won’t be able to prove it. That’s why every circuit could be extended to multiple instances. So you can
always use $n$ <code>MainVm</code> instances to handle up to $nx$ opcode executions.</p>
<p>All circuits have the following PI structure:</p>
<p><img src="specs/prover/circuits/./img/diagram.png" alt="diagram.png" /></p>
<div class="table-wrapper"><table><thead><tr><th>start flag</th><th>Boolean that shows if this is the first instance of corresponding circuit type</th></tr></thead><tbody>
<tr><td>finished flag</td><td>Boolean that shows if this is the last instance of corresponding circuit type</td></tr>
<tr><td>Input</td><td>Structure that contains all inputs to this type of circuit (every instance of one circuit type has the same input)</td></tr>
<tr><td>FSM Input and FSM Output</td><td>The field has the same structure. It represents the inner state of circuit execution (the first fsm_input is empty, the second fsm_input equals the first fsm_output and so on…)</td></tr>
<tr><td>Output</td><td>Structure that contains all outputs of this type of circuit (the last instance contains the real output, the output field of the others is empty)</td></tr>
</tbody></table>
</div>
<p>The code implementation can be found
<a href="https://github.com/matter-labs/era-zkevm_circuits/blob/main/src/fsm_input_output/mod.rs#L32">here</a>.</p>
<p>In terms of Arithmetization we don’t allocate all these fields like public input variables. A more efficient approach
would be computing commitment of type <code>[Num&lt;F&gt;; 4]</code> with poseidon2 and then allocating these 4 variables as public
inputs.</p>
<p><img src="specs/prover/circuits/./img/image.png" alt="image.png" /></p>
<p>The equality of corresponding parts in different circuits is done during aggregating base layer circuits. Aggregating is
done by recursion level circuits that also verify base layer proofs. For now this is out of our scope, so we will focus
only on base layer.</p>
<h2 id="how-do-all-the-base-layer-circuits-fit-together"><a class="header" href="#how-do-all-the-base-layer-circuits-fit-together">How do all the base layer circuits fit together</a></h2>
<p><img src="specs/prover/circuits/./img/flowchart.png" alt="flowchart.png" /></p>
<h2 id="all-base-layer-circuits-inner-parts"><a class="header" href="#all-base-layer-circuits-inner-parts">All base layer circuits inner parts</a></h2>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Main%20Vm.md">Main Vm</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/SortDecommitments.md">SortDecommitments</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/CodeDecommitter.md">CodeDecommitter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/DemuxLogQueue.md">DemuxLogQueue</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/KeccakRoundFunction.md">KeccakRoundFunction</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sha256RoundFunction.md">Sha256RoundFunction</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Ecrecover.md">Ecrecover</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/RAMPermutation.md">RAMPermutation</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageSorter.md">StorageSorter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/StorageApplication.md">StorageApplication</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/LogSorter.md">LogSorter</a></p>
<p><a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/L1MessagesHasher.md">L1MessagesHasher</a></p>
<p>There are a couple of circuits that do queue sorting. Here is the page that describes the algorithm:
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/Circuits%20Section/Circuits/Sorting.md">Sorting</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="era-vm"><a class="header" href="#era-vm">Era VM</a></h1>
<ul>
<li><a href="specs/era_vm_specification/./zkSync_era_virtual_machine_primer.html">VM primer</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-virtual-machine-primer"><a class="header" href="#zksync-virtual-machine-primer">ZkSync Virtual Machine primer</a></h1>
<p>Unlike EVM, zkEVM is a register machine. EVM instructions operate on a stack. Instead, zkEVM operates primarily on
sixteen registers and memory like most modern computers. That simplifies zero-knowledge proofs, which largely rely on
building arithmetic circuits.</p>
<p>This document describes zkEVM assembly language, then the aspects of VM related to smart-contracts. Its purpose is not
to be a complete reference, but to guide you through the main ideas.</p>
<h2 id="vm-architecture"><a class="header" href="#vm-architecture">VM architecture</a></h2>
<p>The native type for zkEVM is a 256-bits wide unsigned integer, we call it a <em>word</em>.</p>
<p>Contracts are sequences of instructions. To support the execution of contracts, VM provides the following transient
state:</p>
<ul>
<li><strong>registers</strong>: 16 general-purpose registers: <code>r0</code>, <code>r1</code>, …, <code>r15</code>.<br />
<code>r0</code> is a special constant register: reading it yields 0, storing to it is ignored.</li>
<li><strong>flags</strong>: three distinct boolean registers LT (less-than), EQ (equals, the result is zero) and GT (greater-than).
Instructions may set or clear flags depending on computation results.</li>
<li><strong>data</strong> <strong>stack</strong>: holds $2^{16}$ words, is free to use.</li>
<li><strong>heap</strong>: for data that we want to pass around between functions and contracts. Heap is bounded, accesses are only
free inside the bound, and we have to pay for growing the bound.</li>
<li><strong>code memory</strong>: stores code of currently running contracts. May also be used as a constant pool.</li>
</ul>
<p>VM is aware of two data types:</p>
<ul>
<li>raw integers</li>
<li>pointers (to fragments of other contracts’ heaps).</li>
</ul>
<p>Registers and data stack are tagged: VM keeps track of whether they hold pointers or raw integer values. Some
instructions will only accept operands tagged as pointers.</p>
<p>Heap and storage are not tagged, so if we store a pointer to the heap, its tag is lost.</p>
<p>Contracts have key-value storages, where keys and values are untagged 256-bit integers. Instructions can change
persistent contract storage.</p>
<p>VM is capable of both near calls (to the code within the same contract) and far calls (to other contracts).</p>
<p>Let us now gradually introduce the VM functionality guided by the instruction set.</p>
<h2 id="basic-instructions"><a class="header" href="#basic-instructions">Basic instructions</a></h2>
<p>Contract code consists of instructions, they are executed sequentially.</p>
<p>Instructions usually operate with registers. For example, an instruction <code>add</code> may look like that:</p>
<pre><code class="language-nasm">; this is a comment
add 5, r2, r8  ; store (5 + r2) to r8
</code></pre>
<p>Or like that:</p>
<pre><code class="language-nasm">add 5, r0, r8  ; store (5 + 0) to r8
</code></pre>
<p>Notice that register <code>r0</code> is used to feed constant zero values to instructions; this allows to use <code>add X, r0, Y</code> to
copy a value <code>X</code> to <code>Y</code> .</p>
<p>Commonly, instructions accept two inputs and one output operands, following the schema:</p>
<p><img src="specs/era_vm_specification/./img/arithmetic_opcode.png" alt="arithmetic opcode.png" /></p>
<p>The first operand can be taken from:</p>
<ul>
<li>registers</li>
<li>an immediate 16-bit value, like in the example above <code>add 5, r2, r8</code>. To use bigger numbers put them as constants in
the code memory, see section <strong>Code Addressing</strong>.</li>
<li>directly from the code memory</li>
<li>stack in various ways, e.g. <code>add stack=[2], r2, r8</code> takes the first element from the stack memory area, by an absolute
address 2.</li>
<li>code memory</li>
</ul>
<p>Only registers can be the source of the second operand.</p>
<pre><code class="language-nasm">add r0, 5, r8  ; error: 5 is an immediate value,
               ; but only register is allowed as second operand
</code></pre>
<p>There is usually at most one output operand. Similarly, the first output operand can be stored to registers or stack. If
there is a second output operand, it can only be stored to a register.</p>
<p>Instructions are executed one after another, and every instruction has a gas cost measured in <em>gas</em>. A program that runs
out of gas panics and none of its side effects are performed.</p>
<p>Every contract may have at most $2^{16}$ instructions.</p>
<h3 id="arithmetic-instructions"><a class="header" href="#arithmetic-instructions">Arithmetic instructions</a></h3>
<p>Besides <code>add</code>, zkEVM implements <code>sub</code> for subtraction, <code>and</code>/ <code>or</code> / <code>xor</code> for bitwise logics, <code>shl</code>/ <code>shr</code> for logical
shifts, <code>rol</code>/ <code>ror</code> for circular shifts. These instructions follow the same format, e.g.:</p>
<pre><code class="language-nasm">shl r1, r4, r3 ; right shift r1 by value of r4, store result in r3
</code></pre>
<p>Instructions <code>mul</code> and <code>div</code> are particular: they have two output operands:</p>
<ul>
<li><code>mul r1, r2, r3, r4</code> stores the low 256 bits of r1<em>r2 in r3, high 256 bits of r1</em>r2 in r4</li>
<li><code>div r1, r2, r3, r4</code> stores the quotient in <code>r3</code> and remainder in <code>r4</code>.</li>
</ul>
<h3 id="modifiers"><a class="header" href="#modifiers">Modifiers</a></h3>
<p>Most instructions support modifiers that alter their behaviour. The modifiers are appended to the name of the
instruction, separated by a dot e.g. <code>sub.s</code> . Three basic modifier types are: <code>set_flags</code> , predicates, and <code>swap</code>.</p>
<h4 id="set-flags"><a class="header" href="#set-flags">Set flags</a></h4>
<p>By default, most instructions preserve flags.</p>
<pre><code class="language-nasm">sub r1, r2, r3 ; r3 &lt;- (r1 - r2), no flags are affected
</code></pre>
<p>The instruction <code>sub</code> is implemented so that it sets <code>EQ</code> if the result is zero (that is, if <code>r1</code> == <code>r2</code>). But in this
case, even if <code>r1-r2</code> is zero, the EQ flag is not set, because we did not allow it explicitly. We allow instruction to
set flags by appending a “set flags” modifier to them, like that:</p>
<pre><code class="language-nasm">sub! r1, r2, r3 ; r3 &lt;- (r1 - r2); EQ = 1
</code></pre>
<p>You can learn more in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/EraVM%20Formal%20specification.pdf">formal specification</a>.</p>
<h4 id="predicates"><a class="header" href="#predicates">Predicates</a></h4>
<p>Another type of modifiers allows transforming any instruction into a <em>predicated</em>, conditional instruction. Predicated
instructions are only executed if flags satisfy their condition.</p>
<p>Recall the three flags: LT, EQ and GT.</p>
<p>For example, this <code>sub</code> instruction is only executed if EQ is set:</p>
<pre><code class="language-nasm">sub.if_eq r1, r2, r5
</code></pre>
<p>Here is how we can execute <code>jump</code> to a label <code>.label_if_equals</code> only if <code>r1 == r2</code> :</p>
<pre><code class="language-nasm">sub! r1, r2, r3 ; r3 &lt;- (r1 - r2); EQ = 1 if r1 == r2
jump.if_eq .label_if_equals
</code></pre>
<p>If the condition is not satisfied, we skip the instruction, but still pay its basic cost in gas.</p>
<p>Here is a full list of available predicates:</p>
<ul>
<li><code>if_gt</code></li>
<li><code>if_eq</code></li>
<li><code>if_lt</code></li>
<li><code>if_ge</code> (short for “GT or EQ”)</li>
<li><code>if_le</code> (short for “LT or EQ”)</li>
<li><code>if_not_eq</code></li>
<li><code>if_gt_or_eq</code></li>
</ul>
<p>You can learn more in the
<a href="https://github.com/code-423n4/2023-10-zksync/blob/main/docs/VM%20Section/EraVM%20Formal%20specification.pdf">formal specification</a>.</p>
<h4 id="swap"><a class="header" href="#swap">Swap</a></h4>
<p>Recall that instructions may only accept data from stack as their first operand. What if we need the second operand from
stack? For commutative operation, like <code>add</code> , <code>mul</code>, or <code>and</code>, the order of operands does not matter and we can just
write <code>add x,y,z</code> instead of <code>add y,x,z</code>. However, for operations like <code>sub</code> or <code>div</code> we implement a special “swap”
modifier which exchanges the operand values before executing the instruction. This is useful to work around the
restriction that the second source operand has to be a register.</p>
<p>For example:</p>
<pre><code class="language-nasm">sub   r1, r2, r3 ; r3 &lt;- r1 - r2
sub.s r1, r2, r3 ; r3 &lt;- r2 - r1

</code></pre>
<p>Finally, here is an example of an instruction adorned with all possible modifiers:</p>
<pre><code class="language-nasm">sub.s.if_lt! r8, r4, r12
</code></pre>
<p>Here is a breakdown of modifiers:</p>
<ul>
<li><code>.if_lt</code> : is only executed if the LT flag is set</li>
<li><code>.s</code> : computes <code>r4 - r8</code> instead of <code>r8 - r4</code></li>
<li><code>!</code> : sets flags</li>
</ul>
<p>$$
\begin{aligned}
LT &amp;\leftarrow r_4 &lt; r_8 \
EQ &amp;\leftarrow r_4 - r_8 = 0 \
GT &amp;\leftarrow r_4 &gt; r_8
\end{aligned}
$$</p>
<p>Other modifiers are instruction-specific. They are described in full in the instruction reference.</p>
<h2 id="calls-and-returns"><a class="header" href="#calls-and-returns">Calls and returns</a></h2>
<p>The <code>jump</code> instruction allows to continue execution from a different place, but it does not allow to return back. An
alternative is using calls; zkEVM supports calling code inside the contract itself (near calls) as well as calling other
contracts (far calls).</p>
<h3 id="far-calls"><a class="header" href="#far-calls">Far calls</a></h3>
<p>Far calls are the equivalent of calls in EVM.</p>
<p>Each call gets its own stack, heap, code memories, and allocated gas.</p>
<p>It is impossible to allocate more than 63/64 of the currently available gas to a far call.</p>
<p>Calls can revert or panic (on executing an illegal instruction for example), which undoes all the changes to storage and
events emitted during the call, and burns all remaining gas allocated to this call.</p>
<p>Suppose we far called a contract $C$. After the execution of $C$, the register <code>r1</code> holds a pointer to the return value,
allowing a read-only access to a fragment of $C$’s heap. Alternatively, <code>r1</code> can hold a pointer to the heap of some
other contract that $C$ called internally. More on that in Pointers section.</p>
<p><strong>Delegate calls.</strong> Beside normal <code>far_call</code>, there is a variant <code>far_call.delegate</code>. Delegate calls are a variation of
far calls allowing to call a contract with the current storage space.</p>
<p>For example, suppose we have contracts A,B,C. Contract A calls B normally, then B delegates to C. Then C’s code is
executed in a context of B’s storage, as if contract A called contract C. If C returns normally, the execution will
proceed from the next instruction of B after delegate call. In case of <code>revert</code> or <code>panic</code> in C, all the usual rules
apply.</p>
<p><strong>Mimic calls.</strong> The last variant of far calls is <code>far_call.mimic</code>; it is inaccessible to users and only allowed in
system contracts.</p>
<p>Any of far call variants can be additionally marked as <code>.static</code> to call a contract in static mode — see section
<strong>Static Mode</strong>.</p>
<h3 id="return-revert-panic"><a class="header" href="#return-revert-panic">Return, revert, panic</a></h3>
<p>There are three types of situations where control returns to the caller:</p>
<ul>
<li>Return: a normal way of returning to the caller when no errors occurred. The instruction is <code>ret</code>.</li>
<li>Revert: a recoverable error happened. Unspent gas is returned to the caller, which will execute the exception handler.
The instruction is <code>revert</code>.</li>
<li>Panic: an irrecoverable error happened. Same as revert, but unspent gas is burned. The instruction is <code>ret.panic</code>.</li>
</ul>
<h3 id="near-calls"><a class="header" href="#near-calls">Near calls</a></h3>
<p>Instruction <code>near_call reg, address</code> passes the control to a different address inside the same contract, like <code>jump</code>.
Additionally, it remembers the context of execution in a special <em>call stack</em> (it is different from data stack and not
accessible to assembly programmers).</p>
<p>Here is an example of calling function <code>f</code> .</p>
<pre><code class="language-nasm">.text

; here will be the code of exception handler
eh:

; caller function
main:
near_call r2, @f, @eh ; refer to labels in code using '@' symbol

; callee function
f:
ret

</code></pre>
<p>Additional two arguments:</p>
<ul>
<li>label <code>@eh</code> is the address of exception handler. Functions, like contracts, may revert or panic, which leads to the
execution of the exception handler.</li>
<li>register <code>r2</code> holds how much gas we allocate to the function.</li>
</ul>
<p>As we see, zkEVM supports allocating ergs not only for far calls, but also for near calls. Passing zero will allocate
all available gas. Unlike in far calls, near calls do not limit the amount of gas passed to 63/64 of available gas.</p>
<ul>
<li>On revert, unspent gas of the function is <strong>returned</strong></li>
<li>On panic, unspent gas of the function is <strong>lost</strong></li>
</ul>
<p>All near calls inside the contract are sharing the same memory space (heap, stack), and do not roll back the changes to
this memory if they fail. They do, however, roll back the changes to storage and events.</p>
<p>Near calls cannot be used from Solidity to their full extent. Compiler generates them, but makes sure that if functions
revert or panic, the whole contract reverts of panics. Explicit exception handlers and allocating just a portion of
available gas are reserved for low-level code.</p>
<h2 id="accessing-data-outside-registers"><a class="header" href="#accessing-data-outside-registers">Accessing data outside registers</a></h2>
<h3 id="stack-addressing"><a class="header" href="#stack-addressing">Stack addressing</a></h3>
<p>As we already know, instructions may accept data not only in registers or as immediate 16-bit values, but also on stack.</p>
<p>Data stack is a collection of $2^{16}$ words with a pointer SP. This pointer contains the next address after the topmost
stack element, so the topmost element has the address SP-1. Stack grows towards maximal address, i.e. pushing an element
to stack increases SP.</p>
<p>On far call, SP starts in a new stack memory at 1024.</p>
<h4 id="reading-from-stack"><a class="header" href="#reading-from-stack">Reading from stack</a></h4>
<p>There are several ways of accessing stack cells:</p>
<pre><code class="language-nasm">.text
main:

; r0 &lt;- stack word by absolute index (r1+42), unrelated to SP
add stack=[r1+42], r0, r2

; r0 &lt;- stack word by index (SP - (r1 + 42))
add stack[r1+42], r0, r2

; r2 &lt;- stack word by index (SP - (r1 + 42)); additionally, SP += (r1+42)
add stack-=[r1+42], r0, r2
</code></pre>
<p>As we see there are three stack address modes for input operands; all of them use (register + offset).</p>
<p>Currently, the last mode is only used in a <code>nop</code> instruction as a way to rewind stack:</p>
<pre><code class="language-nasm">; effectively, SP -= reg+imm
nop stack-=[reg+imm]
</code></pre>
<h4 id="writing-to-stack"><a class="header" href="#writing-to-stack">Writing to stack</a></h4>
<p>Storing results on stack is also possible:</p>
<pre><code class="language-nasm">.text
main:

; r1 -&gt; word by absolute index (r2 + 42)
add r1, r0, stack=[r2 + 42]

; r1 -&gt;  word by absolute index SP - (r2 + 42)
add r1, r0, stack[r2 + 42]

; r1 -&gt;  word by absolute index SP + (r2 + 42)
; additionally, SP += r2 + 42
add r1, r0, stack+=[r2 + 42]
</code></pre>
<p>Currently, the last mode is only used in a <code>nop</code> instruction as a way to forward stack pointer:</p>
<pre><code class="language-nasm">; effectively, SP += reg+imm
nop r0, r0, stack+=[reg+imm]
</code></pre>
<h3 id="code-addressing"><a class="header" href="#code-addressing">Code addressing</a></h3>
<p>Sometimes we might need to work with larger immediates that do not fit into 16-bit. In this case we can use the
(read-only) code memory as a constant pool and read 256-bit constants from there.</p>
<pre><code class="language-nasm">.rodata

datavar:
 .cell 42
 .cell 999
.text
somelabel:

; r2 &lt;- word by index (r0+0)  code memory
add @datavar[0], r0, r2
add @datavar[r2], r0, r2
</code></pre>
<p>Note: instructions are 64-bit wide, but when accessing data in code memory, this memory is treated as word-addressable.
Therefore, e.g. reading the 0-th 256-bit word from this memory will yield a binary representation of the four first
64-bit instructions in the contract.</p>
<p>There is no distinction between static data and code: code can be read, data can be executed, but instructions that are
not correctly encoded will trigger panic.</p>
<p>Contracts always need to be divisible by 32 bytes (4 instructions) because of this addressing mode.</p>
<h3 id="using-heap"><a class="header" href="#using-heap">Using heap</a></h3>
<p>Heap is a bounded memory region to store data between near calls, and to communicate data between contracts.</p>
<h4 id="heap-boundary-growth"><a class="header" href="#heap-boundary-growth">Heap boundary growth</a></h4>
<p>Accessing an address beyond the heap bound leads to heap growth: the bound is adjusted to accommodate this address. The
difference between old and new bounds is paid in gas.</p>
<h4 id="instructions-to-access-heap"><a class="header" href="#instructions-to-access-heap">Instructions to access heap</a></h4>
<p>Most instructions can not use heap directly. Instructions <code>ld.1</code> and <code>st.1</code> are used to load and store data on heap:</p>
<pre><code class="language-nasm">; take a 32-bit number from r1, use it as an offset in heap,
; load the word from heap by this offset to r4
ld.1 r1, r4

; take a 32-bit number from r3, use it as an offset in heap,
; store the word from r5 to heap by this offset
st.1 r3, r5
</code></pre>
<p>Heap is byte-addressable, but reads and writes operate in words. To read two consecutive words in heap starting at an
address A, first, read from A, and then read from A+32. Reading any addresses in between is valid too.</p>
<p>One of the modifiers allows to immediately form a new offset like that:</p>
<pre><code class="language-nasm">; same as ld, but additionally r5 &lt;- r1 + 32
ld.1.inc r1, r4, r5
</code></pre>
<p>This allows reading several consecutive words in a row:</p>
<pre><code class="language-nasm">; reads four consecutive words from heap starting at address in r8
; into registers r1, r2, r3, r4
ld.1.inc r8, r1, r8
ld.1.inc r8, r2, r8
ld.1.inc r8, r3, r8
ld.1.inc r8, r4, r8
</code></pre>
<p>In theory, heap can hold nearly $2^{32}$ bytes, but growing a heap so large is not affordable: the maximum gas allocated
is $2^{32}-1$.</p>
<p>The topmost 32 bytes of heap are considered forbidden addresses, trying to access them results in panic no matter how
much gas is available.</p>
<h4 id="heap-and-auxheap"><a class="header" href="#heap-and-auxheap">Heap and Auxheap</a></h4>
<p>In zkEVM, there are two heaps; every far call allocates memory for both of them.</p>
<p>Heaps are selected with modifiers <code>.1</code> or <code>.2</code> :</p>
<ul>
<li><code>ld.1</code> reads from heap;</li>
<li><code>ld.2</code> reads from auxheap.</li>
</ul>
<p>The reason why we need two heaps is technical. Heap contains calldata and returndata for calls to user contracts, while
auxheap contains calldata and returndata for calls to system contracts. This ensures better compatibility with EVM as
users should be able to call zkEVM-specific system contracts without them affecting calldata or returndata.</p>
<h2 id="fat-pointers"><a class="header" href="#fat-pointers">Fat pointers</a></h2>
<p>A fat pointer is the second type of values in zkEVM, beside raw integers.</p>
<p>As we noted, registers and stacks are internally tagged by VM to keep track of the cells containing pointers in their
low 128 bits. Only cells with a set pointer tag are considered fat pointers.</p>
<p>Fat pointers are used to pass read-only data between contracts. When choosing how to pass data to a contract (whether
when calling or returning from a call) we have a choice:</p>
<ul>
<li>pass an existing fat pointer, or</li>
<li>create a new fat pointer from a fragment of heap/auxheap.</li>
</ul>
<p>Fat pointers combine two aspects:</p>
<ul>
<li>Delimit a fragment accessible to other contract. Accesses outside this fragment through a pointer yield zero.</li>
<li>Provide an offset inside this fragment. This offset can be increased or decreased.</li>
</ul>
<p>The restrictions on fat pointers provide allows to pass data between contracts safely and without excessive copying.</p>
<p><strong>Implementation note.</strong> Internally, fat pointers hold four 32-bit values:</p>
<ul>
<li>bits 0..31 : offset</li>
<li>bits 32..63: internal memory page ID</li>
<li>bits 64…95 : starting address of the fragment</li>
<li>bits 96…127 : length of the fragment</li>
</ul>
<h4 id="instructions-to-manipulate-fat-pointers"><a class="header" href="#instructions-to-manipulate-fat-pointers">Instructions to manipulate fat pointers</a></h4>
<p>Only special instructions can manipulate fat pointers without automatically clearing its pointer tag.</p>
<ul>
<li><code>ptr.add</code>, <code>ptr.sub</code> modify the offset inside pointer</li>
<li><code>ptr.shrink</code> reduces the associates fragment, so if we get a fat pointer from contract A, we can then shrink it and
pass to another contract B up the call chain, again without copying data.</li>
<li><code>ptr.pack</code> allows putting data in the top 128 bit of the pointer value without clearing the pointer tag.</li>
</ul>
<p>Doing e.g. <code>add r1, 0, r2</code> on a pointer in <code>r1</code> clears its tag, and it is now considered as a raw integer.</p>
<p>Instructions <code>ld</code> and <code>[ld.inc](http://ld.inc)</code> (without indices 1 or 2) allow loading data by fat pointers, possibly
incrementing the pointer. It is impossible to write by a fat pointer.</p>
<h2 id="contracts-and-storage"><a class="header" href="#contracts-and-storage">Contracts and storage</a></h2>
<p>All accounts are associated with contracts. There are $2^{160}$ valid account addresses.</p>
<p>In zkEVM, contracts may have multiple <strong>functions</strong> in them; a contract may execute its functions by using <code>near_call</code> ;
it may call other contracts by using <code>far_call</code> or its variations <code>delegate_call</code> / <code>mimic_call</code> (mimic is reserved for
system contracts).</p>
<p>Size of a contract should be divisible by 32 bytes (4 instructions).</p>
<h3 id="storage-of-contracts"><a class="header" href="#storage-of-contracts">Storage of contracts</a></h3>
<p>Every account has a storage. Storage maps $2^{256}$ keys to values; both keys and values are 256-bit untagged words.</p>
<p>Contracts may write to their own storage by using <code>sstore key, value</code> and read from storage using <code>sload key, dest</code>.</p>
<h3 id="static-mode"><a class="header" href="#static-mode">Static mode</a></h3>
<p>Static mode prevents contracts from modifying their storage and emitting events. In static mode, executing an
instruction like <code>sstore</code> sends VM into panic.</p>
<p>To execute a contract C in static mode, use a <code>static</code> modifier: <code>far_call.static</code>. All contracts, called by C
recursively, will also be executed in static mode. VM exits static mode automatically when C terminates.</p>
<h3 id="system-contracts"><a class="header" href="#system-contracts">System contracts</a></h3>
<p>Part of Era’s functionality is implemented through system contracts. These contracts have addresses from 0 to $2^{64}$
and are executed in kernel mode, where they have access to privileged instructions. An example of such instruction is
mimic call, a variant of far call where the caller can pretend to be another contract. This is useful for hiding the
fact that something is implemented via a system contract but in the hands of users it would mean being able to steal
anyone’s tokens.</p>
<p>System contracts implement contract deployment, extensions such as keccak256, decommitting code etc.</p>
<h2 id="server-and-vm-environment"><a class="header" href="#server-and-vm-environment">Server and VM environment</a></h2>
<h3 id="decommitter"><a class="header" href="#decommitter">Decommitter</a></h3>
<p>Decommitter is a module external to zkEVM allowing accessing deployed code by its hash.</p>
<p><img src="specs/era_vm_specification/./img/arch-overview.png" alt="arch-overview.png" /></p>
<p>The system contracts at the address $2^{15}+2$ , called Deployer, keeps hashes of code of each contract in its storage.
Far calls to a contract with address $C$ perform as follows:</p>
<ul>
<li>VM internally accesses the storage of <code>Deployer</code> contract by key $C$. This storage yields the hash value $H$<strong>.</strong></li>
<li>then VM queries the decommitter, providing $H$. Decommitter answers with the contract code.</li>
</ul>
<p>If decommitter does not have the code for the requested hash, one of two things happen:</p>
<ul>
<li>if C is a system contract (i.e. address of $C &lt; 2^{16}$), the call will fail</li>
<li>otherwise, VM will call the <code>DefaultAccount</code> contract.</li>
</ul>
<h3 id="server-2"><a class="header" href="#server-2">Server</a></h3>
<p>The VM is controlled by a <em>server.</em> When the server needs to build a new batch, it starts an instance of zkEVM and feeds
the transactions to the <a href="specs/era_vm_specification/zkSync_era_virtual_machine_primer.html#bootloader">Bootloader</a>.</p>
<p>zkEVM accepts three parameters:</p>
<ol>
<li>Bootloader’s hash. It is used to fetch the bootloader code from decommitter.</li>
<li>Code hash of <code>DefaultAccount</code> contract code. It is used to fetch the default code from Decommitter in case of a far
call to a contract without any associated code.</li>
<li>A boolean flag <code>is_porter_available</code>, to determine the number of shards (two if zkPorter is available, one
otherwise).</li>
</ol>
<p>zkEVM retrieves the code of bootloader from Decommitter and proceeds with sequential execution of instructions on the
bootloader’s code page.</p>
<h4 id="failures-and-rollbacks"><a class="header" href="#failures-and-rollbacks">Failures and rollbacks</a></h4>
<p>There are three types of behaviour triggered by execution failures.</p>
<ol>
<li>
<p>Skipping a malformed transaction. It is a mechanism implemented by the server, external to zkEVM. Server makes a
snapshot of zkEVM state after completing every transaction. If the bootloader encounters a malformed transaction, it
fails, and the server restarts zkEVM from the most recent snapshot, skipping this transaction.</p>
<p>This behaviour is specific to server/bootloader; the contract code has no ways of invoking it.</p>
</li>
<li>
<p>Revert is triggered by the contract code explicitly by executing <code>revert</code>. zkEVM saves its persistent state on every
near or far call. If the contract code identifies a recoverable error, it may execute <code>revert</code>; then zkEVM rolls the
storage and event queues back to the last checkpoint and executes the exception handler.</p>
</li>
<li>
<p>Panic is triggered either explicitly by executing <code>panic</code> or internally when some execution invariants are violated
e.g. attempt to use raw integer in <code>ptr.add</code> instruction.</p>
<p>On panic, the persistent state of zkEVM is rolled back in the same way as on revert.</p>
</li>
</ol>
<h3 id="bootloader-2"><a class="header" href="#bootloader-2">Bootloader</a></h3>
<p>Bootloader is a system contract in charge of block construction
(<strong><a href="https://github.com/matter-labs/era-system-contracts/blob/main/bootloader/bootloader.yul">sources</a></strong>).</p>
<p>Formally, bootloader is assigned an address BOOTLOADER_SYSTEM_CONTRACT_ADDRESS = $2^{15}+1$, but zkEVM decommits its
code directly by its hash.</p>
<p>The heap of the bootloader is special: it acts as an interface between server and zkEVM. Server gradually fills the
bootloader’s heap with transaction data, formatted according to an implementation-defined convention.</p>
<p>The bootloader then acts roughly as the following code (not an actual implementation):</p>
<pre><code class="language-solidity">contract Bootloader {
  function executeBlock(address operatorAddress, Transaction[2] memory transactions) {
    for (uint256 i = 0; i &lt; transactions.length; i++) {
      validateTransaction(transactions[i]);
      chargeFee(operatorAddress, transactions[i]);
      executeTransaction(transactions[i]);
    }
  }

  function validateTransaction(Transaction memory tx) {
    // validation logic
  }

  function chargeFee(address operatorAddress, Transaction memory tx) {
    // charge fee
  }

  function executeTransaction(Transaction memory tx) {
    // execution logic
  }
}

</code></pre>
<p>The bootloader is therefore responsible for:</p>
<ul>
<li>validating transactions;</li>
<li>executing transactions to form a new block;</li>
<li>setting some of the transaction- or block-wide transaction parameters (e.g. <code>blockhash</code>, <code>tx.origin</code>).</li>
</ul>
<p>Server makes a snapshot of zkEVM state after completing every transaction. When the bootloader encounters a malformed
transaction, it fails, and the server restarts zkEVM from the most recent snapshot, skipping this transaction. If a
transaction is well-formed, zkEVM may still panic while handling it outside the bootloader code. This is a normal
situation and is handled by zkEVM in a regular way, through panics.</p>
<p>The exact code of the bootloader is a part of a protocol; its hash is included in the block header.</p>
<h3 id="context-value"><a class="header" href="#context-value">Context value</a></h3>
<p>A part of the zkEVM state is a 128-bit <em>context value</em>. It implements <code>msg.value</code> standing for the amount of wei sent in
a transaction. In assembly, it is used as follows:</p>
<ol>
<li>Execute <code>context.set_context_u128 reg</code> to set the value;</li>
<li>Perform a far call — it captures the context value;</li>
<li>In a called contract, access the context value through <code>context.get_context_u128 reg</code>.</li>
</ol>
<p>Context value can not be set in static mode.</p>
<p>The system contract <code>MsgValueSimulator</code> ensures that whenever this context value is set to <em>C</em>, there are indeed <em>C</em> wei
transferred to the callee.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zksync-development-announcement"><a class="header" href="#zksync-development-announcement">ZKsync development announcement</a></h1>
<p>This directory will contain announcements that don’t necessarily serve as documentation, but still provide valuable
information to be stored long-term.</p>
<p>Current announcements:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="editor.js"></script>
        <script src="mode-rust.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="js/version-box.js"></script>
        <script src="js/mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
