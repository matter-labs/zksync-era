version: '3.2'
services:
  geth:
    image: "matterlabs/geth:latest"
    environment:
      - PLUGIN_CONFIG

  zk:
    image: matterlabs/zk-environment:cuda-12-0-latest
    depends_on:
      - geth
      - postgres
    security_opt:
      - seccomp:unconfined
    command: tail -f /dev/null
    volumes:
      - .:/usr/src/zksync
      - /usr/src/cache:/usr/src/cache
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/src/keys:/mnt/prover_setup_keys
    environment:
      - CACHE_DIR=/usr/src/cache
      - SCCACHE_CACHE_SIZE=50g
      - SCCACHE_GCS_BUCKET=matterlabs-infra-sccache-storage
      - SCCACHE_GCS_SERVICE_ACCOUNT=gha-ci-runners@matterlabs-infra.iam.gserviceaccount.com
      - SCCACHE_ERROR_LOG=/tmp/sccache_log.txt
      - SCCACHE_GCS_RW_MODE=READ_WRITE
      - CI=1
      - GITHUB_WORKSPACE=$GITHUB_WORKSPACE
    # We set CUDAARCHS for l4 gpu's
      - CUDAARCHS=89
    # We need to forward all nvidia-devices, as due to bug with cgroups and nvidia-container-runtime (https://github.com/NVIDIA/libnvidia-container/issues/176#issuecomment-1159454366), cgroups are disabled and thou GPU isn't properly forwarded to dind
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-caps:/dev/nvidia-caps
      - /dev/nvidia-modeset:/dev/nvidia-modeset
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
    env_file:
      - ./.env
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
  postgres:
    image: "postgres:14"
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
